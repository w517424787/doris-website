<!doctype html>
<html lang="en-US" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no">
<meta name="generator" content="Docusaurus v2.4.1">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Apache Doris RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Apache Doris Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DT7W9E9722"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DT7W9E9722",{anonymize_ip:!0})</script>






<link rel="icon" href="/images/logo-only.png">
<link rel="manifest" href="/manifest.json">
<meta name="theme-color" content="#FFFFFF">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/img/docusaurus.png">
<link rel="mask-icon" href="/img/docusaurus.svg" color="rgb(37, 194, 160)">
<meta name="msapplication-TileImage" content="/img/docusaurus.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:500">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400"><title data-rh="true">19 posts tagged with &quot;Best Practice&quot; - Apache Doris</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doris.apache.org/blog/tags/best-practice"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" property="og:title" content="19 posts tagged with &quot;Best Practice&quot; - Apache Doris"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://doris.apache.org/blog/tags/best-practice"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/best-practice" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://doris.apache.org/zh-CN/blog/tags/best-practice" hreflang="zh-Hans-CN"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/best-practice" hreflang="x-default"><link rel="stylesheet" href="https://cdnd.selectdb.com/assets/css/styles.9d7bbf4f.css">
<link rel="preload" href="https://cdnd.selectdb.com/assets/js/runtime~main.772191fd.js" as="script">
<link rel="preload" href="https://cdnd.selectdb.com/assets/js/main.1e662d49.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="announcementBar_s0pr" style="background-color:#3C2FD4;color:#FFFFFF" role="banner"><div class="announcementBarPlaceholder_qxfj"></div><div class="announcementBarContent_dpRF"><a href="https://github.com/apache/doris" target="_blank" style="display: flex; width: 100%; align-items: center; justify-content: center; margin-left: 4px; text-decoration: none; color: white">Do you like Apache Dorisï¼ŸGive us a ðŸŒŸ on Github 
                        <img style="width: 1.2rem; height: 1.2rem; margin-left: 0.4rem;" src="/images/github-white-icon.svg">
                    </a></div><button type="button" class="clean-btn close announcementBarClose_iXyO" aria-label="Close"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="https://cdnd.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--light_HNdA"><img src="https://cdnd.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/">Home</a><a class="navbar__item navbar__link" href="/docs/dev/summary/basic-summary">Docs</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/blog">Blog</a><a class="navbar__item navbar__link" href="/community/team">Community</a><a class="navbar__item navbar__link" href="/users">Users</a></div><div class="navbar__items navbar__items--right"><div class="versions">Version<!-- -->:<div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/docs/dev/get-starting/">dev</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/docs/dev/get-starting/">dev</a></li><li><a class="dropdown__link" href="/docs/1.2/get-starting/">1.2</a></li></ul></div></div><div class="locale-box"><a href="/blog/tags/best-practice" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link dropdown__link--active">EN</a><a href="/zh-CN/blog/tags/best-practice" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link">ä¸­æ–‡</a></div><a class="navbar__item navbar__link header-right-button-primary navbar-download-mobile" href="/download">Download</a><div class="navbar-search searchBox_ZlJk"><div class="navbar__search searchBarContainer_PzyC"><input placeholder="Search" aria-label="Search" class="navbar__search-input"><div class="loadingRing__K5d searchBarLoadingRing_e2f0"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_m7ml"><kbd class="searchHint_zuPL">ctrl</kbd><kbd class="searchHint_zuPL">K</kbd></div></div></div><span class="github-btn desktop header-right-button-github"><span class="github-btn github-btn-large"><a class="gh-btn" href="//github.com/apache/doris/" target="_blank"><span class="gh-ico" aria-hidden="true"></span><span class="gh-text">Star</span></a><a class="gh-count" target="_blank" href="//github.com/apache/doris/stargazers/"></a></span></span><a class="header-right-button-primary navbar-download-desktop" href="/download">Download</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><div class="main-wrapper"><div class="container margin-vert--lg blog-container"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>19 posts tagged with &quot;Best Practice&quot;</h1><a href="/blog/tags">View All Tags</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Moka">Listen to That Poor BI Engineer: We Need Fast Joins</a></h2><div class="blog-info"><time datetime="2023-07-10T00:00:00.000Z" itemprop="datePublished">July 10, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Baoming Zhang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>Business intelligence (BI) tool is often the last stop of a data processing pipeline. It is where data is visualized for analysts who then extract insights from it. From the standpoint of a SaaS BI provider, what are we looking for in a database? In my job, we are in urgent need of support for fast join queries.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="why-join-query-matters">Why JOIN Query Matters<a href="#why-join-query-matters" class="hash-link" aria-label="Direct link to Why JOIN Query Matters" title="Direct link to Why JOIN Query Matters">â€‹</a></h2><p>I work as an engineer that supports a human resource management system. One prominent selling point of our services is <strong>self-service</strong> <strong>BI</strong>. That means we allow users to customize their own dashboards: they can choose the fields they need and relate them to form the dataset as they want. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_1-6653b0bedab8b84497aad6667ab2db9c.png" width="1280" height="709" class="img_ev3q"></p><p>Join query is a more efficient way to realize self-service BI. It allows people to break down their data assets into many smaller tables instead of putting it all in a flat table. This would make data updates much faster and more cost-effective, because updating the whole flat table is not always the optimal choice when you have plenty of new data flowing in and old data being updated or deleted frequently, as is the case for most data input.</p><p>In order to maximize the time value of data, we need data updates to be executed really quickly. For this purpose, we looked into three OLAP databases on the market. They are all fast in some way but there are some differences.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_2-fe0c3aef14ac2449ef661d83ca293e8d.png" width="1280" height="627" class="img_ev3q"></p><p>Greenplum is really quick in data loading and batch DML processing, but it is not good at handling high concurrency. There is a steep decline in performance as query concurrency rises. This can be risky for a BI platform that tries to ensure stable user experience. ClickHouse is mind-blowing in single-table queries, but it only allows batch update and batch delete, so that&#x27;s less timely.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="welcome-to-join-hell">Welcome to JOIN Hell<a href="#welcome-to-join-hell" class="hash-link" aria-label="Direct link to Welcome to JOIN Hell" title="Direct link to Welcome to JOIN Hell">â€‹</a></h2><p>JOIN, my old friend JOIN, is always a hassle. Join queries are demanding for both engineers and the database system. Firstly, engineers must have a thorough grasp of the schema of all tables. Secondly, these queries are resource-intensive, especially when they involve large tables. Some of the reports on our platform entail join queries across up to 20 tables. Just imagine the mess.</p><p>We tested our candidate OLAP engines with our common join queries and our most notorious slow queries. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_3-dab994e57f63d5b0b6c72b18de3a562b.png" width="1280" height="726" class="img_ev3q"></p><p>As the number of tables joined grows, we witness a widening performance gap between Apache Doris and ClickHouse. In most join queries, Apache Doris was about 5 times faster than ClickHouse. In terms of slow queries, Apache Doris responded to most of them within less than 1 second, while the performance of ClickHouse fluctuated within a relatively large range. </p><p>And just like that, we decided to upgrade our data architecture with Apache Doris. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-that-supports-our-bi-services">Architecture that Supports Our BI Services<a href="#architecture-that-supports-our-bi-services" class="hash-link" aria-label="Direct link to Architecture that Supports Our BI Services" title="Direct link to Architecture that Supports Our BI Services">â€‹</a></h2><p><strong>Data Input:</strong> </p><p>Our business data flows into DBLE, a distributed middleware based on MySQL. Then the DBLE binlogs are written into Flink, getting deduplicated, merged, and then put into Kafka. Finally, Apache Doris reads data from Kafka via its Routine Load approach. We apply the &quot;delete&quot; configuration in Routine Load to enable real-time deletion. The combination of Apache Flink and the idempotent write mechanism of Apache Doris is how we get exactly-once guarantee. We have a data size of billions of rows per table, and this architecture is able to finish data updates in one minute. </p><p>In addition, taking advantage of Apache Kafka and the Routine Load method, we are able to shave the traffic peaks and maintain cluster stability. Kafka also allows us to have multiple consumers of data and recompute intermediate data by resetting the offsets.</p><p><strong>Data Output</strong>: </p><p>As a self-service BI platform, we allow users to customize their own reports by configuring the rows, columns, and filters as they need. This is supported by Apache Doris with its capabilities in join queries. </p><p>In total, we have 400 data tables, of which 50 has over 100 million rows. That adds up to a data size measured in TB. We put all our data into two Doris clusters on 40 servers.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="no-longer-stalled-by-privileged-access-queries">No Longer Stalled by Privileged Access Queries<a href="#no-longer-stalled-by-privileged-access-queries" class="hash-link" aria-label="Direct link to No Longer Stalled by Privileged Access Queries" title="Direct link to No Longer Stalled by Privileged Access Queries">â€‹</a></h2><p>On our BI platform, privileged queries are often much slower than non-privileged queries. Timeout is often the case and even more so for queries on large datasets.</p><p>Human resource data is subject to very strict and fine-grained access control policies. The role and position of users and the confidentiality level of data determine who has access to what (the data granularity here is up to fields in a table). Occasionally, we need to separately grant a certain privilege to a particular person. On top of that, we need to ensure data isolation between the multiple tenants on our platform.</p><p>How does all this add to complexity in engineering? Any user who inputs a query on our BI platform must go through multi-factor authentication, and the authenticated information will all be inserted into the SQL via <code>in</code> and then passed on to the OLAP engine. Therefore, the more fine-grained the privilege controls are, the longer the SQL will be, and the more time the OLAP system will spend on ID filtering. That&#x27;s why our users are often tortured by high latency.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_4-64db81a5dd0659c2fe09805142c25b39.png" width="1396" height="650" class="img_ev3q"></p><p>So how did we fix that? We use the <a href="https://doris.apache.org/docs/dev/data-table/index/bloomfilter/" target="_blank" rel="noopener noreferrer">Bloom Filter index</a> in Apache Doris. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_5-666c3e530937abfa6243f0f3bb1f645c.png" width="1280" height="118" class="img_ev3q"></p><p>By adding Bloom Filter indexes to the relevant ID fields, we improve the speed of privileged queries by 30% and basically eliminate timeout errors.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Moka_6-946cd1d988bc4d2cd18f580775cb89a7.png" width="1852" height="863" class="img_ev3q"></p><p>Tips on when you should use the Bloom Filter index:</p><ul><li>For non-prefix filtering</li><li>For <code>in</code> and <code>=</code> filters on a particular column</li><li>For filtering on high-cardinality columns, such as UserID. In essence, the Bloom Filter index is used to check if a certain value exists in a dataset. There is no point in using the Bloom Filter index for a low-cardinality column, like &quot;gender&quot;, for example, because almost every data block contains all the gender values.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="to-all-bi-engineers">To All BI Engineers<a href="#to-all-bi-engineers" class="hash-link" aria-label="Direct link to To All BI Engineers" title="Direct link to To All BI Engineers">â€‹</a></h2><p>We believe self-service BI is the future in the BI landscape, just like AGI is the future for artificial intelligence. Fast join queries is the way towards it, and the foregoing architectural upgrade is part of our ongoing effort to empower that. May there be less painful JOINs in the BI world. Cheers.</p><p>Find the Apache Doris developers on <a href="https://t.co/ZxJuNJHXb2" target="_blank" rel="noopener noreferrer">Slack</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Tianyancha">Replacing Apache Hive, Elasticsearch and PostgreSQL with Apache Doris</a></h2><div class="blog-info"><time datetime="2023-07-01T00:00:00.000Z" itemprop="datePublished">July 1, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Tao Wang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>How does a data service company build its data warehouse? I worked as a real-time computing engineer for a due diligence platform, which is designed to allow users to search for a company&#x27;s business data, financial, and legal details. It has collected information of over 300 million entities in more than 300 dimensions. The duty of my colleagues and I is to ensure real-time updates of such data so we can provide up-to-date information for our registered users. That&#x27;s the customer-facing function of our data warehouse. Other than that, it needs to support our internal marketing and operation team in ad-hoc queries and user segmentation, which is a new demand that emerged with our growing business. </p><p>Our old data warehouse consisted of the most popular components of the time, including <strong>Apache</strong> <strong>Hive</strong>, <strong>MySQL</strong>, <strong>Elasticsearch</strong>, and <strong>PostgreSQL</strong>. They support the data computing and data storage layers of our data warehouse: </p><ul><li><strong>Data Computing</strong>: Apache Hive serves as the computation engine.</li><li><strong>Data Storage</strong>: <strong>MySQL</strong> provides data for DataBank, Tableau, and our customer-facing applications. <strong>Elasticsearch</strong> and <strong>PostgreSQL</strong> serve for our DMP user segmentation system: the former stores user profiling data, and the latter stores user group data packets. </li></ul><p>As you can imagine, a long and complicated data pipeline is high-maintenance and detrimental to development efficiency. Moreover, they are not capable of ad-hoc queries. So as an upgrade to our data warehouse, we replaced most of these components with <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a>, a unified analytic database.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_1-9cc7124fc979257cf029e086ce018e78.png" width="1280" height="640" class="img_ev3q"></p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_2-56765f2ef0a2d26069c3cd115e694882.png" width="1280" height="548" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-flow">Data Flow<a href="#data-flow" class="hash-link" aria-label="Direct link to Data Flow" title="Direct link to Data Flow">â€‹</a></h2><p>This is a lateral view of our data warehouse, from which you can see how the data flows.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_3-733959d2cc60e873ec5b3b9fc06d9e0e.png" width="1280" height="489" class="img_ev3q"></p><p>For starters, binlogs from MySQL will be ingested into Kafka via Canal, while user activity logs will be transferred to Kafka via Apache Flume. In Kafka, data will be cleaned and organized into flat tables, which will be later turned into aggregated tables. Then, data will be passed from Kafka to Apache Doris, which serves as the storage and computing engine. </p><p>We adopt different data models in Apache Doris for different scenarios: data from MySQL will be arranged in the <a href="https://doris.apache.org/docs/dev/data-table/data-model/#unique-model" target="_blank" rel="noopener noreferrer">Unique model</a>, log data will be put in the <a href="https://doris.apache.org/docs/dev/data-table/data-model/#duplicate-model" target="_blank" rel="noopener noreferrer">Duplicate model</a>, while data in the DWS layer will be merged in the <a href="https://doris.apache.org/docs/dev/data-table/data-model/#aggregate-model" target="_blank" rel="noopener noreferrer">Aggregate model</a>.</p><p>This is how Apache Doris replaces the roles of Hive, Elasticsearch, and PostgreSQL in our datawarehouse. Such transformation has saved us lots of efforts in development and maintenance. It also made ad-hoc queries possible and our user segmentation more efficient. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ad-hoc-queries">Ad-Hoc Queries<a href="#ad-hoc-queries" class="hash-link" aria-label="Direct link to Ad-Hoc Queries" title="Direct link to Ad-Hoc Queries">â€‹</a></h2><p><strong>Before</strong>: Every time a new request was raised, we developed and tested the data model in Hive, and wrote the scheduling task in MySQL so that our customer-facing application platforms could read results from MySQL. It was a complicated process that took a lot of time and development work. </p><p><strong>After</strong>: Since Apache Doris has all the itemized data, whenever it is faced with a new request, it can simply pull the metadata and configure the query conditions. Then it is ready for ad-hoc queries. In short, it only requires low-code configuration to respond to new requests. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_4-9a9132537dbc478b0aa9948131184564.png" width="1280" height="712" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="user-segmentation">User Segmentation<a href="#user-segmentation" class="hash-link" aria-label="Direct link to User Segmentation" title="Direct link to User Segmentation">â€‹</a></h2><p><strong>Before</strong>: After a user segmentation task was created based on metadata, the relevant user IDs would be written into the PostgreSQL profile list and the MySQL task list. Meanwhile, Elasticsearch would execute the query according to the task conditions; after the results are produced, it would update status in the task list and write the user group bitmap package into PostgreSQL. (The PostgreSQL plug-in is capable of computing the intersection, union, and difference set of bitmap.) Then PostgreSQL would provide user group packets for downstream operation platforms.</p><p>Tables in Elasticsearch and PostgreSQL were unreusable, making this architecture cost-ineffective. Plus, we had to pre-define the user tags before we could execute a new type of query. That slowed things down.  </p><p><strong>After</strong>: The user IDs will only be written into the MySQL task list. For first-time segmentation, Apache Doris will execute the <strong>ad-hoc query</strong> based on the task conditions. In subsequent segmentation tasks, Apache Doris will perform <strong>micro-batch rolling</strong> and compute the difference set compared with the previously produced user group packet, and notify downstream platforms of any updates. (This is realized by the <a href="https://doris.apache.org/docs/dev/sql-manual/sql-functions/bitmap-functions/bitmap_union" target="_blank" rel="noopener noreferrer">bitmap functions</a> in Apache Doris.) </p><p>In this Doris-centered user segmentation process, we don&#x27;t have to pre-define new tags. Instead, tags can be auto-generated based on the task conditions. The processing pipeline has the flexibility that can make our user-group-based A/B testing easier. Also, as both the itemized data and user group packets are in Apache Doris, we don&#x27;t have to attend to the read and write complexity between multiple components.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_5-82288dba1ffdb438be29168a2eafd7f9.png" width="1280" height="688" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="trick-to-speed-up-user-segmentation-by-70">Trick to Speed up User Segmentation by 70%<a href="#trick-to-speed-up-user-segmentation-by-70" class="hash-link" aria-label="Direct link to Trick to Speed up User Segmentation by 70%" title="Direct link to Trick to Speed up User Segmentation by 70%">â€‹</a></h2><p>Due to risk aversion reasons, random generation of <code>user_id</code> is the choice for many companies, but that produces sparse and non-consecutive user IDs in user group packets. Using these IDs in user segmentation, we had to endure a long waiting time for bitmaps to be generated. </p><p>To solve that, we created consecutive and dense mappings for these user IDs. <strong>In this way, we decreased our user segmentation latency by 70%.</strong></p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_6-22694f7b8d5e06aa2c8c4757c52c8c05.png" width="1030" height="218" class="img_ev3q"></p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_7-e5d5d3312ade5d026533922a01207660.png" width="1280" height="698" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="example">Example<a href="#example" class="hash-link" aria-label="Direct link to Example" title="Direct link to Example">â€‹</a></h3><p><strong>Step 1: Create a user ID mapping table:</strong></p><p>We adopt the Unique model for user ID mapping tables, where the user ID is the unique key. The mapped consecutive IDs usually start from 1 and are strictly increasing. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_8-74c77b6500d66dfb6aa2fc8ba742868c.png" width="1280" height="540" class="img_ev3q"></p><p><strong>Step 2: Create a user group table:</strong></p><p>We adopt the Aggregate model for user group tables, where user tags serve as the aggregation keys. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_9-76a30c385266aadc57e8ab898cc53bce.png" width="1280" height="604" class="img_ev3q"></p><p>Supposing that we need to pick out the users whose IDs are between 0 and 2000000. </p><p>The following snippets use non-consecutive (<code>tyc_user_id</code>) and consecutive (<code>tyc_user_id_continuous</code>) user IDs for user segmentation, respectively. There is a big gap between their <strong>response time:</strong></p><ul><li>Non-Consecutive User IDs: <strong>1843ms</strong></li><li>Consecutive User IDs: <strong>543ms</strong> </li></ul><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_10-c239e3a39b72d21c1d65fc74858b36a3.png" width="1920" height="736" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">â€‹</a></h2><p>We have 2 clusters in Apache Doris accommodating tens of TBs of data, with almost a billion new rows flowing in every day. We used to witness a steep decline in data ingestion speed as data volume expanded. But after upgrading our data warehouse with Apache Doris, we increased our data writing efficiency by 75%. Also, in user segmentation with a result set of less than 5 million, it is able to respond within milliseconds. Most importantly, our data warehouse has been simpler and friendlier to developers and maintainers. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Tianyancha_11-3fe828cadbc9a5972a82bbbd2a0b473e.png" width="1280" height="667" class="img_ev3q"></p><p>Lastly, I would like to share with you something that interested us most when we first talked to the <a href="https://t.co/KcxAtAJZjZ" target="_blank" rel="noopener noreferrer">Apache Doris community</a>:</p><ul><li>Apache Doris supports data ingestion transactions so it can ensure data is written <strong>exactly once</strong>.</li><li>It is well-integrated with the data ecosystem and can smoothly interface with most data sources and data formats.</li><li>It allows us to implement elastic scaling of clusters using the command line interface.</li><li>It outperforms ClickHouse in <strong>join queries</strong>.</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/360">A/B Testing was a Handful, Until we Found the Replacement for Druid</a></h2><div class="blog-info"><time datetime="2023-06-01T00:00:00.000Z" itemprop="datePublished">June 1, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Heyu Dou, Xinxin Wang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>Unlike normal reporting, A/B testing collects data of a different combination of dimensions every time. It is also a complicated kind of analysis of immense data. In our case, we have a real-time data volume of millions of OPS (Operations Per Second), with each operation involving around 20 data tags and over a dozen dimensions.</p><p>For effective A/B testing, as data engineers, we must ensure quick computation as well as high data integrity (which means no duplication and no data loss). I&#x27;m sure I&#x27;m not the only one to say this: it is hard!</p><p>Let me show you our long-term struggle with our previous Druid-based data platform.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="platform-architecture-10">Platform Architecture 1.0<a href="#platform-architecture-10" class="hash-link" aria-label="Direct link to Platform Architecture 1.0" title="Direct link to Platform Architecture 1.0">â€‹</a></h2><p><strong>Components</strong>: Apache Storm + Apache Druid + MySQL</p><p>This was our real-time datawarehouse, where Apache Storm was the real-time data processing engine and Apache Druid pre-aggregated the data. However, Druid did not support certain paging and join queries, so we wrote data from Druid to MySQL regularly, making MySQL the &quot;materialized view&quot; of Druid. But that was only a duct tape solution as it couldn&#x27;t support our ever enlarging real-time data size. So data timeliness was unattainable.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_1-8cb2f7a87f8ce60f9da14e0ec0ea7bb5.png" width="1709" height="960" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="platform-architecture-20">Platform Architecture 2.0<a href="#platform-architecture-20" class="hash-link" aria-label="Direct link to Platform Architecture 2.0" title="Direct link to Platform Architecture 2.0">â€‹</a></h2><p><strong>Components</strong>: Apache Flink + Apache Druid + TiDB</p><p>This time, we replaced Storm with Flink, and MySQL with TiDB. Flink was more powerful in terms of semantics and features, while TiDB, with its distributed capability, was more maintainable than MySQL. But architecture 2.0 was nowhere near our goal of end-to-end data consistency, either, because when processing huge data, enabling TiDB transactions largely slowed down data writing. Plus, Druid itself did not support standard SQL, so there were some learning costs and frictions in usage.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_2-d32b762837d3788bdc43f0370fbf8199.png" width="1592" height="1083" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="platform-architecture-30">Platform Architecture 3.0<a href="#platform-architecture-30" class="hash-link" aria-label="Direct link to Platform Architecture 3.0" title="Direct link to Platform Architecture 3.0">â€‹</a></h2><p><strong>Components</strong>: Apache Flink + <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a></p><p>We replaced Apache Druid with Apache Doris as the OLAP engine, which could also serve as a unified data serving gateway. So in Architecture 3.0, we only need to maintain one set of query logic. And we layered our real-time datawarehouse to increase reusability of real-time data.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_3-c04ebf18268d873153f0365681d2a5d0.png" width="1340" height="1101" class="img_ev3q"></p><p>Turns out the combination of Flink and Doris was the answer. We can exploit their features to realize quick computation and data consistency. Keep reading and see how we make it happen.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="quick-computation">Quick Computation<a href="#quick-computation" class="hash-link" aria-label="Direct link to Quick Computation" title="Direct link to Quick Computation">â€‹</a></h2><p>As one piece of operation data can be attached to 20 tags, in A/B testing, we compare two groups of data centering only one tag each time. At first, we thought about splitting one piece of operation data (with 20 tags) into 20 pieces of data of only one tag upon data ingestion, and then importing them into Doris for analysis, but that could cause a data explosion and thus huge pressure on our clusters. </p><p>Then we tried moving part of such workload to the computation engine. So we tried and &quot;exploded&quot; the data in Flink, but soon regretted it, because when we aggregated the data using the global hash windows in Flink jobs, the network and CPU usage also &quot;exploded&quot;.</p><p>Our third shot was to aggregate data locally in Flink right after we split it. As is shown below, we create a window in the memory of one operator for local aggregation; then we further aggregate it using the global hash windows. Since two operators chained together are in one thread, transferring data between operators consumes much less network resources. <strong>The two-step aggregation method, combined with the</strong> <strong><a href="https://doris.apache.org/docs/dev/data-table/data-model" target="_blank" rel="noopener noreferrer">Aggregate model</a></strong> <strong>of Apache Doris, can keep data explosion in a manageable range.</strong></p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_4-b4cad8ba4f8625718a23e7297885c40d.png" width="1642" height="624" class="img_ev3q"></p><p>For convenience in A/B testing, we make the test tag ID the first sorted field in Apache Doris, so we can quickly locate the target data using sorted indexes. To further minimize data processing in queries, we create materialized views with the frequently used dimensions. With constant modification and updates, the materialized views are applicable in 80% of our queries.</p><p>To sum up, with the application of sorted index and materialized views, we reduce our query response time to merely seconds in A/B testing.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-integrity-guarantee">Data Integrity Guarantee<a href="#data-integrity-guarantee" class="hash-link" aria-label="Direct link to Data Integrity Guarantee" title="Direct link to Data Integrity Guarantee">â€‹</a></h2><p>Imagine that your algorithm designers worked sweat and tears trying to improve the business, only to find their solution unable to be validated by A/B testing due to data loss. This is an unbearable situation, and we make every effort to avoid it.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="develop-a-sink-to-doris-component">Develop a Sink-to-Doris Component<a href="#develop-a-sink-to-doris-component" class="hash-link" aria-label="Direct link to Develop a Sink-to-Doris Component" title="Direct link to Develop a Sink-to-Doris Component">â€‹</a></h3><p>To ensure end-to-end data integrity, we developed a Sink-to-Doris component. It is built on our own Flink Stream API scaffolding and realized by the idempotent writing of Apache Doris and the two-stage commit mechanism of Apache Flink. On top of it, we have a data protection mechanism against anomalies. </p><p>It is the result of our long-term evolution. We used to ensure data consistency by implementing &quot;one writing for one tag ID&quot;. Then we realized we could make good use of the transactions in Apache Doris and the two-stage commit of Apache Flink. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_5-b5f8490ad14a1b485d4472b3db36e9d6.png" width="3380" height="3334" class="img_ev3q"></p><p>As is shown above, this is how two-stage commit works to guarantee data consistency:</p><ol><li>Write data into local files;</li><li>Stage One: pre-commit data to Apache Doris. Save the Doris transaction ID into status;</li><li>If checkpoint fails, manually abandon the transaction; if checkpoint succeeds, commit the transaction in Stage Two;</li><li>If the commit fails after multiple retries, the transaction ID and the relevant data will be saved in HDFS, and we can restore the data via Broker Load.</li></ol><p>We make it possible to split a single checkpoint into multiple transactions, so that we can prevent one Stream Load from taking more time than a Flink checkpoint in the event of large data volumes.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="application-display">Application Display<a href="#application-display" class="hash-link" aria-label="Direct link to Application Display" title="Direct link to Application Display">â€‹</a></h3><p>This is how we implement Sink-to-Doris. The component has blocked API calls and topology assembly. With simple configuration, we can write data into Apache Doris via Stream Load. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_6-9d94599760bc55e52be086ec6d44cc69.png" width="3289" height="1077" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-monitoring">Cluster Monitoring<a href="#cluster-monitoring" class="hash-link" aria-label="Direct link to Cluster Monitoring" title="Direct link to Cluster Monitoring">â€‹</a></h3><p>For cluster and host monitoring, we adopted the metrics templates provided by the Apache Doris community. For data monitoring, in addition to the template metrics, we added Stream Load request numbers and loading rates.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_7-a8f9f0c95e96e136b287be46bdbc4add.png" width="2001" height="832" class="img_ev3q"></p><p>Other metrics of our concerns include data writing speed and task processing time. In the case of anomalies, we will receive notifications in the form of phone calls, messages, and emails.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/360_8-e02d4bf0c8cfab543e5693216fee6357.png" width="1280" height="888" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="key-takeaways">Key Takeaways<a href="#key-takeaways" class="hash-link" aria-label="Direct link to Key Takeaways" title="Direct link to Key Takeaways">â€‹</a></h2><p>The recipe for successful A/B testing is quick computation and high data integrity. For this purpose, we implement a two-step aggregation method in Apache Flink, utilize the Aggregate model, materialized view, and short indexes of Apache Doris. Then we develop a Sink-to-Doris component, which is realized by the idempotent writing of Apache Doris and the two-stage commit mechanism of Apache Flink.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Midland Realty">Building a Data Warehouse for Traditional Industry</a></h2><div class="blog-info"><time datetime="2023-05-12T00:00:00.000Z" itemprop="datePublished">May 12, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Herman Seah</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>By Herman Seah, Data Warehouse Planner &amp; Data Analyst at Midland Realty</p><p>This is a part of the digital transformation of a real estate giant. For the sake of confidentiality, I&#x27;m not going to reveal any business data, but you&#x27;ll get a detailed view of our data warehouse and our optimization strategies.</p><p>Now let&#x27;s get started.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture">Architecture<a href="#architecture" class="hash-link" aria-label="Direct link to Architecture" title="Direct link to Architecture">â€‹</a></h2><p>Logically, our data architecture can be divided into four parts.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_1-13321d195f728638c4903bdd51e60ef0.png" width="1280" height="616" class="img_ev3q"></p><ul><li><strong>Data integration</strong>: This is supported by Flink CDC, DataX, and the Multi-Catalog feature of Apache Doris.</li><li><strong>Data management</strong>: We use Apache Dolphinscheduler for script lifecycle management, privileges in multi-tenancy management, and data quality monitoring.</li><li><strong>Alerting</strong>: We use Grafana, Prometheus, and Loki to monitor component resources and logs.</li><li><strong>Data services</strong>: This is where BI tools step in for user interaction, such as data queries and analysis.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-tables">1. <strong>Tables</strong><a href="#1-tables" class="hash-link" aria-label="Direct link to 1-tables" title="Direct link to 1-tables">â€‹</a></h3><p>We create our dimension tables and fact tables centering each operating entity in business, including customers, houses, etc. If there are a series of activities involving the same operating entity, they should be recorded by one field. (This is a lesson learned from our previous chaotic data management system.)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-layers">2. <strong>Layers</strong><a href="#2-layers" class="hash-link" aria-label="Direct link to 2-layers" title="Direct link to 2-layers">â€‹</a></h3><p>Our data warehouse is divided into five conceptual layers. We use Apache Doris and Apache DolphinScheduler to schedule the DAG scripts between these layers.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_2-4d94af927a13961e91486cef3512b47f.png" width="1280" height="729" class="img_ev3q"></p><p>Every day, the layers go through an overall update besides incremental updates in case of changes in historical status fields or incomplete data synchronization of ODS tables.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-incremental-update-strategies">3. <strong>Incremental Update Strategies</strong><a href="#3-incremental-update-strategies" class="hash-link" aria-label="Direct link to 3-incremental-update-strategies" title="Direct link to 3-incremental-update-strategies">â€‹</a></h3><p>(1) Set <code>where &gt;= &quot;activity time -1 day or -1 hour&quot;</code>  instead of <code>where &gt;= &quot;activity time</code></p><p>The reason for doing so is to prevent data drift caused by the time gap of scheduling scripts. Let&#x27;s say, with the execution interval set to 10 min, suppose that the script is executed at 23:58:00 and a new piece of data arrives at 23:59:00, if we set <code>where &gt;= &quot;activity time</code>, that piece of data of the day will be missed.</p><p>(2) Fetch the ID of the largest primary key of the table before every script execution, store the ID in the auxiliary table, and set <code>where &gt;= &quot;ID in auxiliary table&quot;</code></p><p>This is to avoid data duplication. Data duplication might happen if you use the Unique Key model of Apache Doris and designate a set of primary keys, because if there are any changes in the primary keys in the source table, the changes will be recorded and the relevant data will be loaded. This method can fix that, but it is only applicable when the source tables have auto-increment primary keys.</p><p>(3) Partition the tables</p><p>As for time-based auto-increment data such as log tables, there might be less changes in historical data and status, but the data volume is large, so there could be huge computing pressure on overall updates and snapshot creation. Hence, it is better to partition such tables, so for each incremental update, we only need to replace one partition. (You might need to watch out for data drift, too.)</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-overall-update-strategies">4. <strong>Overall Update Strategies</strong><a href="#4-overall-update-strategies" class="hash-link" aria-label="Direct link to 4-overall-update-strategies" title="Direct link to 4-overall-update-strategies">â€‹</a></h3><p>(1) Truncate Table</p><p>Clear out the table and then ingest all data from the source table into it. This is applicable for small tables and scenarios with no user activity in wee hours.</p><p>(2) <code>ALTER TABLE tbl1 REPLACE WITH TABLE tbl2 </code></p><p>This is an atomic operation and it is advisable for large tables. Every time before executing a script, we create a temporary table with the same schema, load all data into it, and replace the original table with it.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="application">Application<a href="#application" class="hash-link" aria-label="Direct link to Application" title="Direct link to Application">â€‹</a></h2><ul><li><strong>ETL job</strong>: every minute</li><li><strong>Configuration for first-time deployment</strong>: 8 nodes, 2 frontends, 8 backends, hybrid deployment</li><li><strong>Node configuration</strong>: 32C <em> 60GB </em> 2TB SSD</li></ul><p>This is our configuration for TBs of legacy data and GBs of incremental data. You can use it as a reference and scale your cluster on this basis. Deployment of Apache Doris is simple. You don&#x27;t need other components.</p><ol><li>To integrate offline data and log data, we use DataX, which supports CSV format and readers of many relational databases, and Apache Doris provides a DataX-Doris-Writer.</li></ol><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_3-d394cef81ce173d944a379f14824f5e6.png" width="992" height="636" class="img_ev3q"></p><ol start="2"><li>We use Flink CDC to synchronize data from source tables. Then we aggregate the real-time metrics utilizing the Materialized View or the Aggregate Model of Apache Doris. Since we only have to process part of the metrics in a real-time manner and we don&#x27;t want to generate too many database connections, we use one Flink job to maintain multiple CDC source tables. This is realized by the multi-source merging and full database sync features of Dinky, or you can implement a Flink DataStream multi-source merging task yourself. It is noteworthy that Flink CDC and Apache Doris support Schema Change.</li></ol><div class="language-SQL codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-SQL codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">EXECUTE CDCSOURCE demo_doris WITH (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;hostname&#x27; = &#x27;127.0.0.1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;port&#x27; = &#x27;3306&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;username&#x27; = &#x27;root&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;password&#x27; = &#x27;123456&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;checkpoint&#x27; = &#x27;10000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;scan.startup.mode&#x27; = &#x27;initial&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;parallelism&#x27; = &#x27;1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;table-name&#x27; = &#x27;ods.ods_*,ods.ods_*&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.connector&#x27; = &#x27;doris&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.fenodes&#x27; = &#x27;127.0.0.1:8030&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.username&#x27; = &#x27;root&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.password&#x27; = &#x27;123456&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.doris.batch.size&#x27; = &#x27;1000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.max-retries&#x27; = &#x27;1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.batch.interval&#x27; = &#x27;60000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.db&#x27; = &#x27;test&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.properties.format&#x27; =&#x27;json&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.properties.read_json_by_line&#x27; =&#x27;true&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.table.identifier&#x27; = &#x27;${schemaName}.${tableName}&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.label-prefix&#x27; = &#x27;${schemaName}_${tableName}_1&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="3"><li>We use SQL scripts or &quot;Shell + SQL&quot; scripts, and we perform script lifecycle management. At the ODS layer, we write a general DataX job file and pass parameters for each source table ingestion, instead of writing a DataX job for each source table. In this way, we make things much easier to maintain. We manage the ETL scripts of Apache Doris on DolphinScheduler, where we also conduct version control. In case of any errors in the production environment, we can always rollback.</li></ol><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_4-f50219b88be08e1bdf3a7b31c21ae258.png" width="1280" height="625" class="img_ev3q"></p><ol start="4"><li>After ingesting data with ETL scripts, we create a page in our reporting tool. We assign different privileges to different accounts using SQL, including the privilege of modifying rows, fields, and global dictionary. Apache Doris supports privilege control over accounts, which works the same as that in MySQL. </li></ol><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_5-7b83ea92344d586f4de8cd363b7c6357.png" width="1280" height="516" class="img_ev3q"></p><p>We also use Apache Doris data backup for disaster recovery, Apache Doris audit logs to monitor SQL execution efficiency, Grafana+Loki for cluster metric alerts, and Supervisor to monitor the daemon processes of node components.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Direct link to Optimization" title="Direct link to Optimization">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-data-ingestion">1. Data Ingestion<a href="#1-data-ingestion" class="hash-link" aria-label="Direct link to 1. Data Ingestion" title="Direct link to 1. Data Ingestion">â€‹</a></h3><p>We use DataX to Stream Load offline data. It allows us to adjust the size of each batch. The Stream Load method returns results synchronously, which meets the needs of our architecture. If we execute asynchronous data import using DolphinScheduler, the system might assume that the script has been executed, and that can cause a messup. If you use a different method, we recommend that you execute <code>show load</code> in the shell script, and check the regex filtering status to see if the ingestion succeeds.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-data-model">2. Data Model<a href="#2-data-model" class="hash-link" aria-label="Direct link to 2. Data Model" title="Direct link to 2. Data Model">â€‹</a></h3><p>We adopt the Unique Key model of Apache Doris for most of our tables. The Unique Key model ensures idempotence of data scripts and effectively avoids upstream data duplication. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-reading-external-data">3. Reading External Data<a href="#3-reading-external-data" class="hash-link" aria-label="Direct link to 3. Reading External Data" title="Direct link to 3. Reading External Data">â€‹</a></h3><p>We use the Multi-Catalog feature of Apache Doris to connect to external data sources. It allows us to create mappings of external data at the Catalog level.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-query-optimization">4. Query Optimization<a href="#4-query-optimization" class="hash-link" aria-label="Direct link to 4. Query Optimization" title="Direct link to 4. Query Optimization">â€‹</a></h3><p>We suggest that you put the most frequently used fields of non-character types (such as int and where clauses) in the first 36 bytes, so you can filter these fields within milliseconds in point queries.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-data-dictionary">5. Data Dictionary<a href="#5-data-dictionary" class="hash-link" aria-label="Direct link to 5. Data Dictionary" title="Direct link to 5. Data Dictionary">â€‹</a></h3><p>For us, it is important to create a data dictionary because it largely reduces personnel communication costs, which can be a headache when you have a big team. We use the <code>information_schema</code> in Apache Doris to generate a data dictionary. With it, we can quickly grasp the whole picture of the tables and fields and thus increase development efficiency.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="performance">Performance<a href="#performance" class="hash-link" aria-label="Direct link to Performance" title="Direct link to Performance">â€‹</a></h2><p><strong>Offline data ingestion time</strong>: Within minutes</p><p><strong>Query latency</strong>: For tables containing over 100 million rows, Apache Doris responds to ad-hoc queries within one second, and complicated queries in five seconds.</p><p><strong>Resource consumption</strong>: It only takes up a small number of servers to build this data warehouse. The 70% compression ratio of Apache Doris saves us lots of storage resources.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experience-and-conclusion"><strong>Experience and Conclusion</strong><a href="#experience-and-conclusion" class="hash-link" aria-label="Direct link to experience-and-conclusion" title="Direct link to experience-and-conclusion">â€‹</a></h2><p>Actually, before we evolved into our current data architecture, we tried Hive, Spark and Hadoop to build an offline data warehouse. It turned out that Hadoop was overkill for a traditional company like us since we didn&#x27;t have too much data to process. It is important to find the component that suits you most.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Midland_6-52e4498a6ab21c3075077b71435e2d28.png" width="832" height="703" class="img_ev3q"></p><p>(Our old off-line data warehouse)</p><p>On the other hand, to smoothen our  big data transition, we need to make our data platform as simple as possible in terms of usage and maintenance. That&#x27;s why we landed on Apache Doris. It is compatible with MySQL protocol and provides a rich collection of functions so we don&#x27;t have to develop our own UDFs. Also, it is composed of only two types of processes: frontends and backends, so it is easy to scale and track</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Douyu">Zipping up the Lambda Architecture for 40% Faster Performance</a></h2><div class="blog-info"><time datetime="2023-05-05T00:00:00.000Z" itemprop="datePublished">May 5, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Tongyang Han</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>Author: Tongyang Han, Senior Data Engineer at Douyu</p><p>The Lambda architecture has been common practice in big data processing. The concept is to separate stream (real time data) and batch (offline data) processing, and that&#x27;s exactly what we did. These two types of data of ours were processed in two isolated tubes before they were pooled together and ready for searches and queries.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_1-cfd4fa7607d4bf15315307b50436d676.png" width="1276" height="613" class="img_ev3q"></p><p>Then we run into a few problems:</p><ol><li><strong>Isolation of real-time and offline data warehouses</strong><ol><li>I know this is kind of the essence of Lambda architecture, but that means we could not reuse real-time data since it was not layered as offline data, so further customized development was required.</li></ol></li><li><strong>Complex Pipeline from Data Sources to Data Application</strong><ol><li>Data had to go through multi-step processing before it reached our data users. As our architecture involved too many components, navigating and maintaining these tech stacks was a lot of work.</li></ol></li><li><strong>Lack of management of real-time data sources</strong><ol><li>In extreme cases, this worked like a data silo and we had no way to find out whether the ingested data was duplicated or reusable.</li></ol></li></ol><p>So we decided to &quot;zip up&quot; the Lambda architecture a little bit. By &quot;zipping up&quot;, I mean to introduce an OLAP engine that is capable of processing, storing, and analyzing data, so real-time data and offline data converge a little earlier than they used to. It is not a revolution of Lambda, but a minor change in the choice of components, which made our real-time data processing 40% faster.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="zipping-up-lambda-architecture"><strong>Zipping up Lambda Architecture</strong><a href="#zipping-up-lambda-architecture" class="hash-link" aria-label="Direct link to zipping-up-lambda-architecture" title="Direct link to zipping-up-lambda-architecture">â€‹</a></h2><p>I am going to elaborate on how this is done using our data tagging process as an example.</p><p>Previously, our offline tags were produced by the data warehouse, put into a flat table, and then written in <strong>HBase</strong>, while real-time tags were produced by <strong>Flink</strong>, and put into <strong>HBase</strong> directly. Then <strong>Spark</strong> would work as the computing engine.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_2-9cd11673aa896382f99ca957435efd84.png" width="1280" height="602" class="img_ev3q"></p><p>The problem with this stemmed from the low computation efficiency of <strong>Flink</strong> and <strong>Spark</strong>. </p><ul><li><strong>Real-time tag production</strong>: When computing real-time tags that involve data within a long time range, Flink did not deliver stable performance and consumed more resources than expected. And when a task failed, it would take a really long time for checkpoint recovery.</li><li><strong>Tag query</strong>: As a tag query engine, Spark could be slow.</li></ul><p>As a solution, we replaced <strong>HBase</strong> and <strong>Spark</strong> with <strong>Apache Doris</strong>, a real-time analytic database, and moved part of the computational logic of the foregoing wide-time-range real-time tags from <strong>Flink</strong> to <strong>Apache Doris</strong>.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_3-684e3028f23e722b9892e0afdf472e4b.png" width="1280" height="577" class="img_ev3q"></p><p>Instead of putting our flat tables in HBase, we place them in Apache Doris. These tables are divided into partitions based on time sensitivity. Offline tags will be updated daily while real-time tags will be updated in real time. We organize these tables in the Aggregate Model of Apache Doris, which allows partial update of data.</p><p>Instead of using Spark for queries, we parse the query rules into SQL for execution in Apache Doris. For pattern matching, we use Redis to cache the hot data from Apache Doris, so the system can respond to such queries much faster.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_4-afd928fc30baf4ec825e80ab3638e984.png" width="1280" height="486" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="computational-pipeline-of-wide-time-range-real-time-tags"><strong>Computational Pipeline of Wide-Time-Range Real-Time Tags</strong><a href="#computational-pipeline-of-wide-time-range-real-time-tags" class="hash-link" aria-label="Direct link to computational-pipeline-of-wide-time-range-real-time-tags" title="Direct link to computational-pipeline-of-wide-time-range-real-time-tags">â€‹</a></h2><p>In some cases, the computation of wide-time-range real-time tags entails the aggregation of historical (offline) data with real-time data. The following figure shows our old computational pipeline for these tags. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_5-104e16d5c9830069f513dc4c25665bcf.png" width="1280" height="695" class="img_ev3q"></p><p>As you can see, it required multiple tasks to finish computing one real-time tag. Also, in complicated aggregations that involve a collection of aggregation operations, any improper resource allocation could lead to back pressure or waste of resources. This adds to the difficulty of task scheduling. The maintenance and stability guarantee of such a long pipeline could be an issue, too.</p><p>To improve on that, we decided to move such aggregation workload to Apache Doris.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Douyu_6-4243729274c033573acca9a2c621bf45.png" width="1280" height="717" class="img_ev3q"></p><p>We have around 400 million customer tags in our system, and each customer is attached with over 300 tags. We divide customers into more than 10,000 groups, and we have to update 5000 of them on a daily basis. The above improvement has sped up the computation of our wide-time-range real-time queries by <strong>40%</strong>.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="overwrite">Overwrite<a href="#overwrite" class="hash-link" aria-label="Direct link to Overwrite" title="Direct link to Overwrite">â€‹</a></h2><p>To atomically replace data tables and partitions in Apache Doris, we customized the <a href="https://github.com/apache/doris-spark-connector" target="_blank" rel="noopener noreferrer">Doris-Spark-Connector</a>, and added an &quot;Overwrite&quot; mode to the Connector.</p><p>When a Spark job is submitted, Apache Doris will call an interface to fetch information of the data tables and partitions.</p><ul><li>If it is a non-partitioned table, we create a temporary table for the target table, ingest data into it, and then perform atomic replacement. If the data ingestion fails, we clear the temporary table;</li><li>If it is a dynamic partitioned table, we create a temporary partition for the target partition, ingest data into it, and then perform atomic replacement. If the data ingestion fails, we clear the temporary partition;</li><li>If it is a non-dynamic partitioned table, we need to extend the Doris-Spark-Connector parameter configuration first. Then we create a temporary partition and take steps as above.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="conclusion">Conclusion<a href="#conclusion" class="hash-link" aria-label="Direct link to Conclusion" title="Direct link to Conclusion">â€‹</a></h2><p>One prominent advantage of Lambda architecture is the stability it provides. However, in our practice, the processing of real-time data and offline data sometimes intertwines. For example, the computation of certain real-time tags requires historical (offline) data. Such interaction becomes a root cause of instability. Thus, instead of pooling real-time and offline data after they are fully ready for queries, we use an OLAP engine to share part of the pre-query computation burden and make things faster, simpler, and more cost-effective.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/HYXJ">Step-by-step Guide to Building a High-Performing Risk Data Mart</a></h2><div class="blog-info"><time datetime="2023-04-20T00:00:00.000Z" itemprop="datePublished">April 20, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Jacob Chow</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p>Pursuing data-driven management at a consumer financing company, we aim to serve four needs in our data platform development: monitoring and alerting, query and analysis, dashboarding, and data modeling. For these purposes, we built our data processing architecture based on Greenplum and CDH. The most essential part of it is the risk data mart. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="risk-data-mart--apache-hive">Risk Data Mart:  Apache Hive<a href="#risk-data-mart--apache-hive" class="hash-link" aria-label="Direct link to Risk Data Mart:  Apache Hive" title="Direct link to Risk Data Mart:  Apache Hive">â€‹</a></h2><p>I will walk you through how the risk data mart works following the data flow: </p><ol><li>Our <strong>business data</strong> is imported into <strong>Greenplum</strong> for real-time analysis to generate BI reports. Part of this data also goes into Apache Hive for queries and modeling analysis. </li><li>Our <strong>risk control variables</strong> are updated into <strong>Elasticsearch</strong> in real time via message queues, while Elasticsearch ingests data into Hive for analysis, too.</li><li>The <strong>risk management decision data</strong> is passed from <strong>MongoDB</strong> to Hive for risk control analysis and modeling.</li></ol><p>So these are the three data sources of our risk data mart.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_1-7e8b0a7061d967673ece1d403f03edd3.png" width="826" height="486" class="img_ev3q"></p><p>This whole architecture is built with CDH 6.0. The workflows in it can be divided into real-time data streaming and offline risk analysis.</p><ul><li><strong>Real-time data streaming</strong>: Real-time data from Apache Kafka will be cleaned by Apache Flink, and then written into Elasticsearch. Elasticsearch will aggregate part of the data it receives and send it for reference in risk management. </li><li><strong>Offline risk analysis</strong>: Based on the CDH solution and utilizing Sqoop, we ingest data from Greenplum in an offline manner. Then we put this data together with the third-party data from MongoDB. Then, after data cleaning, we pour all this data into Hive for daily batch processing and data queries.</li></ul><p>To give a brief overview, these are the components that support the four features of our data processing platform:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_2-1880ff586d295ecd43f0731f01124965.png" width="1002" height="606" class="img_ev3q"></p><p>As you see, Apache Hive is central to this architecture. But in practice, it takes minutes for Apache Hive to execute analysis, so our next step is to increase query speed.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="what-are-slowing-down-our-queries">What are Slowing Down Our Queries?<a href="#what-are-slowing-down-our-queries" class="hash-link" aria-label="Direct link to What are Slowing Down Our Queries?" title="Direct link to What are Slowing Down Our Queries?">â€‹</a></h3><ol><li><strong>Huge data volume in external tables</strong></li></ol><p>Our Hive-based data mart is now carrying more than 300 terabytes of data. That&#x27;s about 20,000 tables and 5 million fields. To put them all in external tables is maintenance-intensive. Plus, data ingestion can be a big headache.</p><ol><li><strong>Big flat tables</strong></li></ol><p>Due to the complexity of the rule engine in risk management, our company invests a lot in the derivation of variables. In some dimensions, we have thousands of variables or even more. As a result, a few of the frequently used flat tables in Hive have over 3000 fields. So you can imagine how time consuming these queries can be.</p><ol><li><strong>Unstable interface</strong></li></ol><p>Results produced by daily offline batch processing will be regularly sent to our Elasticsearch clusters. (The data volume in these updates is huge, and the call of interface can get expired.) This process might cause high I/O and introduce garbage collection jitter, and further leads to unstable interface services. </p><p>In addition, since our risk control analysts and modeling engineers are using Hive with Spark, the expanding data architecture is also dragging down query performance.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="a-unified-query-gateway">A Unified Query Gateway<a href="#a-unified-query-gateway" class="hash-link" aria-label="Direct link to A Unified Query Gateway" title="Direct link to A Unified Query Gateway">â€‹</a></h2><p>We wanted a unified gateway to manage our heterogenous data sources. That&#x27;s why we introduced Apache Doris.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_3-89141f14a59c83d413d14f31fcf386f4.png" width="1716" height="1094" class="img_ev3q"></p><p>But doesn&#x27;t that make things even more complicated? Actually, no.</p><p>We can connect various data sources to Apache Doris and simply conduct queries on it. This is made possible by the <strong>Multi-Catalog</strong> feature of Apache Doris: It can interface with various data sources, including datalakes like Apache Hive, Apache Iceberg, and Apache Hudi, and databases like MySQL, Elasticsearch, and Greenplum. That happens to cover our toolkit. </p><p>We create Elasticsearch Catalog and Hive Catalog in Apache Doris. These catalogs map to the external data in Elasticsearch and Hive, so we can conduct federated queries across these data sources using Apache Doris as a unified gateway. Also, we use the <a href="https://github.com/apache/doris-spark-connector" target="_blank" rel="noopener noreferrer">Spark-Doris-Connector</a> to allow data communication between Spark and Doris. So basically, we replace Apache Hive with Apache Doris as the central hub of our data architecture. </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_4-e6af4e754989aed3aef02a357e7607ad.png" width="1002" height="608" class="img_ev3q"></p><p>How does that affect our data processing efficiency?</p><ul><li><strong>Monitoring &amp; Alerting</strong>: This is about real-time data querying. We access our real-time data in Elasticsearch clusters using Elasticsearch Catalog in Apache Doris. Then we perform queries directly in Apache Doris. It is able to return results within seconds, as opposed to the minute-level response time when we used Hive.</li><li><strong>Query &amp; Analysis</strong>: As I said, we have 20,000 tables in Hive so it wouldn&#x27;t make sense to map all of them to external tables in Hive. That would mean a hell of maintenance. Instead, we utilize the Multi Catalog feature of Apache Doris 1.2. It enables data mapping at the catalog level, so we can simply create one Hive Catalog in Doris before we can conduct queries. This separates query operations from the daily batching processing workload in Hive, so there will be less resource conflict.</li><li><strong>Dashboarding</strong>: We use Tableau and Doris to provide dashboard services. This reduces the query response time to seconds and milliseconds, compared with the several minutes back in the &quot;Tableau + Hive&quot; days.</li><li><strong>Modeling</strong>: We use Spark and Doris for aggregation modeling. The Spark-Doris-Connector allows mutual synchronization of data, so data from Doris can also be used in modeling for more accurate analysis.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="cluster-monitoring-in-production-environment"><strong>Cluster Monitoring in Production Environment</strong><a href="#cluster-monitoring-in-production-environment" class="hash-link" aria-label="Direct link to cluster-monitoring-in-production-environment" title="Direct link to cluster-monitoring-in-production-environment">â€‹</a></h3><p>We tested this new architecture in our production environment. We built two clusters.</p><p><strong>Configuration</strong>:</p><p>Production cluster: 4 frontends + 8 backends, m5d.16xlarge</p><p>Backup cluster: 4 frontends + 4 backends, m5d.16xlarge</p><p>This is the monitoring board: </p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_5-8a88d55e3ac69ac6be859a9c367b0c76.png" width="1280" height="523" class="img_ev3q"></p><p>As is shown, the queries are fast. We expected that it would take at least 10 nodes but in real cases, we mainly conduct queries via Catalogs, so we can handle this with a relatively small cluster size. The compatibility is good, too. It doesn&#x27;t rock the rest of our existing system.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="guide-to-faster-data-integration">Guide to Faster Data Integration<a href="#guide-to-faster-data-integration" class="hash-link" aria-label="Direct link to Guide to Faster Data Integration" title="Direct link to Guide to Faster Data Integration">â€‹</a></h2><p>To accelerate the regular data ingestion from Hive to Apache Doris 1.2.2, we have a solution that goes as follows:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_6-946a2cf22287a5c16c7fc03d2a3e2c18.png" width="1280" height="681" class="img_ev3q"></p><p><strong>Main components:</strong></p><ul><li>DolphinScheduler 3.1.4</li><li>SeaTunnel 2.1.3</li></ul><p>With our current hardware configuration, we use the Shell script mode of DolphinScheduler and call the SeaTunnel script on a regular basis. This is the configuration file of the data synchronization tasks:</p><div class="language-undefined codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-undefined codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  env{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark.app.name = &quot;hive2doris-template&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark.executor.instances = 10</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark.executor.cores = 5</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark.executor.memory = &quot;20g&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark.sql.catalogImplementation = &quot;hive&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hive {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pre_sql = &quot;select * from ods.demo_tbl where dt=&#x27;2023-03-09&#x27;&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    result_table_name = &quot;ods_demo_tbl&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">transform {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sink {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  doris {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      fenodes = &quot;192.168.0.10:8030,192.168.0.11:8030,192.168.0.12:8030,192.168.0.13:8030&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      user = root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      password = &quot;XXX&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      database = ods</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      table = ods_demo_tbl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      batch_size = 500000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      max_retries = 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      interval = 10000</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      doris.column_separator = &quot;\t&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>This solution consumes less resources and memory but brings higher performance in queries and data ingestion.</p><ol><li><strong>Less storage costs</strong></li></ol><p><strong>Before</strong>: The original table in Hive had 500 fields. It was divided into partitions by day, with 150 million pieces of data per partition. It takes <strong>810G</strong> to store in HDFS.</p><p><strong>After</strong>: For data synchronization, we call Spark on YARN using SeaTunnel. It can be finished within 40 minutes, and the ingested data only takes up <strong>270G</strong> of storage space.</p><ol><li><strong>Less memory usage &amp; higher performance in queries</strong></li></ol><p><strong>Before</strong>: For a GROUP BY query on the foregoing table in Hive, it occupied 720 Cores and 1.44T in YARN, and took a response time of <strong>162 seconds</strong>. </p><p><strong>After</strong>: We perform an aggregate query using Hive Catalog in Doris, <code>set exec_mem_limit=16G</code>, and receive the result after <strong>58.531 seconds</strong>. We also try and put the table in Doris and conduct the same query in Doris itself, that only takes <strong>0.828 seconds</strong>.</p><p>The corresponding statements are as follows:</p><ul><li>Query in Hive, response time: 162 seconds</li></ul><div class="language-SQL codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-SQL codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">select count(*),product_no   FROM ods.demo_tbl where dt=&#x27;2023-03-09&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">group by product_no;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>Query in Doris using Hive Catalog, response time: 58.531 seconds</li></ul><div class="language-SQL codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-SQL codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">set exec_mem_limit=16Gï¼›</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select count(*),product_no   FROM hive.ods.demo_tbl where dt=&#x27;2023-03-09&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">group by product_no;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>Query in Doris directly, response time: 0.828 seconds</li></ul><div class="language-SQL codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-SQL codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">select count(*),product_no   FROM ods.demo_tbl where dt=&#x27;2023-03-09&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">group by product_no;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol><li><strong>Faster data ingestion</strong></li></ol><p><strong>Before</strong>: The original table in Hive had 40 fields. It was divided into partitions by day, with 1.1 billion pieces of data per partition. It takes <strong>806G</strong> to store in HDFS.</p><p><strong>After</strong>: For data synchronization, we call Spark on YARN using SeaTunnel. It can be finished within 11 minutes (100 million pieces per minute ), and the ingested data only takes up <strong>378G</strong> of storage space.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/RDM_7-aabcb97d311b9da69a1d8722339b633a.png" width="1280" height="463" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary">Summary<a href="#summary" class="hash-link" aria-label="Direct link to Summary" title="Direct link to Summary">â€‹</a></h2><p>The key step to building a high-performing risk data mart is to leverage the Multi Catalog feature of Apache Doris to unify the heterogenous data sources. This not only increases our query speed but also solves a lot of the problems coming with our previous data architecture.</p><ol><li>Deploying Apache Doris allows us to decouple daily batch processing workloads with ad-hoc queries, so they don&#x27;t have to compete for resources. This reduces the query response time from minutes to seconds.</li><li>We used to build our data ingestion interface based on Elasticsearch clusters, which could lead to garbage collection jitter when transferring large batches of offline data. When we stored the interface service dataset on Doris, no jitter was found during data writing and we were able to transfer 10 million rows within 10 minutes.</li><li>Apache Doris has been optimizing itself in many scenarios including flat tables. As far as we know, compared with ClickHouse, Apache Doris 1.2 is twice as fast in SSB-Flat-table benchmark and dozens of times faster in TPC-H benchmark.</li><li>In terms of cluster scaling and updating, we used to suffer from a big window of restoration time after configuration revision. But Doris supports hot swap and easy scaling out, so we can reboot nodes within a few seconds and minimize interruption to users caused by cluster scaling.</li></ol><p>(One last piece of advice for you: If you encounter any problems with deploying Apache Doris, don&#x27;t hesitate to contact the Doris community for help, they and a bunch of SelectDB engineers will be more than happy to make your adaption journey quick and easy.)</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Tencent Music">Tencent Data Engineer: Why We Go from ClickHouse to Apache Doris?</a></h2><div class="blog-info"><time datetime="2023-03-07T00:00:00.000Z" itemprop="datePublished">March 7, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Jun Zhang &amp; Kai Dai</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME-7ebdc46ff19cf90eaf92e280c1b1f0e4.png" width="900" height="383" class="img_ev3q"></p><p>This article is co-written by me and my colleague Kai Dai. We are both data platform engineers at Tencent Music (NYSE: TME), a music streaming service provider with a whopping 800 million monthly active users. To drop the number here is not to brag but to give a hint of the sea of data that my poor coworkers and I have to deal with everyday.</p><h1>What We Use ClickHouse For?</h1><p>The music library of Tencent Music contains data of all forms and types: recorded music, live music, audios, videos, etc. As data platform engineers, our job is to distill information from the data, based on which our teammates can make better decisions to support our users and musical partners.</p><p>Specifically, we do all-round analysis of the songs, lyrics, melodies, albums, and artists, turn all this information into data assets, and pass them to our internal data users for inventory counting, user profiling, metrics analysis, and group targeting.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_1-73b51a1362dc4f6f1cadbee5d51aaa05.png" width="1280" height="693" class="img_ev3q"></p><p>We stored and processed most of our data in Tencent Data Warehouse (TDW), an offline data platform where we put the data into various tag and metric systems and then created flat tables centering each object (songs, artists, etc.).</p><p>Then we imported the flat tables into ClickHouse for analysis and Elasticsearch for data searching and group targeting.</p><p>After that, our data analysts used the data under the tags and metrics they needed to form datasets for different usage scenarios, during which they could create their own tags and metrics.</p><p>The data processing pipeline looked like this:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_2-edb671e5b547ca431f4eaa61b59fd2fb.png" width="1280" height="743" class="img_ev3q"></p><h1>The Problems with ClickHouse</h1><p>When working with the above pipeline, we encountered a few difficulties:</p><ol><li><strong>Partial Update</strong>: Partial update of columns was not supported. Therefore, any latency from any one of the data sources could delay the creation of flat tables, and thus undermine data timeliness.</li><li><strong>High storage cost</strong>: Data under different tags and metrics was updated at different frequencies. As much as ClickHouse excelled in dealing with flat tables, it was a huge waste of storage resources to just pour all data into a flat table and partition it by day, not to mention the maintenance cost coming with it.</li><li><strong>High maintenance cost</strong>: Architecturally speaking, ClickHouse was characterized by the strong coupling of storage nodes and compute nodes. Its components were heavily interdependent, adding to the risks of cluster instability. Plus, for federated queries across ClickHouse and Elasticsearch, we had to take care of a huge amount of connection issues. That was just tedious.</li></ol><h1>Transition to Apache Doris</h1><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a>, a real-time analytical database, boasts a few features that are exactly what we needed in solving our problems:</p><ol><li><strong>Partial update</strong>: Doris supports a wide variety of data models, among which the Aggregate Model supports real-time partial update of columns. Building on this, we can directly ingest raw data into Doris and create flat tables there. The ingestion goes like this: Firstly, we use Spark to load data into Kafka; then, any incremental data will be updated to Doris and Elasticsearch via Flink. Meanwhile, Flink will pre-aggregate the data so as to release burden on Doris and Elasticsearch.</li><li><strong>Storage cost</strong>: Doris supports multi-table join queries and federated queries across Hive, Iceberg, Hudi, MySQL, and Elasticsearch. This allows us to split the large flat tables into smaller ones and partition them by update frequency. The benefits of doing so include a relief of storage burden and an increase of query throughput.</li><li><strong>Maintenance cost</strong>: Doris is of simple architecture and is compatible with MySQL protocol. Deploying Doris only involves two processes (FE and BE) with no dependency on other systems, making it easy to operate and maintain. Also, Doris supports querying external ES data tables. It can easily interface with the metadata in ES and automatically map the table schema from ES so we can conduct queries on Elasticsearch data via Doris without grappling with complex connections.</li></ol><p>Whatâ€™s more, Doris supports multiple data ingestion methods, including batch import from remote storage such as HDFS and S3, data reads from MySQL binlog and Kafka, and real-time data synchronization or batch import from MySQL, Oracle, and PostgreSQL. It ensures service availability and data reliability through a consistency protocol and is capable of auto debugging. This is great news for our operators and maintainers.</p><p>Statistically speaking, these features have cut our storage cost by 42% and development cost by 40%.</p><p>During our usage of Doris, we have received lots of support from the open source Apache Doris community and timely help from the SelectDB team, which is now running a commercial version of Apache Doris.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_3-877f2cc02538dcf78f20d08c679df9f3.png" width="1280" height="734" class="img_ev3q"></p><h1>Further Improvement to Serve Our Needs</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="introduce-a-semantic-layer">Introduce a Semantic Layer<a href="#introduce-a-semantic-layer" class="hash-link" aria-label="Direct link to Introduce a Semantic Layer" title="Direct link to Introduce a Semantic Layer">â€‹</a></h2><p>Speaking of the datasets, on the bright side, our data analysts are given the liberty of redefining and combining the tags and metrics at their convenience. But on the dark side, high heterogeneity of the tag and metric systems leads to more difficulty in their usage and management.</p><p>Our solution is to introduce a semantic layer in our data processing pipeline. The semantic layer is where all the technical terms are translated into more comprehensible concepts for our internal data users. In other words, we are turning the tags and metrics into first-class citizens for data definement and management.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_4-f78029c9a317de442e0e00aac053140a.png" width="1280" height="743" class="img_ev3q"></p><p><strong>Why would this help?</strong></p><p>For data analysts, all tags and metrics will be created and shared at the semantic layer so there will be less confusion and higher efficiency.</p><p>For data users, they no longer need to create their own datasets or figure out which one is applicable for each scenario but can simply conduct queries on their specified tagset and metricset.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="upgrade-the-semantic-layer">Upgrade the Semantic Layer<a href="#upgrade-the-semantic-layer" class="hash-link" aria-label="Direct link to Upgrade the Semantic Layer" title="Direct link to Upgrade the Semantic Layer">â€‹</a></h2><p>Explicitly defining the tags and metrics at the semantic layer was not enough. In order to build a standardized data processing system, our next goal was to ensure consistent definition of tags and metrics throughout the whole data processing pipeline.</p><p>For this sake, we made the semantic layer the heart of our data management system:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_5-69933329bfdc217369664b15c2ec4766.png" width="1280" height="714" class="img_ev3q"></p><p><strong>How does it work?</strong></p><p>All computing logics in TDW will be defined at the semantic layer in the form of a single tag or metric.</p><p>The semantic layer receives logic queries from the application side, selects an engine accordingly, and generates SQL. Then it sends the SQL command to TDW for execution. Meanwhile, it might also send configuration and data ingestion tasks to Doris and decide which metrics and tags should be accelerated.</p><p>In this way, we have made the tags and metrics more manageable. A fly in the ointment is that since each tag and metric is individually defined, we are struggling with automating the generation of a valid SQL statement for the queries. If you have any idea about this, you are more than welcome to talk to us.</p><h1>Give Full Play to Apache Doris</h1><p>As you can see, Apache Doris has played a pivotal role in our solution. Optimizing the usage of Doris can largely improve our overall data processing efficiency. So in this part, we are going to share with you what we do with Doris to accelerate data ingestion and queries and reduce costs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-we-want">What We Want?<a href="#what-we-want" class="hash-link" aria-label="Direct link to What We Want?" title="Direct link to What We Want?">â€‹</a></h2><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_6-d0c8cb8b9a7501650f26ae3018b58b14.png" width="1280" height="444" class="img_ev3q"></p><p>Currently, we have 800+ tags and 1300+ metrics derived from the 80+ source tables in TDW.</p><p>When importing data from TDW to Doris, we hope to achieve:</p><ul><li><strong>Real-time availability:</strong> In addition to the traditional T+1 offline data ingestion, we require real-time tagging.</li><li><strong>Partial update</strong>: Each source table generates data through its own ETL task at various paces and involves only part of the tags and metrics, so we require the support for partial update of columns.</li><li><strong>High performance</strong>: We need a response time of only a few seconds in group targeting, analysis and reporting scenarios.</li><li><strong>Low costs</strong>: We hope to reduce costs as much as possible.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="what-we-do">What We Do?<a href="#what-we-do" class="hash-link" aria-label="Direct link to What We Do?" title="Direct link to What We Do?">â€‹</a></h2><ol><li><strong>Generate Flat Tables in Flink Instead of TDW</strong></li></ol><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_7-6ec720f226a737d5cf91c74a386319b4.png" width="1280" height="567" class="img_ev3q"></p><p>Generating flat tables in TDW has a few downsides:</p><ul><li><strong>High storage cost</strong>: TDW has to maintain an extra flat table apart from the discrete 80+ source tables. Thatâ€™s huge redundancy.</li><li><strong>Low real-timeliness</strong>: Any delay in the source tables will be augmented and retard the whole data link.</li><li><strong>High development cost</strong>: To achieve real-timeliness would require extra development efforts and resources.</li></ul><p>On the contrary, generating flat tables in Doris is much easier and less expensive. The process is as follows:</p><ul><li>Use Spark to import new data into Kafka in an offline manner.</li><li>Use Flink to consume Kafka data.</li><li>Create a flat table via the primary key ID.</li><li>Import the flat table into Doris.</li></ul><p>As is shown below, Flink has aggregated the five lines of data, of which â€œIDâ€=1, into one line in Doris, reducing the data writing pressure on Doris.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_8-c5c8e4d117fb6c1157c42f6ab14829e0.png" width="1280" height="622" class="img_ev3q"></p><p>This can largely reduce storage costs since TDW no long has to maintain two copies of data and KafKa only needs to store the new data pending for ingestion. Whatâ€™s more, we can add whatever ETL logic we want into Flink and reuse lots of development logic for offline and real-time data ingestion.</p><p><strong>2. Name the Columns Smartly</strong></p><p>As we mentioned, the Aggregate Model of Doris allows partial update of columns. Here we provide a simple introduction to other data models in Doris for your reference:</p><p><strong>Unique Model</strong>: This is applicable for scenarios requiring primary key uniqueness. It only keeps the latest data of the same primary key ID. (As far as we know, the Apache Doris community is planning to include partial update of columns in the Unique Model, too.)</p><p><strong>Duplicate Model</strong>: This model stores all original data exactly as it is without any pre-aggregation or deduplication.</p><p>After determining the data model, we had to think about how to name the columns. Using the tags or metrics as column names was not a choice because:</p><p>I. Our internal data users might need to rename the metrics or tags, but Doris 1.1.3 does not support modification of column names.</p><p>II. Tags might be taken online and offline frequently. If that involves the adding and dropping of columns, it will be not only time-consuming but also detrimental to query performance.</p><p>Instead, we do the following:</p><ul><li><strong>For flexible renaming of tags and metrics</strong>, we use MySQL tables to store the metadata (name, globally unique ID, status, etc.). Any change to the names will only happen in the metadata but will not affect the table schema in Doris. For example, if a <code>song_name</code> is given an ID of 4, it will be stored with the column name of a4 in Doris. Then if the <code>song_name</code>is involved in a query, it will be converted to a4 in SQL.</li><li><strong>For the onlining and offlining of tags</strong>, we sort out the tags based on how frequently they are being used. The least used ones will be given an offline mark in their metadata. No new data will be put under the offline tags but the existing data under those tags will still be available.</li><li><strong>For real-time availability of newly added tags and metrics</strong>, we prebuild a few ID columns in Doris tables based on the mapping of name IDs. These reserved ID columns will be allocated to the newly added tags and metrics. Thus, we can avoid table schema change and the consequent overheads. Our experience shows that only 10 minutes after the tags and metrics are added, the data under them can be available.</li></ul><p>Noteworthily, the recently released Doris 1.2.0 supports Light Schema Change, which means that to add or remove columns, you only need to modify the metadata in FE. Also, you can rename the columns in data tables as long as you have enabled Light Schema Change for the tables. This is a big trouble saver for us.</p><p><strong>3. Optimize Date Writing</strong></p><p>Here are a few practices that have reduced our daily offline data ingestion time by 75% and our CUMU compaction score from 600+ to 100.</p><ul><li>Flink pre-aggregation: as is mentioned above.</li><li>Auto-sizing of writing batch: To reduce Flink resource usage, we enable the data in one Kafka Topic to be written into various Doris tables and realize the automatic alteration of batch size based on the data amount.</li><li>Optimization of Doris data writing: fine-tune the the sizes of tablets and buckets as well as the compaction parameters for each scenario:</li></ul><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">max_XXXX_compaction_thread</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max_cumulative_compaction_num_singleton_deltas</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>Optimization of the BE commit logic: conduct regular caching of BE lists, commit them to the BE nodes batch by batch, and use finer load balancing granularity.</li></ul><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_9-f599364617a05d42a19e5430e500d6f7.png" width="1280" height="511" class="img_ev3q"></p><p><strong>4. Use Dori-on-ES in Queries</strong></p><p>About 60% of our data queries involve group targeting. Group targeting is to find our target data by using a set of tags as filters. It poses a few requirements for our data processing architecture:</p><ul><li>Group targeting related to APP users can involve very complicated logic. That means the system must support hundreds of tags as filters simultaneously.</li><li>Most group targeting scenarios only require the latest tag data. However, metric queries need to support historical data.</li><li>Data users might need to perform further aggregated analysis of metric data after group targeting.</li><li>Data users might also need to perform detailed queries on tags and metrics after group targeting.</li></ul><p>After consideration, we decided to adopt Doris-on-ES. Doris is where we store the metric data for each scenario as a partition table, while Elasticsearch stores all tag data. The Doris-on-ES solution combines the distributed query planning capability of Doris and the full-text search capability of Elasticsearch. The query pattern is as follows:</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">SELECT tag, agg(metric) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   FROM Doris </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   WHERE id in (select id from Es where tagFilter)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   GROUP BY tag</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>As is shown, the ID data located in Elasticsearch will be used in the sub-query in Doris for metric analysis.</p><p>In practice, we find that the query response time is related to the size of the target group. If the target group contains over one million objects, the query will take up to 60 seconds. If it is even larger, a timeout error might occur.</p><p>After investigation, we identified our two biggest time wasters:</p><p>I. When Doris BE pulls data from Elasticsearch (1024 lines at a time by default), for a target group of over one million objects, the network I/O overhead can be huge.</p><p>II. After the data pulling, Doris BE needs to conduct Join operations with local metric tables via SHUFFLE/BROADCAST, which can cost a lot.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_10-b177da2c3e9ab23ad3fb8e1784012442.png" width="1280" height="883" class="img_ev3q"></p><p>Thus, we make the following optimizations:</p><ul><li>Add a query session variable <code>es_optimize</code> that specifies whether to enable optimization.</li><li>In data writing into ES, add a BK column to store the bucket number after the primary key ID is hashed. The algorithm is the same as the bucketing algorithm in Doris (CRC32).</li><li>Use Doris BE to generate a Bucket Join execution plan, dispatch the bucket number to BE ScanNode and push it down to ES.</li><li>Use ES to compress the queried data; turn multiple data fetch into one and reduce network I/O overhead.</li><li>Make sure that Doris BE only pulls the data of buckets related to the local metric tables and conducts local Join operations directly to avoid data shuffling between Doris BEs.</li></ul><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/TME_11-5ac5f455cdcab0a0b8b1207d61b24afb.png" width="1280" height="924" class="img_ev3q"></p><p>As a result, we reduce the query response time for large group targeting from 60 seconds to a surprising 3.7 seconds.</p><p>Community information shows that Doris is going to support inverted indexing since version 2.0.0, which is soon to be released. With this new version, we will be able to conduct full-text search on text types, equivalence or range filtering of texts, numbers, and datetime, and conveniently combine AND, OR, NOT logic in filtering since the inverted indexing supports array types. This new feature of Doris is expected to deliver 3~5 times better performance than Elasticsearch on the same task.</p><p><strong>5. Refine the Management of Data</strong></p><p>Dorisâ€™ capability of cold and hot data separation provides the foundation of our cost reduction strategies in data processing.</p><ul><li>Based on the TTL mechanism of Doris, we only store data of the current year in Doris and put the historical data before that in TDW for lower storage cost.</li><li>We vary the numbers of copies for different data partitions. For example, we set three copies for data of the recent three months, which is used frequently, one copy for data older than six months, and two copies for data in between.</li><li>Doris supports turning hot data into cold data so we only store data of the past seven days in SSD and transfer data older than that to HDD for less expensive storage.</li></ul><h1>Conclusion</h1><p>Thank you for scrolling all the way down here and finishing this long read. Weâ€™ve shared our cheers and tears, lessons learned, and a few practices that might be of some value to you during our transition from ClickHouse to Doris. We really appreciate the help from the Apache Doris community and the <a href="https://selectdb.com" target="_blank" rel="noopener noreferrer">SelectDB</a> team, but we might still be chasing them around for a while since we attempt to realize auto-identification of cold and hot data, pre-computation of frequently used tags/metrics, simplification of code logic using Materialized Views, and so on and so forth.</p><p><strong># Links</strong></p><p><strong>SelectDB</strong>:</p><p><a href="https://selectdb.com" target="_blank" rel="noopener noreferrer">https://selectdb.com</a> </p><p><strong>Apache Doris</strong>:</p><p><a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p><strong>Apache Doris Github</strong>:</p><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Duyansoft">Best Practice in Duyansoft, Improving Query Speed to Make the Most out of Your Data</a></h2><div class="blog-info"><time datetime="2023-02-27T00:00:00.000Z" itemprop="datePublished">February 27, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Junfei Liu</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>Author: Junfei Liu, Senior Architect of Duyansoft</p></blockquote><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyansoft-338cbc4c47491d4110145175cfa2d0ba.png" width="900" height="383" class="img_ev3q"></p><p>The world is getting more and more value out of data, as exemplified by the currently much-talked-about ChatGPT, which I believe is a robotic data analyst. However, in todayâ€™s era, whatâ€™s more important than the data itself is the ability to locate your wanted information among all the overflowing data quickly. So in this article, I will talk about how I improved overall data processing efficiency by optimizing the choice and usage of data warehouses.</p><h1>Too Much Data on My Plate</h1><p>The choice of data warehouses was never high on my worry list until 2021. I have been working as a data engineer for a Fintech SaaS provider since its incorporation in 2014. In the companyâ€™s infancy, we didnâ€™t have too much data to juggle. We only needed a simple tool for OLTP and business reporting, and the traditional databases would cut the mustard.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyan_1-be681a0c4e3b94cdca6f6476698be732.png" width="1466" height="590" class="img_ev3q"></p><p>But as the company grew, the data we received became overwhelmingly large in volume and increasingly diversified in sources. Every day, we had tons of user accounts logging in and sending myriads of requests. It was like collecting water from a thousand taps to put out a million scattered pieces of fire in a building, except that you must bring the exact amount of water needed for each fire spot. Also, we got more and more emails from our colleagues asking if we could make data analysis easier for them. Thatâ€™s when the company assembled a big data team to tackle the beast.</p><p>The first thing we did was to revolutionize our data processing architecture. We used DataHub to collect all our transactional or log data and ingest it into an offline data warehouse for data processing (analyzing, computing. etc.). Then the results would be exported to MySQL and then forwarded to QuickBI to display the reports visually. We also replaced MongoDB with a real-time data warehouse for business queries.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyan_2-e87f8780d1f9b74df15d81a94718b378.png" width="1564" height="704" class="img_ev3q"></p><p>This new architecture worked, but there remained a few pebbles in our shoes:</p><ul><li><strong>We wanted faster responses.</strong> MySQL could be slow in aggregating large tables, but our product guys requested a query response time of fewer than five seconds. So first, we tried to optimize MySQL. Then we also tried to skip MySQL and directly connect the offline data warehouse with QuickBI, hoping that the combination of query acceleration capability of the former and caching of the latter would do the magic. Still, that five-second goal seemed to be unreachable. There was a time when I believed the only perfect solution was for the product team to hire people with more patience.</li><li><strong>We wanted less pain in maintaining dimension tables.</strong> The offline data warehouse conducted data synchronization every five minutes, making it not applicable for frequent data updates or deletions scenarios. If we needed to maintain dimension tables in it, we would have to filter and deduplicate the data regularly to ensure data consistency. Out of our trouble-averse instinct, we chose not to do so.</li><li><strong>We wanted support for point queries of high concurrency.</strong> The real-time database that we previously used required up to 500ms to respond to highly concurrent point queries in both columnar storage and row storage, even after optimization. That was not good enough.</li></ul><h1>Hit It Where It Hurts Most</h1><p>In March, 2022, we started our hunt for a better data warehouse. To our disappointment, there was no one-size-fits-all solution. Most of the tools we looked into were only good at one or a few of the tasks, but if we gathered the best performer for each usage scenario, that would add up to a heavy and messy toolkit, which was against instinct.</p><p>So we decided to solve our biggest headache first: slow response, as it was hurting both the experience of our users and our internal work efficiency.</p><p>To begin with, we tried to move the largest tables from MySQL to <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a>, a real-time analytical database that supports MySQL protocol. That reduced the query execution time by a factor of eight. Then we tried and used Doris to accommodate more data.</p><p>As for now, we are using two Doris clusters: one to handle point queries (high QPS) from our users and the other for internal ad-hoc queries and reporting. As a result, users have reported smoother experience and we can provide more features that are used to be bottlenecked by slow query execution. Moving our dimension tables to Doris also brought less data errors and higher development efficiency.</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyan_3-0abe8037381914932a2d763843a2ed34.png" width="1356" height="864" class="img_ev3q"></p><p>Both the FE and BE processes of Doris can be scaled out, so tens of PBs of data stored in hundreds of devices can be put into one single cluster. In addition, the two types of processes implement a consistency protocol to ensure service availability and data reliability. This removes dependency on Hadoop and thus saves us the cost of deploying Hadoop clusters.</p><h1>Tips</h1><p>Here are a few of our practices to share with you:</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-model"><strong>Data Model:</strong><a href="#data-model" class="hash-link" aria-label="Direct link to data-model" title="Direct link to data-model">â€‹</a></h2><p>Out of the three Doris data models, we find the Unique Model and the Aggregate Model suit our needs most. For example, we use the Unique Model to ensure data consistency while ingesting dimension tables and original tables and the Aggregate Model to import report data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-ingestion"><strong>Data Ingestion:</strong><a href="#data-ingestion" class="hash-link" aria-label="Direct link to data-ingestion" title="Direct link to data-ingestion">â€‹</a></h2><p>For real-time data ingestion, we use the Flink-Doris-Connector: After our business data, the MySQL-based binlogs, is written into Kafka, it will be parsed by Flink and then loaded into Doris in a real-time manner.</p><p>For offline data ingestion, we use DataX: This mainly involves the computed report data in our offline data warehouse.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-management"><strong>Data Management:</strong><a href="#data-management" class="hash-link" aria-label="Direct link to data-management" title="Direct link to data-management">â€‹</a></h2><p>We back up our cluster data in a remote storage system via Broker. Then, it can restore the data from the backups to any Doris cluster if needed via the restore command.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="monitoring-and-alerting"><strong>Monitoring and Alerting:</strong><a href="#monitoring-and-alerting" class="hash-link" aria-label="Direct link to monitoring-and-alerting" title="Direct link to monitoring-and-alerting">â€‹</a></h2><p>In addition to the various monitoring metrics of Doris, we deployed an audit log plugin to keep a closer eye on certain slow SQL of certain users for optimization.</p><p>Slow SQL queries:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyan_4-4c0a444296e8e0489a68597c56a23c51.png" width="1080" height="437" class="img_ev3q"></p><p>Some of our often-used monitoring metrics:</p><p><img loading="lazy" src="https://cdnd.selectdb.com/assets/images/Duyan_5-fab738ac780df0ae000a0a7238093e35.png" width="1080" height="451" class="img_ev3q"></p><p><strong>Tradeoff Between Resource Usage and Real-Time Availability:</strong></p><p>It turned out that using Flink-Doris-Connector for data ingestion can result in high cluster resource usage, so we increased the interval between each data writing from 3s to 10 or 20s, compromising a little bit on the real-time availability of data in exchange for much less resource usage.</p><h1>Communication with Developers</h1><p>We have been in close contact with the open source Doris community all the way from our investigation to our adoption of the data warehouse, and weâ€™ve provided a few suggestions to the developers:</p><ul><li>Enable Flink-Doris-Connector to support simultaneous writing of multiple tables in a single sink.</li><li>Enable Materialized Views to support Join of multiple tables.</li><li>Optimize the underlying compaction of data and reduce resource usage as much as possible.</li><li>Provide optimization suggestions for slow SQL and warnings for abnormal table creation behaviors.</li></ul><p>If the perfect data warehouse is not there to be found, I think providing feedback for the second best is a way to help make one. We are also looking into its commercialized version called SelectDB to see if more custom-tailored advanced features can grease the wheels.</p><h1>Conclusion</h1><p>As we set out to find a single data warehouse that could serve all our needs, we ended up finding something less than perfect but good enough to improve our query speed by a wide margin and discovered some surprising features of it along the way. So if you wiggle between different choices, you may bet on the one with the thing you want most badly, and taking care of the rest wouldnâ€™t be so hard.</p><p><strong>Try</strong> <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer"><strong>Apache Doris</strong></a> <strong>out!</strong></p><p>It is an open source real-time analytical database based on MPP architecture. It supports both high-concurrency point queries and high-throughput complex analysis. Or you can start your free trial of <a href="https://en.selectdb.com/" target="_blank" rel="noopener noreferrer"><strong>SelectDB</strong></a>, a cloud-native real-time data warehouse developed based on the Apache Doris open source project by the same key developers.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/linkedcare">ClickHouse &amp; Kudu to Doris: 10X Concurrency Increased, 70% Latency Down</a></h2><div class="blog-info"><time datetime="2023-01-28T00:00:00.000Z" itemprop="datePublished">January 28, 2023</time><span class="split-line"></span><span class="authors"><span class="s-author">Yi Yang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-c9c4b972a14903911ba1674b76f5edca.png" width="900" height="383" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="author">Author:<a href="#author" class="hash-link" aria-label="Direct link to Author:" title="Direct link to Author:">â€‹</a></h2><p>YiYang, Senior Big Data Developer, Linkedcare</p><h1>About Linkedcare</h1><p>Linkedcare is a leading SaaS software company in the health technology industry, focusing on the medical dental and cosmetic plastic surgery. In 2021, it was selected as one of the top 150 digital healthcare companies in the world by CB Insights. Linkedcare has served thousands of plastic surgery institutions in Los Angeles, Taiwan, and Hong Kong. Linkedcare also provides integrated management system services for dental clinics, covering electronic medical records, customer relationship management, intelligent marketing, B2B trading platform, insurance payment, BI tools, etc.</p><h1>Doris&#x27; Evolution in Linkedcare</h1><p>Let me briefly introduce Doris&#x27;s development in Linkedcare first. In general, the application of Doris in Linkedcare can be divided into two stages:</p><ol><li>The value-added report provided by Linkedcare to customers was initially provided by ClickHouse, which was later replaced by Apache Doris;</li><li>Due to the continuous improvement of real-time data analysis requirements, T+1&#x27;s data reporting gradually cannot meet business needs. Linkedcare needs a data warehouse that can handle real-time processing, and Doris has been introduced into the company&#x27;s data warehouse since then. With the support of the Apache Doris community and the SelectDB professional technical team, our business data has been gradually migrated from Kudu to Doris.</li></ol><p><img loading="lazy" alt="1" src="https://cdnd.selectdb.com/assets/images/1-39a723280720a07dc2ed0a7de5c99c9b.png" width="1696" height="866" class="img_ev3q"></p><h1>Data Service Architecture: From ClickHouse to Doris</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-service-architecture-requirements">Data Service Architecture Requirements<a href="#data-service-architecture-requirements" class="hash-link" aria-label="Direct link to Data Service Architecture Requirements" title="Direct link to Data Service Architecture Requirements">â€‹</a></h2><ul><li>Support complex queries: When customers do self-service on the dashboard, a complex SQL query statement will be generated to directly query the database, and the complexity of the statement is unknown, which adds a lot of pressure on the database and affects query performance.</li><li>High concurrency and low latency: At least 100 concurrent queries can be supported, and query results can be return within 1 second;</li><li>Real-time data update: The report data comes from the SaaS system. When the customer modifies the historical data in the system, the report data must be changed accordingly to ensure consistentency, which requires real-time processing.</li><li>Low cost and easy deployment: There are a lot of private cloud customers in our SaaS business. In order to reduce labor costs, the business requires that the architecture deployment and operation and maintenance be simple enough.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="early-problems-found-clickhouse-shuts-down-when-high-concurrency-occurs">Early Problems Found: ClickHouse Shuts Down When High-concurrency Occurs<a href="#early-problems-found-clickhouse-shuts-down-when-high-concurrency-occurs" class="hash-link" aria-label="Direct link to Early Problems Found: ClickHouse Shuts Down When High-concurrency Occurs" title="Direct link to Early Problems Found: ClickHouse Shuts Down When High-concurrency Occurs">â€‹</a></h2><p>The previous project chose ClickHouse to provide data query services, but serious concurrency problems occurred during use:
10 concurrent queries will cause ClickHouse to shut down, resulting in the inability to provide services to customers normally, which is the direct reason for us to replace ClickHouse.</p><p>In addition, there are several severe problems:</p><ol><li>The cost of ClickHouse services on the cloud is very high, and the dependency on ClickHouse components is relatively high. The frequent interaction between ClickHouse and Zookeeper during data ingestion will put greater pressure on stability.</li><li>How to seamlessly migrate data without affecting the normal use of customers is another problem.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="selection-between-doris-clickhouse-and-kudu">Selection between Doris, Clickhouse and Kudu<a href="#selection-between-doris-clickhouse-and-kudu" class="hash-link" aria-label="Direct link to Selection between Doris, Clickhouse and Kudu" title="Direct link to Selection between Doris, Clickhouse and Kudu">â€‹</a></h2><p>To deal with the existing problems and meet the business requirements, we decided to conduct research on Doris (0.14), Clickhouse, and Kudu respectively.</p><p><img loading="lazy" alt="2" src="https://cdnd.selectdb.com/assets/images/2-bd04a72816c9ff95512e08d3f6e8e05f.png" width="1600" height="454" class="img_ev3q"></p><p>As shown in the table above, we made a deep comparison of these 3 databases. And we can see that Doris has excellent performance in many aspects:</p><ul><li>High concurrency: Doris can handle high-concurrency of 1,000 and more. So it will easily solve the problem of 10 concurrent queries which led ClickHouse to shut down.</li><li>Query performance: Doris can achieve millisecond-level query response. In single-table query, although Doris and ClickHouse are almost equivalent in query performance, in multi-table query, Doris is far better than ClickHouse. Doris can make sure that the QPS won&#x27;t drop when high-concurrency happens.</li><li>Data update: Doris&#x27; data model can meet our needs for data update to ensure the consistency of system data and business data, which will be described in detail below.</li><li>Ease of use: Doris has a flat architecture, simple and fast deployment, fully-completed data ingest functions, and good at scaling out; At the same time, Doris can automatically perform replica balancing internally, and the operation and maintenance cost is extremely low. However, ClickHouse and Kudu rely heavily on components and require a lot of preparatory work for use. This requires a professional team to handle a large number of daily operation and maintenance tasks.</li><li>Standard SQL: Doris is compatible with the MySQL protocol and uses standard SQL. It is easy for developers to get started and does not require additional learning costs.</li><li>Distributed JOINs: Doris supports distributed JOINs, but ClickHouse has limitations in JOIN queries and functions as well as poor maintainability.</li><li>Active community: The Apache Doris open source community is active with passion. At the same time, SelectDB provides a professional and full-time team for technical support for the Doris community. If you encounter problems, you can directly contact the community and find out a solution in time.</li></ul><p>From the above research, we can find that Doris has excellent capabilities in all aspects and is very in line with our needs. Therefore, we adopt Doris instead of ClickHouse, which solves the problems of poor concurrency and the shutdown of ClickHouse.</p><h1>Data Warehouse Architecture: From Kudu+Impala to Doris</h1><p>In the process of using data reports, we have gradually discovered many advantages of Doris, so we decided to introduce Doris to the company&#x27;s data warehouse.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-warehouse-architecture-requirements">Data Warehouse Architecture Requirements<a href="#data-warehouse-architecture-requirements" class="hash-link" aria-label="Direct link to Data Warehouse Architecture Requirements" title="Direct link to Data Warehouse Architecture Requirements">â€‹</a></h2><ul><li>When the customer modifies the historical data in the system, the report data should also be changed accordingly. At the same time, there should be a feature that can help customers to change the value of a single column;</li><li>When Flink extracts the full amount of data from the business database and writes it into the data warehouse frequently, the version compaction must keep up with the speed of new version generation, and will not cause version accumulation;</li><li>Through resource isolation and other functions, Doris reduces the possibility of resource preemption, improves resource utilization, and makes full use of resources on the core computing nodes;</li><li>Due to the limited memory resources in the company, overloaded tasks must be completed without increasing the number of clusters.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="early-problems-found-kuduimpala-underperforms">Early Problems Found: Kudu+Impala Underperforms<a href="#early-problems-found-kuduimpala-underperforms" class="hash-link" aria-label="Direct link to Early Problems Found: Kudu+Impala Underperforms" title="Direct link to Early Problems Found: Kudu+Impala Underperforms">â€‹</a></h2><p>The early company data warehouse architecture used Kudu and Impala for computing and storage. But we found the following problems during use:</p><ol><li>When the number of concurrent queries (QPS) is large, the simple query response time of Kudu+Impala is always more than a few seconds, which cannot reach the millisecond-level required by the business. The long waiting time has brought bad user experience to customers. </li><li>The Kudu+Impala engine cannot perform incremental aggregation of factual data, and can barely support real-time data analysis.</li><li>Kudu relies on a large number of primary key lookups when ingesting data. The batch processing efficiency is low and Kudu consumes a lot of CPU, which is not friendly to resource utilization.</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-data-warehouse-architecture-design-based-on-doris">New Data Warehouse Architecture Design Based on Doris<a href="#new-data-warehouse-architecture-design-based-on-doris" class="hash-link" aria-label="Direct link to New Data Warehouse Architecture Design Based on Doris" title="Direct link to New Data Warehouse Architecture Design Based on Doris">â€‹</a></h2><p><img loading="lazy" alt="3" src="https://cdnd.selectdb.com/assets/images/3-e7990ac868e7345d5fda0512b0ec6b8c.png" width="1280" height="690" class="img_ev3q"></p><p>As shown in the figure above, Apache Doris is used in the new architecture and is responsible for data warehouse storage and computing; Data ingestion of real-time data and ODS data through Kafka has been replaced with Flink; We use Duckula as our stream computing platform; While we introduce DolphinSchedular for our task scheduling.</p><h1>Benefits of the new architecture based on Apache Doris:</h1><ul><li>The new data warehouse architecture based on Doris no longer depends on Hadoop related components, and the operation and maintenance cost is low.</li><li>Higher performance. Doris uses less server resources but provides stronger data processing capabilities;</li><li>Doris supports high concurrency and can directly support WebApp query services;</li><li>Doris supports the access to external tables, which enable easy data publishing and data ingestion;</li><li>Doris supports dynamic scaling out and automatic data balance;</li><li>Doris supports multiple federated queries, including Hive, ES, MySQL, etc.;</li><li>Doris&#x27; Aggregate Model supports users updating a single column;</li><li>By adjusting BE parameters and cluster size, the problem of version accumulation can be effectively solved;</li><li>Through the Resource Tag and Query Block function, cluster resource isolation can be realized, resource usage rate can be reduced, and query performance can be improved.</li></ul><p>Thanks to the excellent capabilities of the new architecture, the cluster we use has been reduced from 18 pieces of 16Cores 128G to 12 pieces of 16Cores 128G, saving up to 33% of resources compared to before; Further, the computing performance has been greatly improved. Doris can complete an ETL task that was completed in 3 hours on Kudu in only 1 hour. In addition, in frequent updates, Kudu&#x27;s internal data fragmentation files cannot be automatically merged so that the performance will become worse and worse, requiring regular rebuilding; While the compaction function of Doris can effectively solve this problem.</p><h1>Highly Recommended</h1><p>The cost of using Doris is very low. Only 3 low-end servers or even desktops can be used to deploy easily a data warehouse based on Apache Doris; For enterprises with limited investment and do not want to be left behind by the market, it is highly recommended to try Apache Doris.</p><p>Doris is also a mature analytical database with MPP architecture. At the same time, its community is very active and easy to communicate with. SelectDB, the commercial company behind Doris, has set up a full-time technical team for the community. Any questions can be answered within 1 hour. In the last year, the community has been continuously promoted by SelectDB and introduced a series of industry-leading new features. In addition, the community will seriously consider the user habits when iterating, which will bring a lot of convenience.</p><p>I really appreciate the full support from the Doris community and the SelectDB team. And I sincerely recommend developers and enterprises to start with Apache Doris today.</p><h1>Apache Doris</h1><p>Apache Doris is a real-time analytical database based on MPP architecture, known for its high performance and ease of use. It supports both high-concurrency point queries and high-throughput complex analysis. (<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a>)</p><h1>Links</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="github">GitHub:<a href="#github" class="hash-link" aria-label="Direct link to GitHub:" title="Direct link to GitHub:">â€‹</a></h2><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-doris-website">Apache Doris Website:<a href="#apache-doris-website" class="hash-link" aria-label="Direct link to Apache Doris Website:" title="Direct link to Apache Doris Website:">â€‹</a></h2><p><a href="https://doris.apache.org" target="_blank" rel="noopener noreferrer">https://doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/LY">Best Practice: The Efficiency of the Data Warehouse Greatly Improved in LY Digital</a></h2><div class="blog-info"><time datetime="2022-12-19T00:00:00.000Z" itemprop="datePublished">December 19, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Xing Wang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>Guide: Established in 2015, LY Digital is a financial service platform for tourism industry under LY. Com. In 2020, LY Digital introduced Apache Doris to build a data warehouse because of its rich data import methods, excellent parallel computing capabilities, and low maintenance costs. This article describes the evolution of data warehouse in LY Digital and why we switch to Apache Doris. I hope you like it.</p></blockquote><blockquote><p>Author: XingWang, Lead Developer of LY Digital</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-fb77e142257a98bea6656a33a626b310.png" width="900" height="383" class="img_ev3q"></p><h1>1. Background</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-about-ly-digital">1.1 About LY Digital<a href="#11-about-ly-digital" class="hash-link" aria-label="Direct link to 1.1 About LY Digital" title="Direct link to 1.1 About LY Digital">â€‹</a></h2><p>LY Digital is a tourism financial service platform under LY. Com. Formally established in 2015, LY Digital takes &quot;Digital technology empowers the tourism industry.&quot; as its vision.
At present, LY Digital&#x27;s business covers financial services, consumer financial services, financial technology and digital technology. So far, more than 10 million users and 76 cities have enjoyed our services.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="12-requirements-for-data-warehouse">1.2 Requirements for Data Warehouse<a href="#12-requirements-for-data-warehouse" class="hash-link" aria-label="Direct link to 1.2 Requirements for Data Warehouse" title="Direct link to 1.2 Requirements for Data Warehouse">â€‹</a></h2><ul><li>Dashboard: Needs dashboard for T+1 business, etc.</li><li>Early Warning System: Needs risk control, anomaly capital management and traffic monitoring, etc.</li><li>Business Analysis: Needs timely data query analysis and temporary data retrieval, etc.</li><li>Finance: Needs liquidation and payment reconciliation.</li></ul><h1>2. Previous Data Warehouse</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-architecture">2.1 Architecture<a href="#21-architecture" class="hash-link" aria-label="Direct link to 2.1 Architecture" title="Direct link to 2.1 Architecture">â€‹</a></h2><p><img loading="lazy" alt="page_1" src="https://cdnd.selectdb.com/assets/images/page_1-42732f62f592f158a33670ae04987e75.png" width="1152" height="679" class="img_ev3q"></p><p>Our previous data warehouse adopted the combination of SteamSets and Apache Kudu, which was very popular in the past few years. In this architecture, Binlog is ingested into Apache Kudu after passing through StreamSets in real-time, and is finally queried and used through Apache Impala and visualization tools.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="212-downside">2.1.2 Downside<a href="#212-downside" class="hash-link" aria-label="Direct link to 2.1.2 Downside" title="Direct link to 2.1.2 Downside">â€‹</a></h3><ul><li>The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs. </li><li>The previous data warehouse has a sophisticated structure that consists of many components that interact with one another, which requires huge operation and maintenance costs.</li><li>Apache Kudu&#x27;s performance in wide tables Join is not so good.</li><li>SLA is not fully guaranteed because tenant isolation is not provided.</li><li>Although SteamSets are equipped with early warning capabilities, job recovery capabilities are still poor. When configuring multiple tasks, the JVM consumes a lot, resulting in slow recovery.</li></ul><h1>3. New Data Warehouse</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-research-of-popular-data-warehouses">3.1 Research of Popular Data Warehouses<a href="#31-research-of-popular-data-warehouses" class="hash-link" aria-label="Direct link to 3.1 Research of Popular Data Warehouses" title="Direct link to 3.1 Research of Popular Data Warehouses">â€‹</a></h2><p>Due to so many shortcomings, we had to give up the previous data warehouse. In 2020, we conducted an in-depth research on the popular data warehouses in the market.</p><p>During the research, we focused on comparing Clickhouse and Apache Doris. ClickHouse has a high utilization rate of CPU, so it performs well in single-table query. But it does not perform well in multitable Joins and high QPS. On the other hand, Doris can not only support thousands of QPS per node. Thanks to the function of partitioning, it can also support high-concurrency queries at the QPS level of 10,000. Moreover, the horiziontal scaling in and out of ClickHouse are complex, which cannot be done automatically at present. Doris supports online dynamic scaling, and can be expanded horizontally according to the development of the business.</p><p>In the research, Apache Doris stood out. Doris&#x27;s high-concurrency query capability is very attractive. Its dynamic scaling capabilities are also suitable for our flexible advertising business. So we chose Apache Doris for sure.</p><p><img loading="lazy" alt="page_2" src="https://cdnd.selectdb.com/assets/images/page_2-414a885ce6917a5bfddb76d64d882ea4.png" width="1145" height="676" class="img_ev3q"></p><p>After introducing Apache Doris, we upgraded the entire data warehouse:</p><ul><li>We collect MySQL Binlog through Canal and then it is ingested into Kafka. Because Apache Doris is highly capatible with Kafka, we can easily use Routine Load to load and import data.</li><li>We have made minor adjustments to the batch processing. For data stored in Hive, Apache Doris can ingest data from Hive through Broker Load. In this way, the data in batch processing can be directly ingested into Doris.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-why-we-choose-doris">3.2 Why We Choose Doris<a href="#32-why-we-choose-doris" class="hash-link" aria-label="Direct link to 3.2 Why We Choose Doris" title="Direct link to 3.2 Why We Choose Doris">â€‹</a></h2><p><img loading="lazy" alt="page_3" src="https://cdnd.selectdb.com/assets/images/page_3-ec6524eea65a399078e60bff590cb3ab.png" width="1137" height="676" class="img_ev3q"></p><p>The overall performance of Apache Doris is impressive:</p><ul><li>Data access: It provides rich data import methods and can support the access of many types of data sources;</li><li>Data connection: Doris supports JDBC and ODBC connections. And it can easily connect with BI tools. In addition, Doris uses the MySQL protocol for communication. Users can directly access Doris through various Client tools;</li><li>SQL syntax: Doris adopts MySQL protocol and it is highly compatible with MySQL syntax, supporting standard SQL, and is low in learning costs for developers;</li><li>MPP parallel computing: Doris provides excellent parallel computing capabilities and has obvious advantages in complex Join and wide table Join;</li><li>Fully-completed documentation: Doris official documentation is very profound, which is friendly for new users. </li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33--architecture-of-real-time-processing">3.3  Architecture of Real-time Processing<a href="#33--architecture-of-real-time-processing" class="hash-link" aria-label="Direct link to 3.3  Architecture of Real-time Processing" title="Direct link to 3.3  Architecture of Real-time Processing">â€‹</a></h2><p><img loading="lazy" alt="page_4" src="https://cdnd.selectdb.com/assets/images/page_4-b6f04242c2a85d92cfd1814319127b20.png" width="1132" height="668" class="img_ev3q"></p><ul><li>Data source: In real-time processing, data sources come from business branches such as industrial finance, consumer finance, and risk control. They are all collected through Canal and API.</li><li>Data collection: After data collection through Canal-Admin, Canal sends the data to Kafka message queue. After that, the data is ingested into the Doris through Routine Load.</li><li>Inside Doris: The Doris cluster constitutes a  three-level layer of the data warehouse, namely: the DWD layer with the Unique model, the DWS layer with the Aggregation model, and the ADS application layer.</li><li>Data application: The data is applied in three aspects: real-time dashboard, data timeliness analysis and data service.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-new-features">3.4 New Features<a href="#34-new-features" class="hash-link" aria-label="Direct link to 3.4 New Features" title="Direct link to 3.4 New Features">â€‹</a></h2><p>The data import method is simple and adopts 3 different import methods according to different scenarios:</p><ul><li>Routine Load: When we submit the Rountine Load task, there will be a process within Doris that consumes Kafka in real time, continuously reads data from Kafka and ingestes it into Doris.</li><li>Broker Load: Offline data such as dim-tables and historical data are ingested into Doris in an orderly manner.</li><li>Insert Into: Used for batch processing tasks, Insert into is responsible for processing data in the DWD layer</li></ul><p>Doris&#x27; data model improves our development efficiency:</p><ul><li>The Unique model is used when accessing the DWD layer, which can effectively prevent repeated consumption of data.</li><li>In Doris, aggregation supports 4 models, such as Sum, Replace, Min, and Max. In this way, it may reduce a large amount of SQL code,  and no longer allow us to manually write Sum, Min, Max and other codes.</li></ul><p>Doris query is efficient:</p><ul><li>It supports materialized view and Rollup materialized index. The bottom layer of the materialized view is similar to the concept of Cube and the precomputation process. As a way of exchanging space for time, special tables are generated at the bottom layer. In the query, materialized view maps to the tables and responds quickly.</li></ul><h1>4. Benefits of the New Data Warehouse</h1><ul><li>Data access: In the previous architecture, the Kudu table needs to be created manually during the imports through SteamSets. Lack of tools, the entire process of creating tables and tasks takes 20-30 minutes. Nowadays, fast data access can be realized through the platform. The access process of each table has been shortened from the previous 20-30 minutes to the current 3-5 minutes, which is to say that the performance has been improved by 5-6 times.</li><li>Data development: After using Doris, we can directly use the data models, such as Unique and Aggregation.  The Duplicate model can well support logs, greatly speeding up the development process in ETL.</li><li>Query analysis: The bottom layer of Doris has functions such as materialized view and Rollup materialized index. Moreover, Doris has made many optimizations for wide table associations, such as Runtime Filter and other Joins. Compared with Doris, Apache Kudu requires more complex optimization to be better used.</li><li>Data report: It took 1-2 minutes to complete the rendering when we used Kudu to query before, but Doris responded in seconds or even milliseconds.</li><li>Easy maintenance: Doris is not as complex as Hadoop. In March, our IDC was relocated, and 12 Doris virtual machines were all migrated within three days. The overall operation is relatively simple. In addition to physically moving the machine, FE&#x27;s scaling only requires simple commands such as Add and Drop, which do not take a long time to do.</li></ul><h1>5. Look ahead</h1><ul><li>Realize data access based on Flink CDC: At present, Flink CDC is not introduced, but Kafka through Canal instead. The development efficiency can be even faster if we use Flink CDC. Flink CDC still needs us to write a certain amount of code, which is not friendly for data analysts to use directly. We hope that data analysts only need to write simple SQL or directly operate. In the future planning, we plan to introduce Flink CDC.</li><li>Keep up with the latest release: Now the latest version Apache Doris V1.2.0 has made great achievements in vectorization, multi-catalog, and light schema change. We will keep up with the community to upgrade the cluster and make full use of new features.</li><li>Strengthen the construction of related systems: Our current index system management, such as report metadata, business metadata, and other management levels still need to be improved. Although we have data quality monitoring functions, it still needs to be strengthened and improved in automation.</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/BestPractice_Kwai">Best Practice in Kwai: Apache Doris on Elasticsearch</a></h2><div class="blog-info"><time datetime="2022-12-14T00:00:00.000Z" itemprop="datePublished">December 14, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Xiang He</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>Author: Xiang He, Head Developer of Big Data, Commercialization Team of Kwai</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-846e4e39fd88e1e34d2474b23690d9b2.png" width="900" height="383" class="img_ev3q"></p><h1>1 About Kwai</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-kwai">1.1 Kwai<a href="#11-kwai" class="hash-link" aria-label="Direct link to 1.1 Kwai" title="Direct link to 1.1 Kwai">â€‹</a></h2><p>Kwai(HKG:1024) is a social network for short videos and trends. Discover funny short videos, contribute to the virtual community with recordings, videos of your life, playing daily challenges or likes the best memes and videos. Share your life with short videos and choose from dozens of magical effects and filters for them.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="12-kwais-commercial-report-engine">1.2 Kwai&#x27;s Commercial Report Engine<a href="#12-kwais-commercial-report-engine" class="hash-link" aria-label="Direct link to 1.2 Kwai&#x27;s Commercial Report Engine" title="Direct link to 1.2 Kwai&#x27;s Commercial Report Engine">â€‹</a></h2><p>Kwaiâ€™s commercial report engine provides advertisers with real-time query service for multi-dimensional analysis reports. And it also provides query service for multi-dimensional analysis reports for internal users. The engine is committed to dealing with high-performance, high-concurrency, and high-stability query problems in multi-dimensional analysis report cases.</p><h1>2 Previous Architecture</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-background">2.1 Background<a href="#21-background" class="hash-link" aria-label="Direct link to 2.1 Background" title="Direct link to 2.1 Background">â€‹</a></h2><p>Traditional OLAP engines deal with multi-dimensional analysis in a more pre-modeled way, by building a data cube (Cube) to perform operations such as Drill-down, Roll-up, Slice, and Dice and Pivot. Modern OLAP analysis introduces the idea of â€‹â€‹a relational model, representing data in two-dimensional relational tables. In the modeling process, usally there are two modeling methods. One is to ingest the data of multiple tables into one wide table through Join; the other is to use the star schema, divide the data into fact table and dim-table.  And then Join them when querying.
Both options have some pros and cons:</p><p>Wide table:</p><p>Taking the idea of â€‹â€‹exchanging space for time. The primary key of the dim-table is the unique ID to fill all dimensions, and multiple dimension data is stored in redundant storage. Its advantage is that it is convenient to query, unnecessary to associate additional dim-tables, which is way better. The disadvantage is that if there is a change in dimension data, the entire table needs to be refreshed, which is bad for high-frequency Update.</p><p>Star Schema:</p><p>Dimension data is completely separated from fact data. Dimension data is often stored in a dedicated engine (such as MySQL, Elasticsearch, etc.). When querying, dimension data is associated with the primary key. The advantage is that changes in dimension data do not affect fact data, which can support high-frequency Update operations. The disadvantage is that the query logic is relatively more complex, and multi-table Join may lead to performance loss.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-requirement-for-an-olap-engine">2.2 Requirement for an OLAP Engine<a href="#22-requirement-for-an-olap-engine" class="hash-link" aria-label="Direct link to 2.2 Requirement for an OLAP Engine" title="Direct link to 2.2 Requirement for an OLAP Engine">â€‹</a></h2><p>In Kwaiâ€™s business, the commercial reports engine supports the real-time query of the advertising effect for advertisers. When building the report engine, we expect to meet the following requirements:</p><ul><li>Immersive data: the original data of a single table increases by ten billion every day</li><li>High QPS in Query: thousand-level QPS on average</li><li>High stability requirements: SLA level of 99.9999 %</li></ul><p>Most importantly, due to frequent changes in dimension data, dim-tables need to support Update operations up to thousand-level QPS and further support requirements such as fuzzy matching and word segmentation retrieval.
Based on the above requirements, we chose star schema and built a report engine architecture with Apache Druid and Elasticsearch.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-previous-architecture-based-on-apache-druid">2.3 Previous Architecture: Based on Apache Druid<a href="#23-previous-architecture-based-on-apache-druid" class="hash-link" aria-label="Direct link to 2.3 Previous Architecture: Based on Apache Druid" title="Direct link to 2.3 Previous Architecture: Based on Apache Druid">â€‹</a></h2><p>We chose the combination of Elasticsearch and Apache Druid. In data import, we use Flink to pre-aggregate the data at minute-evel, and use Kafka to pre-aggregate the data at hour-level. In data query, the application initiates a query request through RE Front API, and Re Query initiates queries to the dim-table engine (Elasticsearch and MySQL) and the extension engine respectively.</p><p>Druid is a timing-based query engine that supports real-time data ingestion and is used to store and query large amounts of fact data. We adopt Elasticseach based on those concerns:</p><ul><li>High update frequency, QPS is around 1000</li><li>Support word segmentation and fuzzy search, which is suitable for Kwai</li><li>Supports high-level dim-table data, which can be directly qualified without adopting sub-database and sub-table just like MySQL database</li><li>Supports data synchronization monitoring, and has check and recovery services as well</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-engine-of-the-reports">2.4 Engine of the Reports<a href="#24-engine-of-the-reports" class="hash-link" aria-label="Direct link to 2.4 Engine of the Reports" title="Direct link to 2.4 Engine of the Reports">â€‹</a></h2><p>The report engine can be divided into two layers: REFront and REQuery. REMeta is an independent metadata management module. The report engine implements MEMJoin inside REQuery. It supports associative query between fact data in Druid and dimension data in Elasticsearch. And it also provides virtual cube query for upper-layer business, avoiding the exposion of complex cross-engine management and query logic.</p><p><img loading="lazy" alt="page_1" src="https://cdnd.selectdb.com/assets/images/page_1-9e4af3275a17b4c1c893caa7c6f7290b.png" width="709" height="698" class="img_ev3q"></p><h1>3 New Architecture Based on Apache Doris</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-problems-remained">3.1 Problems Remained<a href="#31-problems-remained" class="hash-link" aria-label="Direct link to 3.1 Problems Remained" title="Direct link to 3.1 Problems Remained">â€‹</a></h2><p>First, we came across a problem when we build the report engine. Mem Join is single-machine with serial execution. When the amount of data pulled from Elasticsearch exceeds 100,000 at a single time, the response time is close to 10s, and the user experience is poor. Moreover, using a single node to execute large-scale data Join will consume a lot of memory, causing Full GC.</p><p>Second, Druid&#x27;s Lookup Join function is not so perfect, which is a big problem, and it cannot fully meet our business needs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-database-research">3.2 Database Research<a href="#32-database-research" class="hash-link" aria-label="Direct link to 3.2 Database Research" title="Direct link to 3.2 Database Research">â€‹</a></h2><p>So we conducted a survey on popular OLAP databases in the industry, the most representative of which are Apache Doris and Clickhouse. We found out that Apache Doris is more capable of Join between large and wide tables. ClickHouse can support Broadcast memory-based Join, but the performance  is not good for the Join between large and wide tables with a large data volume. Both Doris and Clickhouse support detailed data storage, but the capability for concurrency of Clickhouse is low. On the contrary, Doris supports high-concurrency and low-latency query services, and a single machine supports up to thousands of QPS. When the concurrency increases, horizontal expansion of FE and BE can be supported. However, Clickhouse&#x27;s data import is not able to support Transaction SQL, which cannot realize Exactly-once semantics and has limited ablility for standard SQL. In contrast, Doris provides Transaction SQL and atomicity for data import. Doris itself can ensure that messages in Kafka are not lost or re-subscribed, which is to say, Exactly-Once semantics is supported. ClickHouse has high learning cost, high operation and maintenance costs, and weak in distribution. The fact that it requires more customization and deeper technical strength is another problem. Doris is different. There are only two core components, FE and BE, and there are fewer external dependencies. We also found that because Doris is closer to the MySQL protocol, it is more convenient than Clickhouse and the cost of migration is not so large. In terms of horizontal expansion, Doris&#x27; expansion and contraction can also achieve self-balancing, which is much better than that of Clickhouse.</p><p>From this point of view, Doris can better improve the performance of Join and is much better in other aspects such as migration cost, horizontal expansion, and concurrency. However, Elasticsearch has inherent advantages in high-frequency Update.</p><p>It would be an ideal solution to deal with high-frequency Upate and Join performance at the same time by building engines through Doris on Elasticsearch.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-good-choice-doris-on-elasticsearch">3.3 Good Choice: Doris on Elasticsearch<a href="#33-good-choice-doris-on-elasticsearch" class="hash-link" aria-label="Direct link to 3.3 Good Choice: Doris on Elasticsearch" title="Direct link to 3.3 Good Choice: Doris on Elasticsearch">â€‹</a></h2><p>What is the query performance of Doris on Elasticsearch?</p><p>First of all, Apache Doris is a real-time analytical database based on MPP architecture, with strong performance and strong horizontal expansion capability. Doris on Elasticsearch takes advantage on this capability and does a lot of query optimization. Secondly, after integrating Elasticsearch, we have also made a lot of optimizations to the query:</p><ul><li>Shard-level concurrency</li><li>Automatic adaptation of row and column scanning, priority to column scanning</li><li>Sequential read, terminated early</li><li>Two-phase query becomes one-phase query</li><li>Broadcast Join is especially friendly for small batch data</li></ul><p><img loading="lazy" alt="page_2" src="https://cdnd.selectdb.com/assets/images/page_2-a916fe2ffe5eeae0b166d30cfe8d8e42.png" width="890" height="1032" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-doris-on-elasticsearch">3.4 Doris on Elasticsearch<a href="#34-doris-on-elasticsearch" class="hash-link" aria-label="Direct link to 3.4 Doris on Elasticsearch" title="Direct link to 3.4 Doris on Elasticsearch">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="341-data-link-upgrade">3.4.1 Data Link Upgrade<a href="#341-data-link-upgrade" class="hash-link" aria-label="Direct link to 3.4.1 Data Link Upgrade" title="Direct link to 3.4.1 Data Link Upgrade">â€‹</a></h3><p>The upgrade of the data link is relatively simple. In the first step, in Doris we build a new Olap table and configure the materialized view. Second, the routine load is initiated based on the Kafka topic of the previous fact data, and then real-time data is ingested. The third step is to ingest offline data from Hive&#x27;s broker load. The last step is to create an Elasticsearch external table through Doris.</p><p><img loading="lazy" alt="page_3" src="https://cdnd.selectdb.com/assets/images/page_3-2f23fe1184980f690da326e4446fd7f7.png" width="1313" height="1265" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="342-upgrades-of-the-report-engine">3.4.2 Upgrades of the Report Engine<a href="#342-upgrades-of-the-report-engine" class="hash-link" aria-label="Direct link to 3.4.2 Upgrades of the Report Engine" title="Direct link to 3.4.2 Upgrades of the Report Engine">â€‹</a></h3><p><img loading="lazy" alt="page_4" src="https://cdnd.selectdb.com/assets/images/page_4-f9c9b95ac997f1d8f09fb5fe182c368f.png" width="1274" height="895" class="img_ev3q"></p><p>Note: The MySQL dim-table associated above is based on future planning. Currently, Elasticsearch is mainly used as the dim-table engine</p><p>Report Engine Adaptation</p><ul><li>Generate virtual cube table based on Doris&#x27;s star schema</li><li>Adapt to cube table query analysis, intelligent Push-down</li><li>Gray Release</li></ul><h1>4  Online Performance</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-fact-table-query-performance-comparison">4.1 Fact Table Query Performance Comparison<a href="#41-fact-table-query-performance-comparison" class="hash-link" aria-label="Direct link to 4.1 Fact Table Query Performance Comparison" title="Direct link to 4.1 Fact Table Query Performance Comparison">â€‹</a></h2><p>Druid</p><p><img loading="lazy" alt="page_5" src="https://cdnd.selectdb.com/assets/images/page_5-8e598f4abd11de7482c1a9dcc0747641.png" width="935" height="276" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="page_6" src="https://cdnd.selectdb.com/assets/images/page_6-7747547b14b4dbce6b2ee99fde03ab16.png" width="959" height="291" class="img_ev3q"></p><p>99th percentile of response time:
Druid: 270ms, Doris: 150ms and which is reduced by 45%</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="42-comparison-of-cube-table-query-performance-in-join">4.2 Comparison of Cube Table Query Performance in Join<a href="#42-comparison-of-cube-table-query-performance-in-join" class="hash-link" aria-label="Direct link to 4.2 Comparison of Cube Table Query Performance in Join" title="Direct link to 4.2 Comparison of Cube Table Query Performance in Join">â€‹</a></h2><p>Druid</p><p><img loading="lazy" alt="page_7" src="https://cdnd.selectdb.com/assets/images/page_7-46c2a88aabf031ee764884d78837880f.png" width="987" height="316" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="page_8" src="https://cdnd.selectdb.com/assets/images/page_8-cc75cc3a5ced01182cac415175d4048a.png" width="950" height="291" class="img_ev3q"></p><p>99th percentile of response time:
Druid: 660ms, Doris: 440ms and which is reduced by 33%</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="43-benefits">4.3 Benefits<a href="#43-benefits" class="hash-link" aria-label="Direct link to 4.3 Benefits" title="Direct link to 4.3 Benefits">â€‹</a></h2><ul><li>The overall time consumption of 99 percentile is reduced by about 35%</li><li>Resource saving about 50%</li><li>Remove the complex logic of MemJoin from the report engine; Realize through DO(in the case of large query: dim-table results exceed 100,000, performance improvement exceeds 10 times, 10s to 1s)</li><li>Richer query semantics (Mem Join is relatively simple and does not support complex queries)</li></ul><h1>5  Summary and Plans</h1><p>In Kwai&#x27;s commercial business, Join queries between dimension data and fact data is very common. After using Doris, query becomes simple. We only need to synchronize the fact table and dim-table on a daily basis and Join while querying. By replacing Druid and Clickhouse with Doris, Doris basically covers all scenarios when we use Druid. In this way, Kwai&#x27;s commercial report engine greatly improves the aggregation and analysis capabilities of massive data. During the use of Apache Doris, we also found some unexpected benefits: For example, the import method of Routine Load and Broker Load is relatively simple, which improves the query speed; The data occupation is greatly reduced; Doris supports the MySQL protocol, which is much easier for data analyst to fetch data and make charts.</p><p>Although the Doris on Elasticsearch has fully meet our requirement, Elasticsearch external table still requires manual creation. However, Apache Doris recently released the latest version V1.2.0. The new version has added Multi-Catlog, which provides the ability to seamlessly access external table sources such as Hive, Elasticsearch, Hudi, and Iceberg. Users can connect to external tables through the CREATE CATALOG command, and Doris will automatically map the library and table information of the external dable. In this way, we don&#x27;t need to manually create the Elasticsearch external tables to complete the mapping in the future, which greatly saves us time and cost of development and improves the efficiency of research and development. The power of other new functions such as Vectorization and Ligt Schema Change also gives us new expectations for Apache Doris. Bless Apache Doris!</p><h1>Contact Us</h1><p>Apache Doris Websiteï¼š<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Githubï¼š<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>Dev Emailï¼š<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/xiaomi_vector">Practice and Optimization of Apache Doris in Xiaomi</a></h2><div class="blog-info"><time datetime="2022-12-08T00:00:00.000Z" itemprop="datePublished">December 8, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">ZuoWei</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>Guide: Xiaomi Group introduced Apache Doris in 2019. At present, Apache Doris has been widely used in dozens of business departments within Xiaomi. A set of data ecology with Apache Doris has been formed. This article is transcribed from an online meetup speech of the Doris community, aiming to share the practice of Apache Doris in Xiaomi.</p></blockquote><blockquote><p>Author: ZuoWei, OLAP Engineer, Xiaomi</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-b27d71e34981d9850785329cea2cb610.png" width="900" height="383" class="img_ev3q"></p><h1>About Xiaomi</h1><p><a href="https://www.mi.com/global" target="_blank" rel="noopener noreferrer">Xiaomi Corporation</a> (â€œXiaomiâ€ or the â€œGroupâ€; HKG:1810), a consumer electronics and smart manufacturing company with smartphones and smart hardware connected by an Internet of Things (IoT) platform.  In 2021, Xiaomi&#x27;s total revenue amounted to RMB328.3 billion(USD472,231,316,200), an increase of 33.5% year-over-year; Adjusted net profit was RMB22.0 billion(USD3,164,510,800), an increase of 69.5% year-over-year.</p><p>Due to the growing need of data analysis, Xiaomi Group introduced Apache Doris in 2019. As one of the earliest users of Apache Doris, Xiaomi Group has been deeply involved in the open-source community. After three years of development, Apache Doris has been widely used in dozens of business departments within Xiaomi, such as Advertising, New Retail, Growth Analysis, Dashboards, UserPortraits, <a href="https://airstar.com/home" target="_blank" rel="noopener noreferrer">AISTAR</a>, <a href="https://www.xiaomiyoupin.com" target="_blank" rel="noopener noreferrer">Xiaomi Youpin</a>. Within Xiaomi, a data ecosystem has been built around Apache Doris. </p><p><img loading="lazy" alt="page_1" src="https://cdnd.selectdb.com/assets/images/page_1-93afbd2f90769776af3083bc49fbf8dd.jpg" width="1135" height="661" class="img_ev3q"></p><p>At present, Apache Doris already has dozens of clusters in Xiaomi, with an overall scale of hundreds of virtual machines . Among them, the largest single cluster reaches nearly 100 nodes, with dozens of real-time data synchronization tasks. And the largest daily increment of a single table rocket to 12 billion, supporting PB-level storage. And a single cluster can support more than 20,000 multi-dimensional analysis queries per day.</p><h1>Architecture Evolution</h1><p>The original intention of Xiaomi to introduce Apache Doris is to solve the problems encountered in user behavior analysis. With the development of Xiaomi&#x27;s Internet business, the demand for growth analysis using user behavior data is becoming stronger and stronger. If each business branch builds its own growth analysis system, it will not only be costly, but also inefficient. Therefore, if there is a product that can help them stop worrying about underlying complex technical details, it would be great to have relevant business personnel focus on their own technical work. In this way, it can greatly improve work efficiency. Therefore, Xiaomi Big Data and the cloud platform jointly developed the growth analysis system called Growing Analytics (referred to as GA), which aims to provide a flexible multi-dimensional real-time query and analysis platform, which can manage data access and query solutions in a unified way, and help business branches to refine operation.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="previous-architecture">Previous Architecture<a href="#previous-architecture" class="hash-link" aria-label="Direct link to Previous Architecture" title="Direct link to Previous Architecture">â€‹</a></h2><p>The growth analysis platform project was established in mid-2018. At that time, based on the consideration of development time and cost, Xiaomi reused various existing big data basic components (HDFS, Kudu, SparkSQL, etc.) to build a growth analysis query system based on Lambda architecture. The architecture of the first version of the GA system is shown in the figure below, including the following aspects:</p><ul><li>Data Source: The data source is the front-end embedded data and user behavior data.</li><li>Data Access: The event tracking data is uniformly cleaned and ingested into Xiaomi&#x27;s internal self-developed message queue, and the data is imported into Kudu through Spark Streaming.</li><li>Storage: Separate hot and cold data in the storage layer. Hot data is stored in Kudu, and cold data is stored in HDFS. At the same time, partitioning is carried out in the storage layer. When the partition unit is day, part of the data will be cooled and stored on HDFS every night.</li><li>Compute and Query: In the query layer, use SparkSQL to perform federated queries on the data on Kudu and HDFS, and finally display the query results on the front-end page.</li></ul><p><img loading="lazy" alt="page_2" src="https://cdnd.selectdb.com/assets/images/page_2-db57a1a2eadb0f1c787f440a26358339.jpg" width="1159" height="683" class="img_ev3q"></p><p>At that time, the first version of the growth analysis platform helped us solve a series of problems in the user operation process, but there were also two problems:</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-no1-scattered-components">Problem No.1: Scattered components<a href="#problem-no1-scattered-components" class="hash-link" aria-label="Direct link to Problem No.1: Scattered components" title="Direct link to Problem No.1: Scattered components">â€‹</a></h3><p>Since the historical architecture is based on the combination of SparkSQL + Kudu + HDFS, too many dependent components lead to high operation and maintenance costs. The original design is that each component uses the resources of the public cluster, but in practice, it is found that during the execution of the query job, the query performance is easily affected by other jobs in the public cluster, and query jitter is prone to occur, especially when reading data from the HDFS public cluster , sometimes slower.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="problem-no2-high-resource-consumption">Problem No.2: High resource consumption<a href="#problem-no2-high-resource-consumption" class="hash-link" aria-label="Direct link to Problem No.2: High resource consumption" title="Direct link to Problem No.2: High resource consumption">â€‹</a></h3><p>When querying through SparkSQL, the latency is relatively high. SparkSQL is a query engine designed based on a batch processing system. In the process of exchanging data shuffle between each stage, it still needs to be placed on the disk, and the delay in completing the SQL query is relatively high. In order to ensure that SQL queries are not affected by resources, we ensure query performance by adding machines. However, in practice, we find that there is limited room for performance improvement. This solution cannot make full use of machine resources to achieve efficient queries. A certain waste of resources.</p><p>In response to the above two problems, our goal is to seek an MPP database that integrates computing and storage to replace our current storage and computing layer components. After technical selection, we finally decided to use Apache Doris to replace the older generation of historical architecture.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-choice">New Choice<a href="#new-choice" class="hash-link" aria-label="Direct link to New Choice" title="Direct link to New Choice">â€‹</a></h2><p>Popular MPP-based query engines such as Impala and Presto, can efficiently support SQL queries, but they still need to rely on Kudu, HDFS, Hive Metastore and other storage system, which increase the operation and maintenance costs. At the same time, due to the separation of storage and compute, the query engine cannot easily find the data changes in the storage layer, resulting in bad performance in detailed query optimization. If you want to cache at the SQL layer, you cannot guarantee that the query results are up-to-date.</p><p>Apache Doris is a top-level project of the Apache Foundation. It is mainly positioned as a high-performance, real-time analytical database, and is mainly used to solve reports and multi-dimensional analysis. It integrates Google Mesa and Cloudera Impala technologies. We conducted an in-depth performance tests on Doris and communicated with the community many times. And finally, we determined to replace the previous computing and storage components with Doris. </p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="new-architecture-based-on-apache-doris">New Architecture Based on Apache Doris<a href="#new-architecture-based-on-apache-doris" class="hash-link" aria-label="Direct link to New Architecture Based on Apache Doris" title="Direct link to New Architecture Based on Apache Doris">â€‹</a></h2><p>The new architecture obtains event tracking data from the data source. Then data is ingested  into Apache Doris. Query results can be directly displayed in the applications. In this way, Doris has truly realized the unification of computing, storage, and resource management tools.</p><p><img loading="lazy" alt="page_3" src="https://cdnd.selectdb.com/assets/images/page_3-30c8cb46f4d289fa768e9a364779bc69.jpg" width="1149" height="674" class="img_ev3q"></p><p>We chose Doris because:</p><ul><li>Doris has excellent query performance and can meet our business needs.</li><li>Doris supports standard SQL, and the learning cost is low.</li><li>Doris does not depend on other external components and is easy to operate and maintain.</li><li>The Apache Doris community is very active and friendly, crowded with contributors. It is easier for further versions upgrades and convenient for maintenance.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="query-performance-comparision-between-apache-doris--spark-sql">Query Performance Comparision between Apache Doris &amp; Spark SQL<a href="#query-performance-comparision-between-apache-doris--spark-sql" class="hash-link" aria-label="Direct link to Query Performance Comparision between Apache Doris &amp; Spark SQL" title="Direct link to Query Performance Comparision between Apache Doris &amp; Spark SQL">â€‹</a></h2><p>Note: The comparison is based on Apache Doris V0.13</p><p><img loading="lazy" alt="page_4" src="https://cdnd.selectdb.com/assets/images/page_4-3e71f2a8753e49f5a73bea4bb628fbbf.jpg" width="1242" height="1000" class="img_ev3q"></p><p>We selected a business model with an average daily data volume of about 1 billion, and conducted performance tests on Doris in different scenarios, including 6 event analysis scenarios, 3 retention analysis scenarios, and 3 funnel analysis scenarios. After comparing it with the previous architecture(SparkSQL+Kudu+HDFS), we found out:</p><ul><li>In the event analysis scenario, the average query time was reduced by 85%.</li><li>In the scenarios of retention analysis and funnel analysis, the average query time was reduced by 50%.</li></ul><h1>Real Practice</h1><p>Below we will introduce our experience of data import, data query, A/B test in the business application of Apache Doris.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-import">Data Import<a href="#data-import" class="hash-link" aria-label="Direct link to Data Import" title="Direct link to Data Import">â€‹</a></h2><p><img loading="lazy" alt="page_5" src="https://cdnd.selectdb.com/assets/images/page_5-010f8edce4b736817d68815f31e52fd7.jpg" width="1130" height="667" class="img_ev3q"></p><p>Xiaomi writes data into Doris mainly through Stream Load, Broker Load and a small amount of data by Insert. Usually data is generally ingested into the message queue first, which is divided into real-time and offline data.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-write-real-time-data-into-apache-doris">How to write real-time data into Apache Doris:<a href="#how-to-write-real-time-data-into-apache-doris" class="hash-link" aria-label="Direct link to How to write real-time data into Apache Doris:" title="Direct link to How to write real-time data into Apache Doris:">â€‹</a></h3><p>After part of real-time data processed by Flink, they will be ingested into Doris through  Flink-Doris-Connector provided by Apache Doris. The rest of the data is ingested through Spark Streaming. The bottom layer of these two writing approaches both rely on the Stream Load provided by Apache Doris.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="how-to-write-offline-data-into-apache-doris">How to write offline data into Apache Doris:<a href="#how-to-write-offline-data-into-apache-doris" class="hash-link" aria-label="Direct link to How to write offline data into Apache Doris:" title="Direct link to How to write offline data into Apache Doris:">â€‹</a></h3><p>After offline data is partially ingested into Hive, they will be ingested into Doris through Xiaomi&#x27;s data import tool. Users can directly submit Broker Load tasks to the Xiaomi&#x27;s data import tool and import data directly into Doris, or import data through Spark SQL, which relies on the Spark-Doris-Connector provided by Apache Doris. Spark Doris Connector is actually the encapsulation of Stream Load.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="data-qurey">Data Qurey<a href="#data-qurey" class="hash-link" aria-label="Direct link to Data Qurey" title="Direct link to Data Qurey">â€‹</a></h2><p><img loading="lazy" alt="page_6" src="https://cdnd.selectdb.com/assets/images/page_6-14cf1592d25e4b6e4cc275e06c2e6673.jpg" width="1120" height="638" class="img_ev3q"></p><p>Users can query after data import is done. Inside Xiaomi, we query through our data platform. Users can perform visual queries on Doris through Xiaomi&#x27;s data platform, and conduct user behavior analysis and user portrait analysis. In order to help our teams conduct event analysis, retention analysis, funnel analysis, path analysis and other behavioral analysis, we have added corresponding UDF (User Defined Function) and UDAF (User Defined Aggregate Function) to Doris.</p><p>In the upcoming version 1.2, Apache Doris adds the function of synchronizing metadata through external table, such as Hive/Hudi/Iceberg and Multi Catalog tool. External table query improves performance, and the ability to access external tables greatly increases ease of use. In the future, we will consider querying Hive and Iceberg data directly through Doris, which builds an architecture of datalake.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ab-test">A/B Test<a href="#ab-test" class="hash-link" aria-label="Direct link to A/B Test" title="Direct link to A/B Test">â€‹</a></h2><p>In real business, the A/B test is a method of comparing two versions of strategies against each other to determine which one performs better. A/B test is essentially an experiment where two or more variants of a page are shown to users at random, and statistical analysis. It is popular approach used to determine which variation performs better for a given conversion goal. Xiaomi&#x27;s A/B test platform is an operation tool product that conducts the A/B test with experimental grouping, traffic splitting, and scientific evaluation to assist in decision making. Xiaomi&#x27;s A/B test platform has several query applications: user deduplication, indicator summation, covariance calculation, etc. The query types will involve Count (distinct), Bitmap, Like, etc.</p><p>Apache Doris also provides services to Xiaomi&#x27;s A/B test platform. Everyday, Xiaomi&#x27;s A/B test platform needs to process a temendous amount of data with billions of queries. That&#x27;s why Xiaomi&#x27;s A/B test platform is eager to improve the query performance. </p><p>Apache Doris V1.1 released just in time and has fully supported vectorization in the processing and storage. Compared with the non-vectorized version, the query performance has been significantly improved. It is time to update Xiaomi&#x27;s Doris cluster to the latest version. That&#x27;s why we first launched the latest vectorized version of Doris on Xiaomi&#x27;s A/B test platform.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="test-before-launch">Test before Launch<a href="#test-before-launch" class="hash-link" aria-label="Direct link to Test before Launch" title="Direct link to Test before Launch">â€‹</a></h2><p>Note: The following tests are based on Apache Doris V1.1.2</p><p>We built a test cluster for Apache Doris V1.1.2, which is as big as that of the Xiaomi online Apache Doris V0.13 version, to test before the vectorization version goes online. The test is divided into two aspects: single SQL parrellel query test and batch SQL concurrent query test.</p><p>The configurations of the two clusters are exactly the same, and the specific configuration information is as follows:</p><ul><li>Scale: 3 FEs + 89 virtual machines</li><li>CPU: Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz 16 cores 32 threads Ã— 2</li><li>Memory: 256GB</li><li>Disk: 7.3TB Ã— 12 HDD</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="single-sql-parrellel-query-test">Single SQL Parrellel Query Test<a href="#single-sql-parrellel-query-test" class="hash-link" aria-label="Direct link to Single SQL Parrellel Query Test" title="Direct link to Single SQL Parrellel Query Test">â€‹</a></h3><p>We choose 7 classic queries in the Xiaomi A/B test. For each query, we limited the time range to 1 day, 7 days, and 20 days for testing, where the daily partition data size is about 3.1 billion (the data volume is about 2 TB). The test results are shown in the figures:</p><p><img loading="lazy" alt="page_7" src="https://cdnd.selectdb.com/assets/images/page_7-b41817232fb711c583332d813de7f684.jpg" width="750" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="page_8" src="https://cdnd.selectdb.com/assets/images/page_8-c8e10196ce6917449e8372205333f12c.jpg" width="750" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="page_9" src="https://cdnd.selectdb.com/assets/images/page_9-cfbcd21a8b00a3b50508251b78ebd163.jpg" width="750" height="450" class="img_ev3q"></p><p>The Apache Doris V1.1.2 has at least 3~5 times performance improvement compared to the Xiaomi online Doris V0.13, which is remarkable.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimization">Optimization<a href="#optimization" class="hash-link" aria-label="Direct link to Optimization" title="Direct link to Optimization">â€‹</a></h2><p>Note: The following tests are based on Apache Doris V1.1.2</p><p>Based on Xiaomi&#x27;s A/B test business data, we tuned Apache Doris V1.1.2 and conducted concurrent query tests on the tuned Doris V1.1.2 and Xiaomi&#x27;s online Doris V0.13. The test results are as follows.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-in-test-1">Optimization in Test 1<a href="#optimization-in-test-1" class="hash-link" aria-label="Direct link to Optimization in Test 1" title="Direct link to Optimization in Test 1">â€‹</a></h3><p>We choose user deduplication, index summation, and covariance calculation query(the total number of SQL is 3245) in the A/B test to conduct concurrent query tests on the two versions. The single-day partition data of the table is about 3.1 billion (the amount of data is about 2 TB) and the query will be based on the latest week&#x27;s data. The test results are shown in the figures:</p><p><img loading="lazy" alt="page_10" src="https://cdnd.selectdb.com/assets/images/page_10-98057ca75a1689b6c6eb9932cdd5e841.jpg" width="1080" height="338" class="img_ev3q"></p><p>Compared with Apache Doris V0.13, the overall average latency of Doris V1.1.2 is reduced by about 48%, and the P95 latency is reduced by about 49%. In this test, the query performance of Doris V1.1.2 was nearly doubled compared to Doris V0.13.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-in-test-2">Optimization in Test 2<a href="#optimization-in-test-2" class="hash-link" aria-label="Direct link to Optimization in Test 2" title="Direct link to Optimization in Test 2">â€‹</a></h3><p>We choose 7 A/B test reports to test the two versions. Each A/B test report is corresponded to two modules in Xiaomi A/B test platform and each module represents thousands of SQL query. Each report submits query tasks to the cluster where the two versions reside at the same concurrency. The test results are shown in the figure:</p><p><img loading="lazy" alt="page_11" src="https://cdnd.selectdb.com/assets/images/page_11-bbf60c474aaea1a007b5b413d6bad77a.jpg" width="750" height="450" class="img_ev3q"></p><p>Compared with Doris V0.13, Doris V1.1.2 reduces the overall average latency by around 52%. In the test, the query performance of Doris V1.1.2 version was more than 1 time higher than that of Doris V0.13. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-in-test-3">Optimization in Test 3<a href="#optimization-in-test-3" class="hash-link" aria-label="Direct link to Optimization in Test 3" title="Direct link to Optimization in Test 3">â€‹</a></h3><p>To verify the performance of the tuned Apache Doris V1.1.2 in other cases, we choose the Xiaomi user behavior analysis to conduct concurrent query performance tests of Doris V1.1.2 and Doris V0.13. We choose behavior analysis query for 4 days on October 24, 25, 26 and 27, 2022. The test results are shown in the figures:</p><p><img loading="lazy" alt="page_12" src="https://cdnd.selectdb.com/assets/images/page_12-58242671fba5bbf25225b4d9d9f6d87c.jpg" width="1080" height="338" class="img_ev3q"></p><p>Compared with Doris V0.13, the overall average latency of Doris V1.1.2 has been reduced by about 77%, and the P95 latency has been reduced by about 83%. In this test, the query performance of Doris V1.1.2 version is 4~6 times higher than that of Doris V0.13.</p><h1>Conclusion</h1><p>Since we adopted Apache Doris in 2019, Apache Doris has currently served dozens of businesses and sub-brands within Xiaomi, with dozens of clusters and hundreds of nodes. It completes more than 10,000 user online analysis queries every day and is responsible for most of the online analysis in Xiaomi.</p><p>After performance test and tuning, Apache Doris V1.1.2 has met the launch requirements of the Xiaomi A/B test platform and does well in query performance and stability. In some cases, it even exceeds our expectations, such as the overall average latency being reduced by about 77% in our tuned version.</p><p>Meanwhile, some functions have in the above been released in Apache Doris V1.0 or V1.1,  some PRs have been merged into the community Master Fork and should be released soon. Recently the activity of the community has been greatly enhanceed. We are glad to see that Apache Doris has become more and more mature, and stepped forward to an integrated datalake. We truly believe that in the future, more data analysis will be explored and realized within Apache Doris.</p><h1>Contact Us</h1><p>Apache Doris Websiteï¼š<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Github Homepageï¼š<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>Email to DEVï¼š<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/JD_OLAP">JD.com&#x27;s Exploration and Practice with Apache Doris in Realtime OLAP</a></h2><div class="blog-info"><time datetime="2022-12-02T00:00:00.000Z" itemprop="datePublished">December 2, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Li Zhe</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-e94fd46c1522a3383d161daec2249d18.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>Guide:
This article discusses the exploration and practice of the search engine team in JD.com  using Apache Flink and Apache Doris in real-time data analysis. The popularity of stream computing is increasing day by day: More papers are published on Google Dataflow; Apache Flink has become the one of the most popular engine in the world; There is wide application of real-time analytical databases more than ever before, such as Apache Doris; Stream computing engines are really flourishing. However, no engine is perfect enough to solve every problem. It is important to find a  suitable OLAP engine for the business. We hope that JD.com&#x27;s practice in  real-time OLAP and stream computing may give you some inspiration.</p></blockquote><blockquote><p>Author: Li Zhe, data engineer of JD.com, who focused on offline data, stream computing and application development.</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-jdcom">About JD.com<a href="#about-jdcom" class="hash-link" aria-label="Direct link to About JD.com" title="Direct link to About JD.com">â€‹</a></h2><p>JD.com (NASDAQ: JD), a leading e-commerce company in China, had a net income of RMB 951.6 billion in 2021. JD Group owns JD Retail, JD Global, JD Technology, JD Logistics, JD Cloud, etc. Jingdong Group was officially listed on the NASDAQ Stock Exchange in May 2014.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="jd-search-boxs-requirement-real-time-data-analysis">JD Search Box&#x27;s Requirement: Real-time Data Analysis<a href="#jd-search-boxs-requirement-real-time-data-analysis" class="hash-link" aria-label="Direct link to JD Search Box&#x27;s Requirement: Real-time Data Analysis" title="Direct link to JD Search Box&#x27;s Requirement: Real-time Data Analysis">â€‹</a></h2><p>JD search box, as the entrance of the e-commerce platform, provides a link betwee merchants and users. Users can express their needs through the search box. In order to better understand user intentions and quickly improve the conversion rate, multiple A/B tests are running online at the same time, which apply to multiple products. The category, organization, and brand all need to be monitored online for better conversion. At present, JD search box demands real-time data in application mainly includes three parts:</p><ol><li>The overall data of JD search box.</li><li>Real-time monitoring of the A/B test.</li><li>Top list of hot search words to reflect changes in public opinion. Words trending can reflect what users care</li></ol><p>The analysis mentioned above needs to refine the data to the SKU-level. At the same time, we also undertake the task of building a real-time data platform to show our business analysists different real-time stream computing data.</p><p>Although different business analysists care about different data granularity, time frequency, and dimensions, we are hoping to establish a unified real-time OLAP data warehouse and provide a set of safe, reliable and flexible real-time data services.</p><p>At present, the newly generated exposure logs every day reach hundreds of millions. The logs willl increase by 10 times if they are stored as SKU. And they would grow to billions of records if based on A/B test. Aggregation queries cross multi-dimension require second-level response time. </p><p>Such an amount of data also brings huge challenges to the team: 2 billion rows have been created daily; Up to 60 million rows need to be imported per minute; Data latency should be limited to 1 minute; MDX query needs to be executed within 3 seconds; QPS has reached above 20. Yet a new reliable OLAP database with high stability should be able to respond to priority 0 emergency.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-evolution-of-the-real-time-architecture">The Evolution of the Real-time Architecture<a href="#the-evolution-of-the-real-time-architecture" class="hash-link" aria-label="Direct link to The Evolution of the Real-time Architecture" title="Direct link to The Evolution of the Real-time Architecture">â€‹</a></h2><p>Our previous architecture is based on Apache Storm for a point-to-point data processing. This approach can quickly meet the needs of real-time reports during the stage of rapid business growth in the early days. However, with the continuous development of business, disadvantages gradually appear. For example, poor flexibility, poor data consistency, low development efficiency and increased resource costs.</p><p><img loading="lazy" alt="page_2" src="https://cdnd.selectdb.com/assets/images/page_2-bc63d65e9c203504cbc7900319d0211c.png" width="1684" height="801" class="img_ev3q"></p><p>In order to solve the problems of the previous architecture, we first upgraded the architecture and replaced Apache Storm with Apache Flink to achieve high throughput. At the same time, according to the characteristics of the search data, the real-time data is processed hierarchically, which means the PV data flow, the SKU data flow and the A/B test data flow are created. It is expected to build the upper real-time OLAP layer based on the real-time flow.</p><p>When selecting OLAP database, the following points need to be considered:</p><ol><li>The data latency is at minute-level and the query response time is at second-level</li><li>Suppots standard SQL, which reduces the cost of use</li><li>Supports JOIN to facilitate adding dimension</li><li>Traffic data can be deduplicated approximately, but order data must be exact deduplicated </li><li>High throughput with tens of millions of records per minute and tens of billions of new records every day</li><li>Query concurrency needs to be high because Front-end may need it</li></ol><p>By comparing the OLAP engines that support real-time import , we made an in-depth comparison among Apache Druid, Elasticsearch, Clickhouse and Apache Doris:</p><p><img loading="lazy" alt="page_3" src="https://cdnd.selectdb.com/assets/images/page_3-578754e222201a65b0601326dc8b298b.png" width="2667" height="778" class="img_ev3q"></p><p>We found out that Doris and Clickhouse can meet our needs. But the concurrency of Clickhouse is low for us, which is a potential risk. Moreover, the data import of Clickhouse has no TRANSACTION and cannot achieve Exactly-once semantics. Clickhouse is not fully supportive of SQL.</p><p>Finally, we chose Apache Doris as our real-time OLAP database. For user behavior log data, we use Aggregation Key data table; As for E-commerce orders data, we use Unique Key data table. Moreover, we split the previous tasks and reuse the logic we tried before. Therefore, when Flink is processing, there will be new topic flow and real-time flow of different granularities generated in DWD. The new architecture is as follows:</p><p><img loading="lazy" alt="page_4" src="https://cdnd.selectdb.com/assets/images/page_4-1f5e1ab38f22766b4ac14b73ee164d59.png" width="3004" height="1571" class="img_ev3q"></p><p>In the current technical architecture, flink task is very light. Based on the production data detail layer, we directly use Doris to act as the aggregation layer function.  And we ask Doris to complete window calculation which previously belongs to Flink. We also take advantage of the routine load to consume real-time data. Although the data is fine-grained before importing, based on the Aggregation Key, asynchronous aggregation will be automatically performed. The degree of aggregation is completely determined by the number of dimensions. By creating Rollup on the base table, double-write or multi-write and pre-aggregate operations are performed during import, which is similar to the function of materialized view, which can highly aggregate data to improve query performance.</p><p>Another advantage of using Kafka to directly connect to Doris at the detail layer is that it naturally supports data backtracking. Data backtracking means that when real-time data is out of order, the &quot;late&quot; data can be recalculated and the previous results can be updated. This is because delayed data can be written to the table whenever it arrives. The final solution is as follows:</p><p><img loading="lazy" alt="page_5" src="https://cdnd.selectdb.com/assets/images/page_5-e8fecc91db2d8fcc3495fb45a0e8e8c2.png" width="1116" height="705" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="optimization-during-the-promotion">Optimization during the Promotion<a href="#optimization-during-the-promotion" class="hash-link" aria-label="Direct link to Optimization during the Promotion" title="Direct link to Optimization during the Promotion">â€‹</a></h2><p>As mentioned above, we have established Aggregation Key of different granularities in Doris, including PV, SKU, and A/B test granularity. Here we take the exposure A/B test model with the largest amount of daily production data as an example to explain how to support the query of tens of billions of records per day during the big promotion period.</p><p>Strategy we used:</p><ul><li>Monitoring: 10, 30, 60 minutes A/B test with indicators, such as exposure PV, UV, exposure SKU pieces, click PV, click UV and CTR.</li><li>Data Modeling: Use exposed real-time data to establish Aggregation Key; And perform HyperLogLog approximate calculation with UV and PV</li></ul><p>Clusters we had:</p><ul><li>30+ virtual machines with storage of NVMe SSD</li><li>40+ partitions exposed by A/B test</li><li>Tens of billions of new data are created every day</li><li>2 Rollups</li></ul><p>Benefits overall:</p><ul><li>Bucket Field can quickly locate tablet partition when querying</li><li>Import 600 million records in 10 minutes</li><li>2 Rollups have relatively low IO, which meet the requirement of the query</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="look-ahead">Look Ahead<a href="#look-ahead" class="hash-link" aria-label="Direct link to Look Ahead" title="Direct link to Look Ahead">â€‹</a></h2><p>JD search box introduced Apache Doris in May 2020, with a scale of 30+ BEs, 10+ routine load tasks running online at the same time. Replacing Flink&#x27;s window computing with Doris can not only improve development efficiency, adapt to dimension changes, but also reduce computing resources. Apache Doris provides unified interface services ensuring data consistency and security.
We are also pushing the upgrade of JD search box&#x27;s OLAP platform to the latest version. After upgrading, we plan to use the bitmap function to support accurate deduplication operations of UV and other indicators. In addition, we also plan to use the appropriate Flink window to develop the real-time stream computing of the aggregation layer to increase the richness and completeness of the data.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/Netease">Apache Doris Helped Netease Create a Refined Operation DMP System</a></h2><div class="blog-info"><time datetime="2022-11-30T00:00:00.000Z" itemprop="datePublished">November 30, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Xiaodong Liu</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>Apache Doris Helped Netease Create a Refined Operation DMP System</h1><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/kv-a63c2e8908df91d10704f971aa636fa6.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>Guide: Refined operation is a trend of the future Internet, which requires excellent data analysis. In this article, you will get knowledge of: the construction of Netease Lifease&#x27;s DMP system and the application of Apache Doris.</p></blockquote><blockquote><p>Author | Xiaodong Liu, Lead Developer, Netease</p></blockquote><p>Better data analysis enables users to get better experience. Currently, the normal analysis method is to build a user tags system to accurately generate user portraits and improve user experience. The topic we shared today is the practice of Netease DMP tags system.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-netease-and-lifease">About Netease and Lifease<a href="#about-netease-and-lifease" class="hash-link" aria-label="Direct link to About Netease and Lifease" title="Direct link to About Netease and Lifease">â€‹</a></h2><p>NetEase (NASDAQ: NTES) is a leading Internet technology company in China, providing users with free emails, gaming, search engine services, news and entertainment, sports, e-commerce and other services.</p><p>Lifease is Netease&#x27;s self-operated home furnishing e-commerce brand. Its products cover 8 categories in total: home life, apparel, food and beverages, personal care and cleaning, baby products, outdoor sport, digital home appliances, and Lifease&#x27;s Special. In Q1 of 2022, Lifease launches &quot;Pro &quot; membership and other multiple memberships for different users. The number of Pro members has increased by 65% â€‹â€‹compared with the previous year.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-dmp-system">About the DMP System<a href="#about-the-dmp-system" class="hash-link" aria-label="Direct link to About the DMP System" title="Direct link to About the DMP System">â€‹</a></h2><p>DMP system plays an important role in Lifease&#x27;s data analysis.
The data sources of DMP mainly include:</p><ul><li>Business logs of APPs, H5s, PCs and other terminals</li><li>Basic data constructed within NetEase Group</li><li>Data from products sold by third-party such as JD.com, Alibaba, and Bytedance
Through data collection and data cleaning, the above data is ingested into data assets. Based on these data, DMP has created a system of functions, such as tag creation, grouping and portrait analysis, which supports the business including: intelligent product matching, user engagement, and user insight. In general, the DMP system concentrates on building a data-centric tagging system and portrait system to assist the business.</li></ul><p>You can get basic knowledge of the DMP system starting from the concepts below:</p><ul><li>Tagging: Tagging is one of the user monitoring abilities to uniquely identify individual users across different browsers, devices, and user sessions. This approach to user tagging works by capturing available data in your application&#x27;s page source: age, address, preference and other variables. </li><li>Targeting: Target audience may be dictated by age, gender, income, location, interests or a myriad of other factors.</li><li>User Portrait Analysis: User portrait analysis is to develop user profiles, actions and attributes after targeting audience. For instance, check the behavior paths and consumption models of users whose portraits are &quot;City: Hangzhou, Gender: Female&quot; on Lifease APP.</li></ul><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/1__core_capability-188f05fadbac0c4dfa3574a4e140cb8b.png" width="1153" height="642" class="img_ev3q"></p><p>Llifease&#x27;s tagging system mainly provides two core capabilities: </p><ol><li>Tag Query: the ability to query the specified tag of a specific entity, which is often used to display basic information. </li><li>Targeting Audience: for both real-time and offline targets. Result after targeting is mainly used for:</li></ol><ul><li>As Grouping Criteria: It can be used to tell if the user is in one or more specified groups. This occasionally occurs in scenarios such as advertising and contact marketing. </li><li>Resultset Pull: Extract specified data to business system for customized development.</li><li>Portrait Analysis: Analyze the behavioral and consumption models in specific groups of people for more refined operations.</li></ul><p>The overall business process is as follows:</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/2__business_process-ca10e9f507ff8157caa521d0c44d7fc4.png" width="1223" height="662" class="img_ev3q"></p><ul><li>First define the rules for tags and grouping;</li><li>After defining the DSL, the task can be submitted to Spark for processing;</li><li>After the processing is done, the results can be stored in Hive and Doris;</li><li>Data from Hive or Doris can be queried and used according to the actual business needs.</li></ul><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/3__dmp_architecture-82a3358b3eb8794fcff543415248505e.png" width="1197" height="706" class="img_ev3q"></p><p>The DMP platform is divided into four modules: Processing&amp;storage layer, scheduling layer, service layer, and metadata management.
All tag meta-information is stored in the source data table; The scheduling layer schedules tasks for the entire business process: Data processing and aggregation are converted into basic tags, and the data in the basic tags and source tables are converted into something that can be used for data query through SQL; The scheduling layer dispatches tasks to Spark to process, and then stores results in both Hive and Doris. The service layer consists of four parts: tag service, entity grouping service, basic tag data service, and portrait analysis service.</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/4__tag_lifecycle-ec086d95f04379a7f9a10993c0089e63.png" width="1124" height="648" class="img_ev3q"></p><p>The lifecycle of tag consists of 5 phases:</p><ul><li>Tag requirements: At this stage, the operation team demands and the product manager team evaluates the rationality and urgency of the requirements.</li><li>Scheduling production: Developers first sort out the data from ODS to DWD, which is the entire link of DM layer. Secondly, they build a model based on data, and at the same time, monitor the production process.</li><li>Targeting Audience: After the tag is produced, group the audience by those tags.</li><li>Precision marketing: Carry out precision marketing strategy to people grouped by.</li><li>Effect evaluation: In the end, tage usage rate and use effect need to be evaluated for future optimization.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="production-of-tags">Production of Tags<a href="#production-of-tags" class="hash-link" aria-label="Direct link to Production of Tags" title="Direct link to Production of Tags">â€‹</a></h2><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/5__production_of_tags-a53f5f1d2e03dc74f8d0e69092e4bd02.png" width="1145" height="675" class="img_ev3q"></p><p>Tag data layering:</p><ul><li>The bottom layer is the ODS layer, including user login logs, event tracking records, transaction data, and Binlog data of various databases</li><li>The data processed by the ODS layer, such as user login table, user activity table and order information table reaches the DWD detail layer</li><li>The DWD layer data is aggregated to the DM layer and the tags are all implemented based on the DM layer data.
At present, we have fully automated the data output from the original database to the ODS layer. And we also realized partial automation from the ODS layer to the DWD layer. And there are a small number of automated operations from the DWD to the DM layer, which will be our focus in the future.</li></ul><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/6__type_of__tags-91b30c2315a91d57aa96017a4ec716eb.png" width="1154" height="677" class="img_ev3q"></p><p>Tags are devided based on timeliness: offline tags, quasi-real-time tags and real-time tags. According to the scale of data, it is divided into: aggregation tags and detail tags. In other cases, tags can also be divided into: account attribute tags, consumption behavior tags, active behavior tags, user preference tags, asset information tags, etc. </p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/7__tags_settings-8a8c1c99a4afbc7f78ceb4659da2c184.png" width="1163" height="672" class="img_ev3q"></p><p>It is inconvenient to use the data of the DM layer directly because the basic data is relatively primitive. The abstraction level is lacking and it is not easy to use. By combining basic data with AND, OR, and NOT, business tags are formed for further use, which can reduce the cost of understanding operations and make it easier to use.</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/8__target_audience-cfe11c32b47db0639303f640a3452d98.png" width="1161" height="696" class="img_ev3q"></p><p>After the tags are merged, it is necessary to apply the tags to specific business scenarios, such as grouping. The configuration is shown on the left side of the figure above, which supports offline crowd packages and real-time behaviors (need to be configured separately). After configuration, generate the DSL rules shown on the right side of the figure above, expressed in Json format, which is more friendly to FE, and can also be converted into query statements of the datebase engine.</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/9__target_audience-mapping-1b00b571d178577b4f0c4f2c8a5b1acf.png" width="1120" height="649" class="img_ev3q"></p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/10__automation-fe72dc6c87f37fdd94f217a9174706bd.png" width="1114" height="649" class="img_ev3q"></p><p>Tagging is partially automated. The degree of automation in grouping is relatively high. For example, group refresh can be done regularly every day; Advanced processing, such as intersection/merge/difference between groups; Data cleaning means timely cleaning up expired and invalid data.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="tags-storage">Tags Storage<a href="#tags-storage" class="hash-link" aria-label="Direct link to Tags Storage" title="Direct link to Tags Storage">â€‹</a></h2><p>Lifease&#x27;s DMP labeling system needs to carry relatively large customer end traffic, and has relatively high requirements for real-time performance. Our storage requirements include:</p><ul><li>Need support high-performance query to deal with large-scale customer end traffic</li><li>Need support SQL to facilitate data analysis scenarios</li><li>Need support data update mechanism</li><li>Can store large amount of data</li><li>Need support for extension functions to handle custom data structures</li><li>Closely integrated with big data ecology</li></ul><p>In the field of big data, multiple engines vary in different applicable scenarios. We used the popular engines in the chart below to optimize our database architecture for 2 times.</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/11__comparision-dd0d69a571e362dcca7711561a30db7c.png" width="1133" height="660" class="img_ev3q"></p><p>Our architecture V1.0 is shown below:</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/12__architecture_v1_0-59dffe2265ac0754860a4bc796c090fa.png" width="1175" height="695" class="img_ev3q"></p><p>Most of the offline data is stored in Hive while a small part is stored in Hbase (mainly used for querying basic tags). Part of the real-time data is stored in Hbase for basic tags query and the rest is double-written into KUDU and Elasticsearch for real-time grouping and data query. The data offline is processed by Impala and cached in Redis.
Disadvantages :</p><ul><li>Too many database engines.</li><li>Double writing has hidden problems with data quality. One side may succeed while the other side fails, resulting in data inconsistency.</li><li>The project is complex and maintainability is poor.
In order to reduce the usage of engine and storage, we improved and implemented version 2.0 :</li></ul><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/13__architecture_v2_0-1f1c2b508793cf146a606b3a453e01a5.png" width="1148" height="677" class="img_ev3q"></p><p>In storage architecture V2.0, Apache Doris is adopted. Offline data is mainly stored in Hive. At the same time, basic tags are imported into Doris, and real-time data as well. The query federation of Hive and Doris is performed based on Spark, and the results are stored in Redis. After this improvement, an storage engine which can manages offline and real-time data has been created. We are currently use Apache Doris 1.0, which enables : 1. The query performance can be controlled within 20ms at 99% 2.  The query performance can be controlled within 50ms at 99.9%.  Now the architecture is simplified, which greatly reduces operation and maintenance costs.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="advantages-of-apache-doris-in-practice">Advantages of Apache Doris in Practice<a href="#advantages-of-apache-doris-in-practice" class="hash-link" aria-label="Direct link to Advantages of Apache Doris in Practice" title="Direct link to Advantages of Apache Doris in Practice">â€‹</a></h2><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/14__advantages_in_practice-3fc1c9893383a6635c8c9612e3ef0a15.png" width="1128" height="658" class="img_ev3q"></p><p>Lifeuse has adopted Apache Doris to check, batch query, path analyse and grouping. The advantages are as follows:</p><ul><li>The query federation performance of  key query and a small number of tables exceeds 10,000 QPS, with RT99&lt;50MS.</li><li>The horizontal expansion capability is relatively strong and maintenance cost is relatively low.</li><li>The offlin and real-time data are unified to reduce the complexity of the tags model.</li></ul><p>The downside is that importing a large amount of small data takes up more resources. But this problem has been optimized in Doris 1.1. Apache Doris has greatly enhanced the data compaction capability in version 1.1, and can quickly complete aggregation of new data, avoiding the -235 error caused by too many versions of sharded data and the low query efficiency problems.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="future-plan">Future Plan<a href="#future-plan" class="hash-link" aria-label="Direct link to Future Plan" title="Direct link to Future Plan">â€‹</a></h2><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/15__future_plan-199b125bad243e0dcd93f00b9f4395fe.png" width="1117" height="652" class="img_ev3q"></p><p>Hive and Spark are gradually turning into Apache Doris.
Optimize the tagging system:</p><ul><li>Establish a rich and accurate tag evaluation system</li><li>Improve tag quality and output speed</li><li>Improve tag coverage
More precision operation:</li><li>Build a rich user analysis model</li><li>Improve the user insight model evaluation system based on the frequency of use and user value</li><li>Establish general image analysis capabilities to assist intelligent decision-making in operations</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/NIO">The Application of Apache Doris in NIO</a></h2><div class="blog-info"><time datetime="2022-11-28T00:00:00.000Z" itemprop="datePublished">November 28, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Huaidong Tang</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>The Application of Apache Doris in NIO</h1><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/assets/images/NIO_kv-7601d71a49c7ecd7fb42f03de600ae6c.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>Guide: The topic of this sharing is the application of Apache Doris in NIO, which mainly includes the following topics:</p><ol><li>Introduction about NIO</li><li>The Development of OLAP in NIO</li><li>Apache Doris-the Unified OLAP Data warehouse</li><li>Best Practice of Apache Doris on CDP Architecture</li><li>Summery and Benefits</li></ol></blockquote><p>Authorï¼šHuaidong Tang, Data Team Leader, NIO INC</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="about-nio">About NIO<a href="#about-nio" class="hash-link" aria-label="Direct link to About NIO" title="Direct link to About NIO">â€‹</a></h2><p>NIO Inc. (NYSE: NIO)is a leading company in the premium smart electric vehicle market. Founded in November 2014, NIO designs, develops, jointly manufactures and sells premium smart electric vehicles, driving innovations in autonomous driving, digital technologies, electric powertrains and batteries.</p><p>Recently, NIO planned to enter the U.S. market alongside other western markets by the end of 2025. The company has already established a U.S. headquarters in San Jose, California, where they started hiring people.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-architecture-evolution-of-olap-in-nio">The Architecture Evolution of OLAP in NIO<a href="#the-architecture-evolution-of-olap-in-nio" class="hash-link" aria-label="Direct link to The Architecture Evolution of OLAP in NIO" title="Direct link to The Architecture Evolution of OLAP in NIO">â€‹</a></h2><p>The architectural evolution of OLAP in NIO took several steps for years.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-introduced-apache-druid">1. Introduced Apache Druid<a href="#1-introduced-apache-druid" class="hash-link" aria-label="Direct link to 1. Introduced Apache Druid" title="Direct link to 1. Introduced Apache Druid">â€‹</a></h3><p>At that time, there were not so many OLAP storage and query engines to choose from. The more common ones were Apache Druid and Apache Kylin. There are 2 reasons why we didn&#x27;t choose Kylin.</p><ul><li><p>The most suitable and optimal storage at the bottom of Kylin is HBase and adding it would increase the cost of operation and maintenance.</p></li><li><p>Kylin&#x27;s precalculation involves various dimensions and indicators. Too many dimensions and indicators would cause great pressure on storage.</p></li></ul><p>We prefer Druid because we used to be users and are familiar with it. Apache Druid has obvious advantages. It supports real-time and offline data import, columnar storage, high concurrency, and high query efficiency. But it has downsides as well:</p><ul><li><p>Standard protocols such as JDBC are not used</p></li><li><p>The capability of JOIN is weak</p></li><li><p>Significant performance downhill when performing dedeplication</p></li><li><p>High in operation and maintenance costs, different components have separate installation methods and different dependencies; Data import needs extra integration with Hadoop and the dependencies of JAR packages</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-introduced-tidb">2. Introduced TiDB<a href="#2-introduced-tidb" class="hash-link" aria-label="Direct link to 2. Introduced TiDB" title="Direct link to 2. Introduced TiDB">â€‹</a></h3><p><strong>TiDB is a mature datawarehouse focused on OLTP+OLAP, which also has distinctive advantages and disadvantages:</strong></p><p>Advantage:</p><ul><li><p>OLTP database, can be updated friendly</p></li><li><p>Supports detailed and aggregated query, which can handle dashboard statistical reports or query of detailed data at the same time</p></li><li><p>Supports standard SQL, which has low cost of use</p></li><li><p>Low operation and maintenance cost</p></li></ul><p>Disadvantages:</p><ul><li><p>It is not an independent OLAP. TiFlash relies on OLTP and will increase storage. Its OLAP ability is insufficient</p></li><li><p>The overall performance should be measured separately by each scene</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-introduced-apache-doris">3. Introduced Apache Doris<a href="#3-introduced-apache-doris" class="hash-link" aria-label="Direct link to 3. Introduced Apache Doris" title="Direct link to 3. Introduced Apache Doris">â€‹</a></h3><p>Since 2021, we have officially introduced Apache Doris. In the process of selection, we are most concerned about various factors such as product performance, SQL protocol, system compatibility, learning and operation and maintenance costs. After deep research and detailed comparison of the following systems, we came to the following conclusions:</p><p><strong>Apache Doris, whose advantages fully meet our demands:</strong></p><ul><li><p>Supports high concurrent query (what we concerned most)</p></li><li><p>Supports both real-time and offline data</p></li><li><p>Supports detailed and aggregated query</p></li><li><p>UNIQ model can be updated</p></li><li><p>The ability of Materialized View can greatly speed up query efficiency</p></li><li><p>Fully compatible with the MySQL protocol and the cost of development is relatively low</p></li><li><p>The performance fully meets our requirements</p></li><li><p>Lower operation and maintenance costs</p></li></ul><p><strong>Moreover, there is another competitor, Clickhouse. Its stand-alone performance is extremely strong, but its disadvantages are hard to accept:</strong></p><ul><li><p>In some cases, its multi-table JOIN is weak</p></li><li><p>Relatively low in concurrency</p></li><li><p>High operation and maintenance costs</p></li></ul><p>With multiple good performances, Apache Doris outstands Druid and TiDB. Meanwhile Clickhouse did not fit well in our business, which lead us to Apache Doris.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="apache-doris-the-unified-olap-datawarehouse">Apache Doris-the Unified OLAP Datawarehouse<a href="#apache-doris-the-unified-olap-datawarehouse" class="hash-link" aria-label="Direct link to Apache Doris-the Unified OLAP Datawarehouse" title="Direct link to Apache Doris-the Unified OLAP Datawarehouse">â€‹</a></h2><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/assets/images/olap-96ad3bb86cebd92a200a0581f0418d3c.png" width="1018" height="669" class="img_ev3q"></p><p>This diagram basically describes our OLAP Architecuture, including data source, data import, data processing, data warehouse, data service and application.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-data-source">1. Data Source<a href="#1-data-source" class="hash-link" aria-label="Direct link to 1. Data Source" title="Direct link to 1. Data Source">â€‹</a></h3><p>In NIO, the data source not only refers to database, but also event tracking data, device data, vehicle data, etc. The data will be ingested into the big data platform. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-data-import">2. Data Import<a href="#2-data-import" class="hash-link" aria-label="Direct link to 2. Data Import" title="Direct link to 2. Data Import">â€‹</a></h3><p>For business data, you can trigger CDC and convert it into a data stream, store it in Kafka, and then perform stream processing. Some data that can only be passed in batches will directly enter our distributed storage.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-data-processing">3. Data Processing<a href="#3-data-processing" class="hash-link" aria-label="Direct link to 3. Data Processing" title="Direct link to 3. Data Processing">â€‹</a></h3><p>We took the Lambda architecture rather than stream-batch integration.</p><p>Our own business determines that our Lambda architecture should be divided into two paths: offline and real-time:</p><ul><li><p>Some data is streamed.</p></li><li><p>Some data can be stored in the data stream, and some historical data will not be stored in Kafka.</p></li><li><p>Some data requires high precision in some circumstances. In order to ensure the accuracy of the data, an offline pipeline will recalculate and refresh the entire data.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-data-warehouse">4. Data Warehouse<a href="#4-data-warehouse" class="hash-link" aria-label="Direct link to 4. Data Warehouse" title="Direct link to 4. Data Warehouse">â€‹</a></h3><p>From data processing to the data warehouse, we did not adopt Flink or Spark Doris Connector. We use Routine Load to connect Apache Doris and Flink, and Broker Load to connect Doris and Spark. The data generated in batches by Spark will be backed up to Hive for further use in other scenarios. In this way, each calculation is used for multiple scenarios at the same time, which greatly improves the efficiency. It also works for Flink.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-data-service">5. Data Service<a href="#5-data-service" class="hash-link" aria-label="Direct link to 5. Data Service" title="Direct link to 5. Data Service">â€‹</a></h3><p>What behind Doris is One Service. By registering the data source or flexible configuration, the API with flow and authority control is automatically generated, which greatly improves flexibility. And with the k8s serverless solution, the entire service is much more flexible.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-application">6. Application<a href="#6-application" class="hash-link" aria-label="Direct link to 6. Application" title="Direct link to 6. Application">â€‹</a></h3><p>In the application layer, we mainly deploy some reporting applications and other services.</p><p>We mainly have two types of scenarios:</p><ul><li><p><strong>User-oriented</strong> , which is similar to the Internet, contains a data dashboard and data indicators.</p></li><li><p><strong>Car-oriented</strong> , car data enters Doris in this way. After certain aggregation, the volume of Doris data is about billions. But the overall performance can still meet our requirements.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="best-practice-of-apache-doris-on-cdp-architecture">Best Practice of Apache Doris on CDP Architecture<a href="#best-practice-of-apache-doris-on-cdp-architecture" class="hash-link" aria-label="Direct link to Best Practice of Apache Doris on CDP Architecture" title="Direct link to Best Practice of Apache Doris on CDP Architecture">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-cdp-architecture">1. CDP Architecture<a href="#1-cdp-architecture" class="hash-link" aria-label="Direct link to 1. CDP Architecture" title="Direct link to 1. CDP Architecture">â€‹</a></h3><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/assets/images/cdp-3d65926e741a2837759b07514e914bbf.png" width="1471" height="422" class="img_ev3q"></p><p>Next, let me introduce Doris&#x27; practice on the operating platform. This is what happens in our real business. Nowadays, Internet companies will make their own CDP, which includes several modules:</p><ul><li><p><strong>Tags</strong> , which is the most basic part.</p></li><li><p><strong>Target</strong> , based on tags, select people according to some certain logic.</p></li><li><p><strong>Insight</strong> , aiming at a group of people, clarify the distribution and characteristics of the group.</p></li><li><p><strong>Touch</strong> , use methods such as text messages, phone calls, voices, APP notifications, IM, etc. to reach users, and cooperate with flow control.</p></li><li><p><strong>Effect analysis,</strong> to improve the integrity of the operation platform, with action, effect and feedback.</p></li></ul><p>Doris plays the most important role here, including: tags storage, groups storage, and effect analysis.</p><p>Tags are divided into basic tags and basic data of user behavior. We can flexibly customize other tags based on those facts. From the perspective of time effectiveness, tags are also divided into real-time tags and offline tags.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-considerations-for-cdp-storage-selection">2. Considerations for CDP Storage Selection<a href="#2-considerations-for-cdp-storage-selection" class="hash-link" aria-label="Direct link to 2. Considerations for CDP Storage Selection" title="Direct link to 2. Considerations for CDP Storage Selection">â€‹</a></h3><p>We took five dimensions into account when we select CDP storage.</p><p><strong>(1) Unification of Offline and Real-time</strong></p><p>As mentioned earlier, there are offline tags and real-time tags. Currently we are close to quasi-real-time. For some data, quasi-real-time is good enough to meet our needs. A large number of tags are still offline tags. The methods used are Doris&#x27;s Routine Load and Broker Load.</p><table><thead><tr><th><strong>Scenes</strong></th><th><strong>Requirements</strong></th><th><strong>Apache Doris&#x27;s Function</strong></th></tr></thead><tbody><tr><td>Real-time tags</td><td>Real-time data updates</td><td>Routine Load</td></tr><tr><td>Offline tags</td><td>Highly efficient batch import</td><td>Broker Load</td></tr><tr><td>Unification of offline and real-time</td><td>Unification of offline and real-time data storage</td><td>Routine Load and Broker Load update different columns of the same table</td></tr></tbody></table><p>In addition, on the same table, the update frequency of different columns is also different. For example, we need to update the user&#x27;s identity in real time because the user&#x27;s identity changes all the time. T+1&#x27;s update does not meet our needs. Some tags are offline, such as the user&#x27;s gender, age and other basic tags, T+1 update is sufficient to meet our standards. The maintenance cost caused by putting the tags of basic users on the same table is very low. When customizing tags later, the number of tables will be greatly reduced, which benefits the overall performance.</p><p><strong>(2) Efficient Targets</strong></p><p>When users tags are done, is time to target right group of people. The target is to filter out all the people who meet the conditions according to different combinations of tags. At this time, there will be queries with different combinations of tag conditions. There was an obvious improvement when Apache Doris upgraded to vectorization.</p><table><thead><tr><th><strong>Scenes</strong></th><th><strong>Requirements</strong></th><th><strong>Apache Doris&#x27;s Function</strong></th></tr></thead><tbody><tr><td>Complex Condition Targets</td><td>Highly efficient combination of tags</td><td>Optimization of SIMD</td></tr></tbody></table><p><strong>(3) Efficient Polymerization</strong></p><p>The user insights and effect analysis statistics mentioned above require statistical analysis of the data, which is not a simple thing of obtaining tags by user ID. The amount of data read and query efficiency have a great impact on the distribution of our tags, the distribution of groups, and the statistics of effect analysis. Apache Doris helps a lot:</p><ul><li><p>Data Partition. We shard the data by time order and the analysis and statistics will greatly reduce the amount of data, which can greatly speed up the efficiency of query and analysis.</p></li><li><p>Node aggregation. Then we collect them for unified aggregation.</p></li><li><p>Vectorization. The vectorization execution engine has significant performance improvement.</p></li></ul><table><thead><tr><th><strong>Scenes</strong></th><th><strong>Requirements</strong></th><th><strong>Apache Doris&#x27;s Function</strong></th></tr></thead><tbody><tr><td>Distribution of Tags Values</td><td>The distribution values â€‹â€‹of all tags need to be updated every day. Fast and efficient statistics are required</td><td>Data partition lessens data transfer and calculation</td></tr><tr><td>Distribution of Groups</td><td>Same as Above</td><td>Unified storage and calculation, each node aggregates first</td></tr><tr><td>Statistics for Performance Analysis</td><td>Same as Above</td><td>Speed up SIMD</td></tr></tbody></table><p><strong>(4) Multi-table Association</strong></p><p>Our CDP might be different from common CDP scenarios in the industry, because common CDP tags in some scenarios are estimated in advance and no custom tags, which leaves the flexibility to users who use CDP to customize tags themselves. The underlying data is scattered in different database tables. If you want to create a custom tag, you must associate the tables.</p><p>A very important reason we chose Doris is the ability to associate multiple tables. Through performance tests, Apache Doris is able to meet our requirements. And Doris provides users with powerful capabilities because tags are dynamic.</p><table><thead><tr><th><strong>Scenes</strong></th><th><strong>Requirements</strong></th><th><strong>Apache Doris&#x27;s Function</strong></th></tr></thead><tbody><tr><td>Distributed Characteristics of the Population</td><td>The distribution of statistical groups under a certain characteristic</td><td>Table Association</td></tr><tr><td>Single Tag</td><td>Display tags</td><td></td></tr></tbody></table><p><strong>(5) Query Federation</strong></p><p>Whether the user is successfully reached or not will be recorded in TiDB. Notifications during operations may only affect user experience. If a transaction is involved, such as gift cards or coupons, the task execution must be done without repetition. TiDB is more suitable for this OLTP scenario.</p><p>But for effect analysis, it is necessary to understand the extent to which the operation plan is implemented, whether the goal is achieved and its distribution. It is necessary to combine task execution and group selection for analysis, which requires the query association between Doris and TiDB.</p><p>The size of the tag is probably small, so we would like to save it into Elasticsearch. However, it proves us wrong later.</p><table><thead><tr><th><strong>Scenes</strong></th><th><strong>Requirements</strong></th><th><strong>Apache Doris&#x27;s Function</strong></th></tr></thead><tbody><tr><td>Effect Analysis Associated with Execution Details</td><td>Doris query associated with TiDB</td><td>Query Association with other databases</td></tr><tr><td>Group Tags Associated with Behavior Aggregation</td><td>Doris query associated with Elasticsearch</td><td></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summery-and-benefits">Summery and Benefits<a href="#summery-and-benefits" class="hash-link" aria-label="Direct link to Summery and Benefits" title="Direct link to Summery and Benefits">â€‹</a></h2><ol><li><p><strong>bitmap</strong>. Our volume are not big enough to test its full efficiency. If the volume reaches a certain level, using bitmap might have a good performance improvement. For example, when calculating UV , bitmap aggregation can be considered if the full set of Ids is greater than 50 million.</p></li><li><p><strong>The performance is good</strong> when Elasticsearch single-table query is associated with Doris.</p></li><li><p><strong>Better to update columns in batches</strong>. In order to reduce the number of tables and improve the performance of the JOIN table, the table designed should be as streamlined as possible and aggregated as much as possible. However, fields of the same type may have different update frequencies. Some fields need to be updated at daily level, while others may need to be updated at hourly level. Updating a column alone is an important requirement. The solution from Apache Doris is to use REPLACE<!-- -->_<!-- -->IF<!-- -->_<!-- -->NOT<!-- -->_<!-- -->NULL. Note: It is impossible to replace the original non-null value with null. You can replace all nulls with meaningful default values, such as unknown.</p></li><li><p><strong>Online Services</strong>. Apache Doris serves online and offline scenarios at the same time, which requires high resource isolation.</p></li></ol></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/scenario">How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario</a></h2><div class="blog-info"><time datetime="2022-11-24T00:00:00.000Z" itemprop="datePublished">November 24, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Zhao Wei</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>How Does Apache Doris Help AISPEACH Build a Datawherehouse in AI Chatbots Scenario</h1><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/assets/images/kv-7d5af44f82188444fd1c6ac613c1d7eb.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>Guide: In 2019, AISPEACH built a real-time and offline datawarehouse based on Apache Doris. Reling on its flexible query model, extremely low maintenance costs, high development efficiency, and excellent query performance, Apache Doris has been used in many business scenarios such as real-time business operations, AI chatbots analysis. It meets various data analysis needs such as device portrait/user label, real-time operation, data dashboard, self-service BI and financial reconciliation. And now I will share our experience through this article.</p></blockquote><p>Authorï½œZhao Wei, Head Developer of AISPEACH&#x27;s Big Data Departpment</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="backgounds">Backgounds<a href="#backgounds" class="hash-link" aria-label="Direct link to Backgounds" title="Direct link to Backgounds">â€‹</a></h2><p>AISPEACH is a professional conversational artificial intelligence company in China. It has full-link intelligent voice and language technology. It is committed to becoming a platform-based enterprise for full-link intelligent voice and language interaction. Recently it has developed a new generation of human-computer interaction platform DUI and artificial intelligence chip TH1520, providing natural language interaction solutions for partners in many industry scenarios such as Internet of Vehicles, IoT, government affairs and fintech.</p><p>Aspire introduced Apache Doris for the first time in 2019 and built a real-time and offline data warehouse based on Apache Doris. Compared with the previous architecture, Apache Doris has many advantages such as flexible query model, extremely low maintenance cost, high development efficiency and excellent query performance. Multiple business scenarios have been applied to meet various data analysis needs such as device portraits/user tags, real-time operation of business scenarios, data analysis dashboards, self-service BI, and financial reconciliation.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-evolution">Architecture Evolution<a href="#architecture-evolution" class="hash-link" aria-label="Direct link to Architecture Evolution" title="Direct link to Architecture Evolution">â€‹</a></h2><p>Offline data analysis in the early business was our main requirement. Recently, with the continuous development of business, the requirements for real-time data analysis in business scenarios have become higher and higher. The early datawarehouse architecture failed to meet our requirements. In order to meet the higher requirements of business scenarios for query performance, response time, and concurrency capabilities, Apache Doris was officially introduced in 2019 to build a real-time and offline integrated datawarehouse architecture.</p><p>In the following I will introduce the evolution of the AISPEACH Data Warehouse architecture, and share the reasons why we chose Apache Doris to build a new architecture.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="early-data-warehouse-architecture">Early Data Warehouse Architecture<a href="#early-data-warehouse-architecture" class="hash-link" aria-label="Direct link to Early Data Warehouse Architecture" title="Direct link to Early Data Warehouse Architecture">â€‹</a></h3><p>As shown in the architecture diagram below, the offline data warehouse is based on Hive + Kylin while the real-time data warehouse is based on Spark + MySQL.</p><p><img loading="lazy" alt="data_wharehouse_architecture_v1_0_git" src="https://cdnd.selectdb.com/assets/images/data_wharehouse_architecture_v1_0_git-006b22817872b04ad8f909e54e8c1411.png" width="1953" height="1106" class="img_ev3q"></p><p>There are three main types of data sources in our business, business databases such as MySQL, application systems such as K8s container service logs, and logs of automotive T-Box. Data sources are first written to Kafka through various methods such as MQTT/HTTP protocol, business database Binlog, and Filebeat log collection. In the early time, the data will be divided into real-time and offline links after passing through Kafka. Real-time part has a shorter link. The data buffered by Kafka is processed by Spark and put into MySQL for further analysis. MySQL can basically meet the early analysis requirements. After data cleaning and processing by Spark, an offline datawarehouse is built in Hive, and Apache Kylin is used to build Cube. Before building Cube, it is necessary to design the data model in advance, including association tables, dimension tables, index fields, and aggregation functions. After construction through the scheduling system, we can finally use HBase to store the Cube.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="pain-points-of-early-architecture">Pain Points of Early Architectureï¼š<a href="#pain-points-of-early-architecture" class="hash-link" aria-label="Direct link to Pain Points of Early Architectureï¼š" title="Direct link to Pain Points of Early Architectureï¼š">â€‹</a></h4><ol><li><p><strong>There are many dependent components.</strong> Kylin strongly relies on Hadoop and HBase in versions 2.x and 3.x. The large number of application components leads to low development efficiency, many hidden dangers of architecture stability, and high maintenance costs.</p></li><li><p><strong>The construction process of Kylin is complicated and the construction task always fail.</strong> When we do construction for Kylin, we always need to do the following: widen tables, de-duplicate columns, generate dictionaries, build cubes, etc. If there are 1000-2000 or more tasks per day, at least 10 or more tasks will fail to build, resulting in a lot of time to write automatic operation and maintenance scripts.</p></li><li><p><strong>Dimension/dictionary expansion is heavy.</strong> Dimension expansion refers to the need for multiple analysis conditions and fields in some business scenarios. If many fields are selected in the data analysis model without pruning, it will lead to severe cube dimension expansion and longer construction time. Dictionary inflation means that in some scenarios, it takes a long time to do global accurate deduplication, which will make the dictionary construction bigger and bigger, and the construction time will become longer and longer, resulting in a continuous decline in data analysis performance.</p></li><li><p><strong>The data analysis model is fixed and low in flexibility.</strong> In the actual application, if a calculation field or business scenario is changed, some or even all of the data needs to be backtracked.</p></li><li><p><strong>Data detail query is not supported.</strong> The early data warehouse architecture could not provide detailed data query. The official Kylin solution is to relate to Presto for detailed query, which introduces another architecture and increases development costs.</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="architecture-selection">Architecture Selection<a href="#architecture-selection" class="hash-link" aria-label="Direct link to Architecture Selection" title="Direct link to Architecture Selection">â€‹</a></h3><p>In order to solve the problems above, we began to explore other datawarehouse architecture solutions. And we conducted a series of research on OLAP engines such as Apache Doris and Clickhouse, which are most widely used in the market.</p><p>As the original creator, SelectDB provides commercial support and services for Apache Doris. With the new Apache Doris, SelectDB is now providing global users with a fully-managed database option for deployment.</p><p>Comparing with ClickHouse&#x27;s heavy maintenance, various table types, and lack of support for associated queries, Apache Doris performed better. And combined with our OLAP analysis scenario, we finally decided to introduce Apache Doris.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="new-data-warehouse-architecture">New Data Warehouse Architecture<a href="#new-data-warehouse-architecture" class="hash-link" aria-label="Direct link to New Data Warehouse Architecture" title="Direct link to New Data Warehouse Architecture">â€‹</a></h3><p><img loading="lazy" alt="data_wharehouse_architecture_v2_0_git" src="https://cdnd.selectdb.com/assets/images/data_wharehouse_architecture_v2_0_git-825df043f0abf0fda4a92b8dc5d10956.png" width="1993" height="1144" class="img_ev3q"></p><p>As shown in the figure above, we built a new real-time + offline data warehouse architecture based on Apache Doris. Unlike the previous architecture, real-time and offline data are processed separately and written to Apache Doris for analysis.</p><p>Due to some historical reasons, data migration is difficult. The offline data is basically consistent with the previous datawarehouse architecture, and it is entirely possible to directly build an offline data warehouse on Apache Doris.</p><p>Comparing with the earlier architecture, the offline data is cleaned and processed by Spark, which is possible to build data warehouse in Hive. Then the data stored in Hive can be written to Apache Doris through Broker Load. What I want to explain here is that the data import speed of Broker Load is very fast and it only takes 10-20 minutes to import 100-200G data into Apache Doris on a daily basis.</p><p>When it comes to the real-time data flow, the new architecture uses Doris-Spark-Connector to consume data in Kafka and write it to Apache Doris after simple tasks. As shown in the architecture diagram, real-time and offline data are analyzed and processed in Apache Doris, which meets the business requirements of data applications for both real-time and offline.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="benefits-of-the-new-architecture">Benefits of the New Architecture:<a href="#benefits-of-the-new-architecture" class="hash-link" aria-label="Direct link to Benefits of the New Architecture:" title="Direct link to Benefits of the New Architecture:">â€‹</a></h4><ol><li><p><strong>Simplified operation, low maintenance cost, and does not depend on Hadoop ecological components.</strong> The deployment of Apache Doris is simple. There are only two processes of FE and BE. Both FE and BE processes can be scaled out. A single cluster supports hundreds of machines and tens of PB storage capacity. These two types of processes pass the consistency agreement to ensure high availability of services and high reliability of data. This highly integrated architecture design greatly reduces the operation and maintenance cost of a distributed system. The operation and maintenance time spent in the three years of using Doris is very small. Comparing with the previous architecture based on Kylin, the new architecture spends little time on operation and maintenance.</p></li><li><p><strong>The difficulty of developing and troubleshooting problems is greatly reduced.</strong> The real-time and offline unified data warehouse based on Doris supports real-time data services, interactive data analysis, and offline data processing scenarios, which greatly reduces the difficulty of troubleshooting.</p></li><li><p><strong>Apache Doris supports JOIN query in Runtime format.</strong> Runtime is similar to MySQL&#x27;s table association, which is friendly to the scene where the data analysis model changes frequently, and solves the problem of low flexibility in the early structured data model.</p></li><li><p><strong>Apache Doris supports JOIN, aggregation, and detailed query at the same time.</strong> Meanwhile, it solves the problem that data details could not be queried in the previous architecture.</p></li><li><p><strong>Apache Doris supports multiple accelerated query methods.</strong> And it also supports rollup index, materialized view, and implements secondary index through rollup index to speed up query, which greatly improves query response time.</p></li><li><p><strong>Apache Doris supports multiple types of Query Federation.</strong> And it supports Federation Query analysis on data lakes such as Hive, Iceberg, and Hudi, and also databases such as MySQL and Elasticsearch.</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="applications">Applications<a href="#applications" class="hash-link" aria-label="Direct link to Applications" title="Direct link to Applications">â€‹</a></h2><p>Apache Doris was first applied in real-time business and AI Chatbots analysis scenarios in AISPEACH. This chapter will introduce the requirements and applications of the two scenarios.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="real-time-business">Real-time Business<a href="#real-time-business" class="hash-link" aria-label="Direct link to Real-time Business" title="Direct link to Real-time Business">â€‹</a></h3><p><img loading="lazy" alt="real-time_operation_git" src="https://cdnd.selectdb.com/assets/images/real-time_operation_git-87d6e8ede096ba1551cb290941741126.png" width="1977" height="1226" class="img_ev3q"></p><p>As shown in the figure above, the technical architecture of the real-time operation business is basically the same as the new version of the data warehouse architecture mentioned above:</p><ul><li><p>Data Source: The data source is consistent in the new version with the architecture diagram in the new version, including business data in MySQL, event tracking data of the application system, device and terminal logs.</p></li><li><p>Data Import: Broker Load is used for offline data import, and Doris-Spark-Connector is used for real-time data import.</p></li><li><p>Data Storage and Development: Almost all real-time data warehouses are built on Apache Doris, and some offline data is placed on Airflow to perform DAG batch tasks.</p></li><li><p>Data Application: The top layer is the business analysis requirements, including large-screen display, real-time dashboard for data operation, user portrait, BI tools, etc.</p></li></ul><p><strong>In real-time operation business, there are two main requirements for data analysis:</strong></p><ul><li><p>Due to the large amount of real-time imported data, the query efficiency requirement is high.</p></li><li><p>In this scenario, a team of 20+ people is in charge. The data operation dashboard needs to be opened at the same time, so there will be relatively high requirements for real-time writing performance and query concurrency.</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="ai-chatbots-analysis">AI Chatbots Analysis<a href="#ai-chatbots-analysis" class="hash-link" aria-label="Direct link to AI Chatbots Analysis" title="Direct link to AI Chatbots Analysis">â€‹</a></h3><p>In addition, the second application of Apache Doris in AISPEACG is a AI Chatbots analysis.</p><p><img loading="lazy" alt="ai_chatbots_git" src="https://cdnd.selectdb.com/assets/images/ai_chatbots_git-f094d1221b56b522cb93ba3bc766e659.png" width="1953" height="1118" class="img_ev3q"></p><p>As shown in the figure above, different from normal BI cases, our users only needs to describe the data analysis needs by typing. Based on our company&#x27;s NLP capabilities, AI Chatbots BI will convert natural language into SQL, which similar to NL2SQL technology. It should be noted that the natural language analysis used here is customized. Comparing with open source NL2SQL, the hit rate is high and the analysis is more precise. After the natural language is converted into SQL, the SQL will give Apache Doris query to get the analysis result. As a result, users can view detailed data in any cases at any time by typing. <strong>Compared with pre-computed OLAP engines such as Apache Kylin and Apache Druid, Apache Doris performs better for the following reasons:</strong></p><ul><li><p>The query is flexible and the model is not fixed, which supports customization.</p></li><li><p>It needs to support table association, aggregation calculation, and detailed query.</p></li><li><p>Response time needs to be fast.</p></li></ul><p>Therefore, we have successfully implemented AI Chatbots analysis by using Apache Doris. At the same time, feedback on the application in our company is awesome.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="experience">Experience<a href="#experience" class="hash-link" aria-label="Direct link to Experience" title="Direct link to Experience">â€‹</a></h2><p>Based on the above two scenarios, we have accumulated some experience and insights and I will share them with you now.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="datawarehouse-table-design">Datawarehouse Table Design:<a href="#datawarehouse-table-design" class="hash-link" aria-label="Direct link to Datawarehouse Table Design:" title="Direct link to Datawarehouse Table Design:">â€‹</a></h3><ol><li><p>Tables which contain about tens of millions of data(for reference, related to the size of the cluster) is better to use the Duplicate table type. The Duplicate table type supports aggregation and detailed query at the same time, without additional detailed tables required.</p></li><li><p>When the amount of data is relatively large, we suggest to use the Aggregate aggregation table type, build a rollup index on the aggregation table type, use materialized views to optimize queries, and optimize aggregation fields.</p></li><li><p>When the amount of data is large with many associated tables, ETL can be used to write wide tables, imports to Doris, combined with Aggregate to optimize the aggregation table type. Or we suggest you use the official Doris JOIN optimization refer to: https://doris .apache.org/en-US/docs/dev/advanced/join-optimization/doris-join-optimization</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="storage">Storage:<a href="#storage" class="hash-link" aria-label="Direct link to Storage:" title="Direct link to Storage:">â€‹</a></h3><p>We use SSD and HDD to separate hot and warm data storage. Data within the past year is stored in SSD, and data more than one year is stored in HDD. Apache Doris supports setting cooling time for partitions. The current solution is to set automatic synchronization to migrate historical data from SSD to HDD to ensure that the data within one year is placed in on the SSD.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="upgrade">Upgrade<a href="#upgrade" class="hash-link" aria-label="Direct link to Upgrade" title="Direct link to Upgrade">â€‹</a></h3><p>Make sure to back up the metadata before upgrading. You can also use the method of starting a new cluster to back up the data files to a remote storage system such as S3 or HDFS through Broker, and then import the previous cluster data into the new cluster through backup and recovery.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="performance-comparison">Performance Comparison<a href="#performance-comparison" class="hash-link" aria-label="Direct link to Performance Comparison" title="Direct link to Performance Comparison">â€‹</a></h3><p>Aspire started using Apache Doris from version 0.12. This year we completed the upgrade from version 0.15 to the latest version 1.1, and conducted performance tests based on real business data.</p><p><img loading="lazy" alt="doris_1_1_performance_test_git" src="https://cdnd.selectdb.com/assets/images/doris_1_1_performance_test_git-ad375d6872f12ab1e3cca76d30caa1f6.png" width="1961" height="1126" class="img_ev3q"></p><p>As can be seen from the test report, among the 13 SQLs test in total, the performance difference of the first 3 SQLs after the upgrade is not obvious, because these 3 scenarios are mainly simple aggregation functions, which do not require high performance of Apache Doris. Version 0.15 can meet demand. In the scenario after Q4, SQL is more complex while Group By needs multiple fields, aggregation functions and complex functions. Therefore, the performance improvement after upgrading is obvious to see: the average query performance is 2- 3 times. We highly recommend that you upgrade to the latest version of Apache Doris.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="summary-and-benefits">Summary and Benefits<a href="#summary-and-benefits" class="hash-link" aria-label="Direct link to Summary and Benefits" title="Direct link to Summary and Benefits">â€‹</a></h2><ol><li><p>Apache Doris supports the construction of offline plus real-time unified data warehouses. One ETL script can support both real-time and offline data warehouses, which greatly greatly improved efficiency, reduces storage costs, and avoids problems such as inconsistencies between offline and real-time indicators.</p></li><li><p>Apache Doris 1.1.x version fully supports vectorization, which improves the query performance by 2-3 times compared with the previous version. After testing, the query performance of Apache Doris version 1.1.x in the wide table is equal to that of ClickHouse.</p></li><li><p>Apache Doris is powerful and does not depend on other components. Compared with Apache Kylin, Apache Druid, ClickHouse, Apache Doris does not need a second component to fill the technical gap. Apache Doris supports aggregation, detailed queries, and associated queries. Currently, more than 90% of AISPEACH&#x27; analysis have migrated to Apache Doris. Thanks to this advantage, developers operate and maintain fewer components, which greatly reduces the cost of operation and maintenance.</p></li><li><p>It is extremely easy to use, supporting MySQL protocol and standard SQL, which greatly reduces user learning costs.</p></li></ol><p><em>Special thanks to SelectDB, the company building Apache Doris helps us work with the community and get sufficient technical support.</em></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/jd">Best Practice of Apache Doris in JD</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">July 20, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1><strong>Introductionï¼š</strong></h1><p>Apache Doris is an open source MPP analytical database product that not only can get query results in sub-second response time, effectively supporting real-time data analysis, but also supports huge data sets of more than 10PB. Compared with other industry-hot OLAP database systems, the distributed architecture of Apache is very simple. Itsupports elastic scaling and is easy to operate and maintain, saving a lot of labor and time costs. At present, the domestic community is very popular , and there are also many companies which have large scale uses, such as Meituan and Xiaomi,etc. </p><p>This paper mainly discusses how to use Doris for business exploration and practice in the multi-dimensional analysis of real-time and offline data in the large real-time screen of JD customer service in the scenarios of manual consultation, customer event list, after-sales service list, etc.</p><p>In recent years, with the explosive growth of data volume and the emergence of the demand for online analysis of massive data, traditional relational databases such as MySQL and Oracle have encountered bottlenecks under large data volume, while databases such as Hive and Kylin lack timeliness. So Apache Doris, Apache Druid, ClickHouse and other real-time analytic databases begun to appear, not only to cope with the second-level queries of massive data, but also to meet the real-time and quasi-real-time analysis needs. Offline and real-time computing engines are in full bloom. But for different scenarios and facing different problems, no single engine is a panacea. We hope that this article can give you some inspiration on the application and practice of offline and real-time analytics in JD&#x27;s customer service business, and we hope you will communicate more and give us valuable suggestions.</p><h1><strong>JD Customer Service Business Form</strong></h1><p>As the entrance to the group&#x27;s services, JD Customer Service provides efficient and reliable protection for users and merchants. JD customer service is responsible for solving users&#x27; problems in a timely manner and providing them with detailed and easy-to-understand instructions and explanations; in order to better understand users&#x27; feedback and the status of products, it is necessary to monitor a series of indicators such as the number of inquiries, pick-up rates, complaints, etc. in real time, and discover problems in a timely manner through ring comparison and year-on-year comparison, in order to better adapt to users&#x27; shopping styles, improve service quality and efficiency, and thus enhance the brand of JD influence.</p><h1><strong>Easy OLAP Design</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-easyolap-doris-data-import-links"><strong>01 EasyOLAP Doris Data Import Links</strong><a href="#01-easyolap-doris-data-import-links" class="hash-link" aria-label="Direct link to 01-easyolap-doris-data-import-links" title="Direct link to 01-easyolap-doris-data-import-links">â€‹</a></h3><p>EasyOLAP Doris data sources are mainly real-time Kafka and offline HDFS files. The import of real-time data relies on Routine Load; offline data is mainly imported using Broker Load and Stream Load.</p><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/assets/images/jd03-00bd471f0fab2d98798f5e3148b35fce.png" width="1080" height="604" class="img_ev3q"></p><p>EasyOLAP Doris Data Import Links</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-easyolap-doris-full-link-monitor"><strong>02 EasyOLAP Doris Full Link Monitor</strong><a href="#02-easyolap-doris-full-link-monitor" class="hash-link" aria-label="Direct link to 02-easyolap-doris-full-link-monitor" title="Direct link to 02-easyolap-doris-full-link-monitor">â€‹</a></h3><p>The EasyOLAP Doris project currently uses the Prometheus + Grafana framework for monitoring. The node_exporter is responsible for collecting machine-level metrics, and Doris automatically spits out FE and BE service-level metrics in Prometheus format. In addition, OLAP Exporter service is deployed to collect Routine Load related metrics, aiming to discover real-time data stream import at the first time and ensure real-time data timeliness.</p><p><img loading="lazy" alt="EasyOLAP Doris monitoring link" src="https://cdnd.selectdb.com/assets/images/jd04-8770adfb04ffe977f931d9eaff4cb534.png" width="1080" height="594" class="img_ev3q"></p><p>EasyOLAP Doris monitoring link</p><p><img loading="lazy" alt="640" src="https://cdnd.selectdb.com/assets/images/jd01-47257e8bb0b14785f854db959cdfd931.png" width="871" height="600" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-easyolap-doris-primary-secondary-dual-stream-design"><strong>03 EasyOLAP Doris Primary-Secondary Dual Stream Design</strong><a href="#03-easyolap-doris-primary-secondary-dual-stream-design" class="hash-link" aria-label="Direct link to 03-easyolap-doris-primary-secondary-dual-stream-design" title="Direct link to 03-easyolap-doris-primary-secondary-dual-stream-design">â€‹</a></h3><p>EasyOLAP Doris adopts a dual-write approach for the primary and secondary clusters in order to guarantee the service stability of Level 0 services during the promotion time.</p><p><img loading="lazy" alt="03 EasyOLAP Doris Primary-Secondary Dual Stream Design" src="https://cdnd.selectdb.com/assets/images/jd02-a6a4279c0c33a25862e89b56e7c986a7.png" width="1080" height="669" class="img_ev3q"></p><p>EasyOLAP Doris Primary-Secondary Dual Stream Design</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-easyolap-doris-dynamic-partition-management"><strong>04 EasyOLAP Doris Dynamic Partition Management</strong><a href="#04-easyolap-doris-dynamic-partition-management" class="hash-link" aria-label="Direct link to 04-easyolap-doris-dynamic-partition-management" title="Direct link to 04-easyolap-doris-dynamic-partition-management">â€‹</a></h3><p>After analyzing the requirements, the JD OLAP team did some customization of Doris, which involved dynamic partition management. Although the community version already had the function of dynamic partitioning, the function could not retain partitions of a specified time. For the characteristics of JD Group, we have retained historical data of specified time, such as data during 618 and 11.11, which will not be deleted due to dynamic partitioning. The dynamic partition management feature can control the amount of data stored in the cluster, and it is easy to use by the business side without the need to manage partition information manually or with additional code.</p><h1><strong>Doris Caching Mechanism</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-demand-scenarios"><strong>01 Demand Scenarios</strong><a href="#01-demand-scenarios" class="hash-link" aria-label="Direct link to 01-demand-scenarios" title="Direct link to 01-demand-scenarios">â€‹</a></h3><p>Committed to continuously improving user experience, JD Customer Service&#x27;s data analysis pursues the ultimate timeliness. Offline data analysis scenario is write less read more, data is written once and read frequently many times; real-time data analysis scenario, part of the data is not updated historical partition, part of the data is in the updated partition. In most analysis applications, there are the following scenarios:</p><ul><li><p>High concurrency scenario: Doris better support high concurrency, but too high QPS will cause cluster jitter, and a single node can not carry too high QPS;.</p></li><li><p>Complex queries: JD customer service real-time operation platform monitoring needs to display multi-dimensional complex indicators according to business scenarios, rich indicators display corresponding to a variety of different queries, and data sources from multiple tables . Although the response time of individual queries at milliseconds level , the overall response time may be at the second level.</p></li><li><p>Repeated queries: if there is no anti-refresh mechanism, repeatedly refreshing the page will lead to the submission of a large number of repeated queries due to delays or hand errors.</p></li></ul><p>For the above scenario, there are solutions at the application layer â€”â€” the query results are put into Redis and the cache is refreshed periodically or manually by the user, but there are some problemsï¼š</p><ul><li><p>Data inconsistency: can not respond immediately to data updates, and the user may receive results with old data.</p></li><li><p>Low hit rate: if the data is highly real-time and the cache is frequently invalidated, the hit rate of the cache is low and the load on the system cannot be relieved.</p></li></ul><p>Additional cost: introduction of external components increases system complexity and adds additional cost.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-introduction-to-caching-mechanism"><strong>02 Introduction to Caching Mechanism</strong><a href="#02-introduction-to-caching-mechanism" class="hash-link" aria-label="Direct link to 02-introduction-to-caching-mechanism" title="Direct link to 02-introduction-to-caching-mechanism">â€‹</a></h3><p>There are three different types of Cache in EasyOLAP Doris, respectively Result Cache, SQL Cache and Partition Cache, depending on the applicable scenario. All three types of caches can be switched on and off by MySQL client commands.</p><p>These three caching mechanisms can coexist: which can be turned on at the same time. When querying, the query parser first determines whether the Result Cache is enabled or not, and if the Result Cache is enabled, it first finds out whether the cache exists for the query from the Result Cache, and if the cache fails or does not exist, it directly takes the cached value and returns it to the client. The cache is placed in the memory of each FE node for fast reading.</p><p>SQL Cache stores and gets the cache according to the signature of SQL, the ID of the partition of the queried table, and the latest version number of the partition. These three together serve as cache conditions. If one of these three conditions is changed, such as SQL statement change or partition version number change after data update, the cache will not be hit. In the case of multiple table joins, the partition update of one of the tables will also result in failure to hit the cache. SQL Cache is more suitable for T+1 update scenarios.</p><p>Partition Cache is a more fine-grained caching mechanism. Partition cache mainly splits a query into read-only partition and updatable partition in parallel based on partition, read-only partition is cached, updatable partition is not cached, and the corresponding result set is generated n, and then the results of each split subquery are merged. Therefore, if the query N days of data, data update the most recent D days, each day is only a different date range but similar queries, you can use Partition Cache, only need to query D partitions can be, the other parts are from the cache, can effectively reduce the cluster load, shorten the query response time.</p><p>When a query enters Doris, the system will first process the query statement and take it as the key, before executing the query statement, the query analyzer can automatically select the most suitable caching mechanism to ensure that the caching mechanism is used to shorten the query response time in the best case. Then, it checks whether the query result exists in the Cache, and if it does, it gets the data in the cache and returns it to the client; if it does not, it queries normally and stores the query result as Value and the query statement Key in the cache. SQL Cache is more suitable for T+1 scenarios and works well when partition updates are infrequent and SQL statements are repetitive Partition Cache is the least granular cache. When a query statement queries data for a time period, the query statement is split into multiple subqueries. It can shorten the query time and save cluster resources when the data is written to only one partition or partial partition.</p><p>To better observe the effectiveness of caching, metrics have been added to Doris&#x27; service metrics, which are monitored visually through Prometheus and Grafana monitoring systems. The metrics include the number of hits for different types of Cache, the hit rate for different types of Cache, the memory size of the Cache, and other metrics.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-caching-mechanism-effect"><strong>03 Caching Mechanism Effect</strong><a href="#03-caching-mechanism-effect" class="hash-link" aria-label="Direct link to 03-caching-mechanism-effect" title="Direct link to 03-caching-mechanism-effect">â€‹</a></h3><p>For the JD Customer Service Doris main cluster, some services reached 100% CPU usage during 11.11 period without caching on; with Result Cache on, CPU usage was between 30% and 40%. The caching mechanism ensures that the business can get the query results quickly and protects the cluster resources well under high concurrency scenarios.</p><h1><strong>Doris&#x27; optimization during the 11.11 sale, 2020</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-import-task-optimization"><strong>01 Import Task Optimization</strong><a href="#01-import-task-optimization" class="hash-link" aria-label="Direct link to 01-import-task-optimization" title="Direct link to 01-import-task-optimization">â€‹</a></h3><p>The import of real-time data has always been a challenge. Among them, ensuring real-time data and importing stability is the most important. In order to observe the real-time data import situation more intuitively, JD OLAP team developed OLAP Exporter independently to collect real-time data import-related metrics, such as import speed, import backlog and suspended tasks. The import speed and import backlog can be used to determine the status of a real-time import task, and if find a trend of backlog, the sampling tool developed independently can be used to sample and analyze the real-time task. Real-time tasks have three main thresholds to control the submission of tasks, which are the maximum processing interval per batch, the maximum number of processing entries per batch and the maximum amount of data processed per batch, and a task will be submitted as soon as one of these thresholds is reached. By increasing the logs, we found that the task queue in FE was relatively busy, so the parameters were mainly adjusted to make the maximum number of processing entries per batch and the maximum amount of data processed per batch larger, and then the maximum processing interval per batch was adjusted to ensure that the data latency was within twice the maximum processing interval per batch according to the business requirements. Through the sampling tool, the analysis task ensures not only the real-time data, but also the stability of the import. In addition, we also set up alarms to detect abnormalities such as backlog of real-time import tasks and suspension of import tasks in a timely manner.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-monitoring-metrics-optimization"><strong>02 Monitoring Metrics Optimization</strong><a href="#02-monitoring-metrics-optimization" class="hash-link" aria-label="Direct link to 02-monitoring-metrics-optimization" title="Direct link to 02-monitoring-metrics-optimization">â€‹</a></h3><p>The monitoring metrics are divided into two main sections, a machine level metrics section and a business level metrics section. In the whole monitoring panel, detailed metrics bring comprehensive data and at the same time make it more difficult to get important metrics. So, to get a better view of important metrics for all clusters, a separate panel is created - 11.11 Important Metrics Summary Panel. The board contains metrics such as BE CPU usage, real-time task consumption backlog rows, TP99, QPS, and so on. The number of metrics is small, but the situation of all clusters can be observed, which can eliminate the trouble of frequent switching in monitoring.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-peripheral-tools-support"><strong>03 Peripheral Tools Support</strong><a href="#03-peripheral-tools-support" class="hash-link" aria-label="Direct link to 03-peripheral-tools-support" title="Direct link to 03-peripheral-tools-support">â€‹</a></h3><p>In addition to the sampling tools and OLAP Exporter mentioned above, the JD OLAP team has also developed a series maintenance tools for Doris.</p><ol><li>Import sampling tool: The import sampling tool not only collects the data imported in real time, but also supports adjusting the parameters of the real time import task, or generating creation statements (including the latest loci and other information) for task migration and other operations when the real time import task is paused.</li></ol><ol start="2"><li>Big query tool: Big queries not only cause jitter in cluster BE CPU usage, but also lead to longer response time for other queries. Before the Big Query tool, if you found jitter in cluster CPU, you needed to check the audit logs on all FEs and then do the statistics, which is not only time-consuming but also not intuitive. The Big Query tool is designed to solve the above problem. When the monitoring side finds that the cluster has jitter, you can use the Big Query tool and enter the cluster name and time point to get the total number of queries for different services at that time point, the number of queries with more than 5 seconds, 10 seconds, 20 seconds, the number of queries with huge scanning volume, etc. It is convenient for us to analyze the big queries from different dimensions. The details of the big queries will also be saved in the intermediate file, which can directly get the big queries of different businesses. The whole process only takes a few tens of seconds to a minute to locate the big query that is happening and get the corresponding query statements, which greatly saves time and operation and maintenance costs.</li></ol><ol start="3"><li>Downgrade and recovery tools: In order to ensure the stability of the Level 0 business during the 11.11 promotion, when the cluster pressure exceeds the safety level, it is necessary to downgrade other non-Level 0 businesses, and then restore them to the pre-downgrade settings with one click after the peak period. The degradation mainly involves reducing the maximum number of connections to the service, suspending non-level 0 real-time import tasks, and so on. This greatly increases the ease of operation and improves efficiency.</li></ol><ol start="4"><li>Cluster inspection tool: During 11.11 period, the health inspection of clusters is extremely important. Routine inspections include primary and secondary cluster consistency checks for dual-stream services. In order to ensure that the business can quickly switch to the other cluster when one cluster has problems, it is necessary to ensure that the library tables on both clusters are consistent and the data volume is not too different; check whether the number of copies of the library tables is 3 and whether there are unhealthy Tablet in the cluster; check the machine disk utilization, memory and other machine-level indicators, etc. Check the machine disk utilization, memory and other machine-level metrics, etc.</li></ol><h1><strong>Summary &amp; Outlook</strong></h1><p>   JD Customer Service was introduced to Doris in early 2020, and currently has one standalone cluster and one shared cluster, and is an experienced user of JD OLAP.</p><p>   In the business use, we also encountered problems such as task scheduling-related, import task configuration-related and query-related problems, which are driving the JD OLAP team to understand Doris more deeply. We plan to promote the use of materialized views to further improve the efficiency of queries; use Bitmap to support accurate de-duplication of UV and other metrics; use audit logs to make it easier to count large and slow queries; and solve the scheduling problem of real-time import tasks to make them more efficient and stable. In addition, we also plan to optimize table building, create high-quality Rollup or materialized views to improve the smoothness of the application, and accelerate more businesses to the OLAP platform to improve the impact of the application.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/meituan">Best Practice of Apache Doris in Meituan</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">July 20, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>Best Practice of Apache Doris in Meituan</h1><p>Introduction: This paper mainly introduces a general method and practice of real-time data warehouse construction. The real-time data warehouse aims at end-to-end low latency, SQL standardization, rapid response to changes, and data unification. In practice, the best practice we summarize is: a common real-time production platform + a common interactive real-time analysis engine cooperate with each other to meet real-time and quasi-real-time business scenarios. The two have a reasonable division of labor and complement each other to form an easy-to-develop, easy-to-maintain, and most efficient assembly line, taking into account development efficiency and production costs, and satisfying diverse business needs with a better input-output ratio.</p><h1>real-time scene</h1><p>There are many scenarios in which real-time data is delivered in Meituan, mainly including these following points:</p><ul><li>Operational level: Such as real-time business changes, real-time marketing effects, daily business status and daily real-time business trend analysis, etc.</li><li>Production level: such as whether the real-time system is reliable, whether the system is stable, real-time monitoring of the health of the system, etc.</li><li>C-end users: For example, search recommendation sorting requires real-time understanding of users&#x27; thoughts, behaviors and characteristics, and recommendation of more concerned content to users.</li><li>Risk control: Food delivery and financial technology are used a lot. Real-time risk identification, anti-fraud, abnormal transactions, etc., are all scenarios where a large number of real-time data are applied</li></ul><h1>Real-time technology and architecture</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1real-time-computing-technology-selection">1.Real-time computing technology selection<a href="#1real-time-computing-technology-selection" class="hash-link" aria-label="Direct link to 1.Real-time computing technology selection" title="Direct link to 1.Real-time computing technology selection">â€‹</a></h3><p>At present, there are many open source real-time technologies, among which Storm, Spark Streaming and Flink are common. The specific selection depends on the business situation of different companies.</p><p>Meituan Takeaway relies on the overall construction of meituan&#x27;s basic data system. In terms of technology maturity, It used Storm a few years ago, which was irreplaceable in terms of performance stability, reliability and scalability. As Flink becomes more and more mature, it has surpassed Storm in terms of technical performance and framework design advantages. In terms of trends, just like Spark replacing MR, Storm will be gradually replaced by Flink. Of course, there will be a process of migrating from Storm to Flink. We currently have some old tasks still on Storm, and we are constantly promoting task migration.</p><p>The comparison between Storm and Flink can refer to the form above.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2real-time-architecture">2.Real-time Architecture<a href="#2real-time-architecture" class="hash-link" aria-label="Direct link to 2.Real-time Architecture" title="Direct link to 2.Real-time Architecture">â€‹</a></h3><h4 class="anchor anchorWithStickyNavbar_LWe7" id="-lambda-architecture">â‘  Lambda Architecture<a href="#-lambda-architecture" class="hash-link" aria-label="Direct link to â‘  Lambda Architecture" title="Direct link to â‘  Lambda Architecture">â€‹</a></h4><p>The Lambda architecture is a relatively classic architecture. In the past, there were not many real-time scenarios, mainly offline. When a real-time scene is attached, the technical ecology is different due to the different timeliness of offline and real- time. The Lambda architecture is equivalent to attaching a real-time production link, which is integrated at the application level, and two-way production is independent of each other.This is also a logical approach to adopt in business applications.</p><p>There will be some problems in dual-channel production, such as double processing logic, double development and operation and maintenance, and resources will also become two resource links. Because of these problems,  Kappa architecture has been evolved.</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="-kappa-architecture">â‘¡ Kappa Architecture<a href="#-kappa-architecture" class="hash-link" aria-label="Direct link to â‘¡ Kappa Architecture" title="Direct link to â‘¡ Kappa Architecture">â€‹</a></h4><p>The Kappa architecture is relatively simple in terms of architecture design, unified in production, and a set of logic produces both offline and real time. However, there are relatively large limitations in practical application scenarios. There are few cases in the industry that directly use the Kappa architecture for production and implementation,  and the scene is relatively simple. These problems will also be encountered on our side, and we will also have some thoughts of our own, which will be discussed later.</p><h1>Business Pain Points</h1><p>In the take-away business, we also encountered some problems.</p><p>In the early stage of the business, in order to meet the business needs, the requirements are generally completed case by case after the requirements are obtained. The business has high real-time requirements. From the perspective of timeliness, there is no opportunity for middle-level precipitation. In the scenario, the business logic is generally directly embedded. This is a simple and effective method that can be imagined. This development mode is relatively common in the early stage of business development.</p><p>As shown in the figure above, after getting the data source, it will go through data cleaning, dimension expansion, business logic processing through Storm or Flink, and finally direct business output. Taking this link apart, the data source will repeatedly refer to the same data source, and the operations such as cleaning, filtering, and dimension expansion must be repeated. The only difference is that the code logic of the business is different. IIf there is less business, this model is acceptable, but when the subsequent business volume increases, there will be a situation where whoever develops will be responsible for operation and maintenance, the maintenance workload will increase, and the operations cannot be managed in a unified manner. Moreover, everyone is applying for resources, resulting in a rapid expansion of resource costs, and resources cannot be used intensively and effectively. Therefore, it is necessary to think about how to construct real-time data from the whole data source.</p><h1>Data features and Application Scenario</h1><p>So how to build a real-time data warehouse?</p><p>First of all, we need to disassemble this task into what data, what scenarios, and what features these scenarios have in common. For takeaway business scenarios, there are two categories, log class and business category.</p><ul><li><p>Log class: It is characterized by a large amount of data, semi-structured, and deeply nested.Log data has a great feature that once the log stream is formed, it will not change. It will collect all the logs of the platform by means of buried points, and then collect and distribute them uniformly. Just like a tree with really large roots.  The whole process of pushing to the front-end application is just like the process of a tree branching from the root to a branch (the decomposition process from 1 to n). If all businesses search for data from the root, although the path seems to be the shortest, because of the heavy burden,the data retrieval efficiency is low. Log data is generally used for production monitoring and user behavior analysis. The timeliness requirements are relatively high . Generally, the  time window will be 5 minutes or 10 minutes, or up to the current state. The main application is the real-time large screen and real-time features, such as behaviour can immediately perceive the need for waiting every time the user clicks.</p></li><li><p>Business category: The business class is mainly about business transaction data. Business systems are usually self-contained and distribute data down in the form of Binlog logs. All business systems are transactional, mainly using paradigm modeling methods, which have a structured characteristic and the main part can be seen clearly. However, due to the large number of data tables, multi-table associations are required to express the complete business. So it&#x27;s an integrated machining process from n to 1 .</p></li></ul><p>Several difficulties faced by business real-time processing:</p><ul><li><p>Diversity of business: Business processes are constantly changing from the beginning to the end, such as from ordering -&gt; payment -&gt; delivery. The business database is changed  on the original basis,and Binlog will generate a lot of changed logs. Business analysis is more focused on the end state, which leads to the problem of data retraction calculation, such as placing an order at 10 o&#x27;clock and canceling it at 13 o&#x27;clock, but hoping to subtract the canceled order at 10 o&#x27;clock.</p></li><li><p>Business integration: Business analysis data usually cannot be expressed by a single subject, and often many tables are associated to obtain the desired information. When confluent alignment of data is performed in real-time streaming, it often requires large cache processing and is complicated.</p></li><li><p>The analysis is batch, and the processing process is streaming: for a single data, no analysis can be formed, so the analysis object must be batch, and the data processing is one by one.</p></li></ul><p>The scenarios of log classes and business classes generally exist at the same time and are intertwined. Whether it is Lambda architecture or Kappa architecture, a single application will have some problems, so it is more meaningful to choose the architecture and practice according to the scenario.</p><h1>Architecture Design of Real-time Data Warehouse</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1real-time-architecture-exploration-of-stream-batch-combination">1.Real-time Architecture: Exploration of Stream-Batch Combination<a href="#1real-time-architecture-exploration-of-stream-batch-combination" class="hash-link" aria-label="Direct link to 1.Real-time Architecture: Exploration of Stream-Batch Combination" title="Direct link to 1.Real-time Architecture: Exploration of Stream-Batch Combination">â€‹</a></h3><p>Based on the above problems, we have our own thinking and ideasï¼Œit is to deal with different business scenarios through the combination of flow and batch.</p><p>As shown in the figure above, the data is collected from the log to the message queue, and then to the ETL process of the data stream. The construction of the basic data stream is unified. Afterwards, for log real-time features, real-time large-screen applications use real-time stream computing. Real-time OLAP batch processing is used for Binlog business analysis.</p><p>What are the Pain Points of Stream Processing Analysis Business? For the paradigm business, both Storm and Flink require a large amount of external memory to achieve business alignment between data streams, which requires a lot of computing resources. Due to the limitation of external memory, the window limitation strategy must be carried out, and may eventually discard some data as a result. After calculation, it is generally stored in Redis as query support, and KV storage has many limitations in dealing with analytical query scenarios.</p><p>How to achieve real-time OLAP? Is there a real-time computing engine with its own storage, when the real-time data is entered,it can flexibly and freely calculate within a certain range, and has a certain data carrying capacity, and supports analysis of query responses at the same time? With the development of technology, the current MPP engine is developing very rapidly, and its performance is also improving rapidly, so there is a new possibility in this scenario, just like the Doris engine we use here.</p><p>This idea has been practiced in the industry and has become an important exploration direction. For example, Alibaba&#x27;s real-time OLAP solution based on ADB, etc.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2architecture-design-of-real-time-data-warehouse">2.Architecture Design of Real-time Data Warehouse<a href="#2architecture-design-of-real-time-data-warehouse" class="hash-link" aria-label="Direct link to 2.Architecture Design of Real-time Data Warehouse" title="Direct link to 2.Architecture Design of Real-time Data Warehouse">â€‹</a></h3><p>From the perspective of the entire real-time data warehouse architecture, the first thing to consider is how to manage all real-time data, how to effectively integrate resources, and how to construct data.</p><p>In terms of methodology, the real-time and offline are very similar to each other. In the early stage of offline data warehouse, it is also case by case. Consider how to govern it when the scale of data increases to a certain amount. We all know that layering is a very effective way of data governing. So, on the issue of how to manage the real-time data warehouse, the first consideration is also the hierarchical processing logic, as follows:</p><ul><li><p>Data source: At the data source level, offline and real-time data sources are consistent. They are mainly divided into log classes and business classes. Log classes include user logs, DB logs, and server logs.</p></li><li><p>Real-time detail layer: At the detail level, in order to solve the problem of repeated construction, a unified construction should be carried out.Using the offline data warehouse model to build a unified basic detailed data layer, managed according to the theme, the purpose of the detail layer is to provide directly available data downstream, so the basic layer should be processed uniformly, such as cleaning, filtering, and dimension expansion.</p></li><li><p>Aggregation layer: The summary layer can directly calculate the result through the concise operator of Flink or Storm. And form a summary of indicators, all indicators are processed at the summary layer, and everyone manages and constructs according to unified specifications, forming a reusable summary result.</p></li></ul><p>In conclusion, from the perspective of the construction of the entire real-time data warehouse,first of all, the data construction needs to be layered, build the framework first, and set the specifications includs  what extent each layer is processed and how each layer is used.The definition of specifications facilitates standardized processing in production.Due to the need to ensure timeliness, don&#x27;t design too many layers when designing.For scenarios with high real-time requirements, you can basically refer to the left side of the figure above. For batch processing requirements, you can import from the real-time detail layer to the real-time OLAP engine, and perform fast retraction calculations based on the OLAP engine&#x27;s own calculation and query capabilities, as shown in the data flow on the right side of the figure above.</p><h1>Real-time platform construction</h1><p>After the architecture is determined, the next consideration is how to build a platform.The construction of the real-time platform is completely attached to the real-time data warehouse management.</p><p>First, abstract the functions and abstract them into components, so that standardized production can be achieved, and systematic guarantees can be further constructed. For the basic processing layer cleaning, filtering, confluence, dimension expansion, conversion, encryption, screening and other functions can be abstracted, and the base layer builds a directly usable data result stream in this componentized way. How to meet diverse needs and how to be compatible with users are the problems that we need to figure out. In this case it may occur problems with redundant processing. In terms of storage, real-time data does not have a history and will not consume too much storage. This redundancy is acceptable.The production efficiency can be improved by means of redundancy, which is an ideological application of changing space for time.</p><p>Through the processing of the base layer, all data is deposited in the IDL layer, and written to the base layer of the OLAP engine at the same time, and then the real-time summary layer is calculated. Based on Storm, Flink or Doris, multi-dimensional summary indicators are produced to form a unified summary layer for unified storage distribution.</p><p>When these functions are available, system capabilities such as metadata management, indicator management, data security, SLA, and data quality will be gradually built.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1real-time-base-layer-functions">1.Real-time base layer functions<a href="#1real-time-base-layer-functions" class="hash-link" aria-label="Direct link to 1.Real-time base layer functions" title="Direct link to 1.Real-time base layer functions">â€‹</a></h3><p>The construction of the real-time base layer needs to solve some problems.</p><p>The first is the problem of repeated reading of a stream. When a Binlog is called, it exists in the form of a DB package. Users may only use one of the tables. If everyone wants to use it, there may be a problem that everyone needs to access this stream. The solution can be deconstructed according to different businesses, restored to the basic data flow layer, made into a paradigm structure according to the needs of the business, and integrated with the theme construction according to the modeling method of the data warehouse.</p><p>Secondly, we need to encapsulate components, such as basic layer cleaning, filtering, and dimension expansion . Users can write logic by a very simple expression. Trans part is more flexible. For example, converting from one value to another value, for this custom logic expression, we also open custom components, which can develop custom scripts through Java or Python for data processing.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2real-time-feature-production-capabilities">2.Real-time feature production capabilities<a href="#2real-time-feature-production-capabilities" class="hash-link" aria-label="Direct link to 2.Real-time feature production capabilities" title="Direct link to 2.Real-time feature production capabilities">â€‹</a></h3><p>Feature production can be expressed logically through SQL syntax, and the underlying logic is adapted, and transparently transmitted to the computing engine, shielding the user&#x27;s dependence on the computing engine.Just like for offline scenarios, currently large companies rarely develop through code, unless there are some special cases, so they can basically be expressed in SQL.</p><p>At the functional level, the idea of indicator management is integrated. Atomic indicators, derived indicators, standard calculation apertures, dimension selection, window settings and other operations can be configured in a configurable way.In this way, the production logic can be uniformly parsed and packaged uniformly.</p><p>Another question,with the same source code a lot of SQL is written, and each submission will have a data stream which is a waste of resources.Our solution is to produce dynamic metrics through the same data stream, so that metrics can be added dynamically without stopping the service.</p><p>So, during the construction of the real-time platform, engineers should consider more about how to use resources more effectively and which links can use resources more economically.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3sla-construction">3.SLA construction<a href="#3sla-construction" class="hash-link" aria-label="Direct link to 3.SLA construction" title="Direct link to 3.SLA construction">â€‹</a></h3><p>SLA mainly solves two problems, one is about the end-to-end SLA, the other is  about the SLA of job productivity. We adopt the method of burying points + reporting.Because the real-time stream is relatively large, the burying point should be as simple as possible, do not bury too many things,can express the business information is enough.The output of each job is reported to the SLA monitoring platform in a unified manner, and the required information is reported at each job point through a unified interface, and finally the end-to-end SLA can be counted.</p><p>In real-time production, because the process is very long, it is impossible to control all links, but it can control the efficiency of its own operations, so job SLA is also essential.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4real-time-olap-solution">4.Real-time OLAP solution<a href="#4real-time-olap-solution" class="hash-link" aria-label="Direct link to 4.Real-time OLAP solution" title="Direct link to 4.Real-time OLAP solution">â€‹</a></h3><p>Problems:</p><ul><li><p>Binlog business restoration is complexï¼šThere are many changes in the business, and changes at a certain point in time are required. Therefore, sorting and data storage are required, which consumes a lot of memory and CPU resources.</p></li><li><p>Binlog business association is complexï¼šIn stream computing, the relationship between streams and streams is very difficult to express business logic.</p></li></ul><p>solutionsï¼š</p><p>To solve the problem through the OLAP engine with computing power, there is no need to logically map a data stream, and only the problem of real-time and stable data storage needs to be solved.</p><p>We use Doris as a high-performance OLAP engine here.Due to the need for derivative calculations between the results generated by the business data and the results, Doris can quickly restore the business by using the unique model or the aggregation model, and can also perform aggregation at the summary layer while restoring the business,and is also designed for reuse.The application layer can be physical or logical view.</p><p>This mode focuses on solving the business rollback calculation. For example, when the business state changes, the value needs to be changed at a certain point in history. The cost of using flow calculation in this scenario is very high. The OLAP mode can solve this problem very well.</p><h1>Real-time use cases</h1><p>In the end, we use a case to illustrate.For example, merchants want to offer discounts to users based on the number of historical orders placed by users. Merchants need to see how many orders they have placed in history. They must have historical T+1 data and real-time data today.This scenario is a typical Lambda architecture,You can design a partition table in Doris, one is the historical partition, and the other is the today partition. The historical partition can be produced offline. Today&#x27;s indicators can be calculated in real time and written to today&#x27;s partition. When querying, a simple summary.</p><p>This scenario seems relatively simple, but the difficulty lies in the fact that many simple problems will become complicated after the number of merchants increases.Therefore, in the future, we will use more business input to precipitate more business scenarios, abstract them to form a unified production plan and function, and support diversified business needs with minimized real-time computing resources, which is also what needs to be achieved in the future. </p><p>That&#x27;s all for today, thank you.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="about-the-author">about the author:<a href="#about-the-author" class="hash-link" aria-label="Direct link to about the author:" title="Direct link to about the author:">â€‹</a></h3><p>Zhu Liang, more than 5 years experience in data warehouse construction in traditional industries, 6 years experience in Internet data warehouse, technical direction involves offline, real-time data warehouse management, systematic capacity building, OLAP system and engine, big data related technologies, focusing on OLAP,and real-time technology frontier development trends.The business direction involves ad hoc query, operation analysis, strategy report product, user portrait, crowd recommendation, experimental evaluation, etc.</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/blog/xiaomi">Best Practice of Apache Doris in Xiaomi Group</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">July 20, 2022</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">Best Practice</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>Background</h1><p>In order to improve the query performance of the Xiaomi growth analysis platform and reduce the operation and maintenance costs, Xiaomi Group introduced Apache Doris in September 2019. In the past two and a half years, <strong>Apache Doris has been widely used in Xiaomi Group,</strong> <strong>such as business growth analytic platform, realtime dashboards for all business groups,  finance analysis, user profile analysis, advertising reports, A/B testing platform and so on.</strong> This article will share the best practice of Apache Doris in Xiaomi Group. </p><h1>Business Practice</h1><p>The typical business practices of Apache Doris in Xiaomi are as follows:</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-user-access">01 User Access<a href="#01-user-access" class="hash-link" aria-label="Direct link to 01 User Access" title="Direct link to 01 User Access">â€‹</a></h2><p>Data Factory is a one-stop data development platform developed by Xiaomi for data developers and data analysts. This platform supports data sources such as Doris, Hive, Kudu, Iceberg, ES, Talso, TiDB, MySQL, etc. It also supports computing engines such as Flink, Spark,  Presto,etc.</p><p>Inside Xiaomi, users need to access the Doris service through the data factory. Users need to register in the data factory and complete the approval for building the database. The Doris operation and maintenance classmates will connect according to the descriptions of the business scenarios and data usage expectations submitted by users in the data factory. After completing the access approval, users can use the Doris service to perform operations such as visual table creation and data import in the data factory.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-data-import">02 Data import<a href="#02-data-import" class="hash-link" aria-label="Direct link to 02 Data import" title="Direct link to 02 Data import">â€‹</a></h2><p>In Xiaomi&#x27;s business, the two most common ways to import data into Doris are Stream Load and Broker Load. User data will be divided into real-time data and offline data, and users&#x27; real-time and offline data will generally be written to Talos first (Talos is a distributed, high-throughput message queue developed by Xiaomi). The offline data from Talos will be sink to HDFS, and then imported to Doris through the data factory. Users can directly submit Broker Load tasks in the data factory to import large batches of data on HDFS into Doris, In addition, you can run the SparkSQL command in the data factory to query data from Hive, Import the data found in SparkSQL into Doris through Spark-doris-Connector, and encapsulate Stream Load at the bottom layer of Spark-doris-Connector. Real-time data from Talos is generally imported into Doris in two ways. One is to first perform ETL on the data through Flink, and then import small batches of data to Doris through.Flink- Doris-connector encapsulates the Stream Load at the bottom layer. Another way is to import small batches of data into Doris through Stream Load encapsulated by Spark Streaming at regular intervals.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03-data-query">03 Data Query<a href="#03-data-query" class="hash-link" aria-label="Direct link to 03 Data Query" title="Direct link to 03 Data Query">â€‹</a></h2><p>Doris users of Xiaomi generally analyze and query Doris and display the results through the ShuJing platform.ShuJing is a general-purpose BI analysis tool developed by Xiaomi. Users can query and visualize Doris through ShuJing  platform, and realize user behavior analysis (in order to meet the needs of business event analysis, retention analysis, funnel analysis, path analysis and other behavior analysis needs, We added corresponding UDF and UDAF ) and user profile analysis for Doris.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-compaction-tuning">04 Compaction Tuning<a href="#04-compaction-tuning" class="hash-link" aria-label="Direct link to 04 Compaction Tuning" title="Direct link to 04 Compaction Tuning">â€‹</a></h2><p>For Doris, each data import will generate a data version under the relevant data shard (Tablet) of the storage layer, and the Compaction mechanism will asynchronously merge the smaller data versions generated by the import (the detailed principle of the Compaction mechanism can be Refer to the previous article &quot;Doris Compaction Mechanism Analysis&quot;).</p><p>Xiaomi has many high-frequency, high-concurrency, near-real-time import business scenarios, and a large number of small versions will be generated in a short period of time. If Compaction does not merge data versions in time, it will cause version accumulation.On the one hand, too many minor versions will increase the pressure on metadata, and on the other hand, too many versions will affect query performance.In Xiaomi&#x27;s usage scenarios, many tables use the Unique and Aggregate data models, and the query performance is heavily dependent on whether Compaction can merge data versions in time.In our business scenario, the query performance was reduced by tens of times due to delayed version merging, thus affecting online services.When a Compaction happens, it consumes CPU, memory, and disk I/O resources. Too much compaction will take up too many machine resources, affect query performance, and may cause OOM.</p><p><strong>In response to this problem of Compaction, we first start from the business side and guide users through the following aspects:</strong></p><ul><li><p>Set reasonable partitions and buckets for tables to avoid generating too many data fragments.</p></li><li><p>Standardize the user&#x27;s data import operation, reduce the frequency of data import, increase the amount of data imported in a single time, and reduce the pressure of Compaction.</p></li><li><p>Avoid using delete operations too much.The delete operation will generate a delete version under the relevant data shard in the storage layer.The Cumulative Compaction task will be truncated when the delete version is encountered. This task can only merge the data version after the Cumulative Point and before the delete version, move the Cumulative Point to the delete version, and hand over the delete version to the subsequent Base Compaction task. to process. If you use the delete operation too much, too many delete versions will be generated under the Tablet, which will cause the Cumulative Compaction task to slow down the progress of version merging. Using the delete operation does not actually delete the data from the disk, but records the deletion conditions in the delete version. When the data is queried, the deleted data will be filtered out by Merge-On-Read. Only the delete version is merged by the Base Compaction task. After that, the data to be deleted by the delete operation can be cleared from the disk as expired data with the Stale Rowset. If you need to delete the data of an entire partition, you can use the truncated partition operation instead of the delete operation.</p></li></ul><p><strong>Second, we tuned Compaction from the operation and maintenance side:</strong></p><ul><li><p>According to different business scenarios, different Compaction parameters (Compaction strategy, number of threads, etc.) are configured for different clusters.</p></li><li><p>Appropriately lowers the priority of the Base Compaction task and increases the priority of the Cumulative Compaction task, because the Base Compaction task takes a long time to execute and has serious write amplification problems, while the Cumulative Compaction task executes faster and can quickly merge a large number of small versions.</p></li><li><p>Version backlog alarm, dynamic adjustment of Compaction parameters.When the Compaction Producer produces Compaction tasks, it will update the corresponding metric.It records the value of the largest Compaction Score on the BE node. You can check the trend of this indicator through Grafana to determine whether there is a version backlog. In addition, we have added a Version backlog alert.In order to facilitate the adjustment of Compaction parameters, we have optimized the code level to support dynamic adjustment of the Compaction strategy and the number of Compaction threads at runtime, avoiding the need to restart the process when adjusting the Compaction parameters.</p></li><li><p>Supports manual triggering of the Compaction task of the specified Table and data shards under the specified Partition, and improves the Compaction priority of the specified Table and data shards under the specified Partition.</p></li></ul><h1>Monitoring and Alarm Management</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-monitoring-system">01 Monitoring System<a href="#01-monitoring-system" class="hash-link" aria-label="Direct link to 01 Monitoring System" title="Direct link to 01 Monitoring System">â€‹</a></h2><p>Prometheus will regularly pull Metrics metrics from Doris&#x27;s FE and BE and display them in the Grafana monitoring panel.The service metadata based on QingZhou Warehouse will be automatically registered in Zookeeper, and Prometheus will regularly pull the latest cluster metadata information from Zookeeper and display it dynamically in the Grafana monitoring panel.ï¼ˆQingzhou Data Warehouse is a data warehouse constructed by the Qingzhou platform based on the operation data of Xiaomi&#x27;s full-scale big data service. It consists of 2 base tables and 30+ dimension tables.Covers the whole process data such as resources, server cmdb, cost, process status and so on when big data components are runningï¼‰We have also added statistics and display boards for common troubleshooting data such as Doris large query list, real-time write data volume, data import transaction numbers, etc. in Grafana.In Grafana, we also added statistics and display boards for common troubleshooting data such as the Doris big query list, the amount of real-time data written, and the number of data import transactions, so that alarms can be linked. When the cluster is abnormal, Doris&#x27; operation and maintenance students can locate the cause of the cluster failure in the shortest time.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02--falcon">02  Falcon<a href="#02--falcon" class="hash-link" aria-label="Direct link to 02  Falcon" title="Direct link to 02  Falcon">â€‹</a></h2><p>Falcon is a monitoring and alarm system widely used inside Xiaomi.Because Doris provides a relatively complete metrics interface, which can easily provide monitoring functions based on Prometheus and Grafana, we only use Falcon&#x27;s alarm function in the Doris service.For different levels of faults in Doris, we define alarms as three levels of P0, P1 and P2:</p><ul><li><p>P2 alarm (alarm level is low): single node failure alarm.When a single node indicator or process status is abnormal, an alarm is generally issued as a P2 level.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.(Xiaomi Office is a privatized deployment product of ByteDance Feishu in Xiaomi, and its functions are similar to Feishu.)</p></li><li><p>P1 alarm (alarm level is higher):In a short period of time (within 3 minutes), the cluster will issue a P1 level alarm if there are short-term exceptions such as increased query delay and abnormal writing,etc.The alarm information is sent to the members of the alarm group in the form of Xiaomi Office messages.P1 level alarms require Oncall engineers to respond and provide feedback.</p></li><li><p>P0 alarm (alarm level is high):In a long period of time (more than 3 minutes), the cluster will issue a P0 level alarm if there are exceptions such as increased query delay and abnormal writing,etc.Alarm information is sent in the form of Xiaomi office messages and phone alarms.P0 level alarm requires Oncall engineers to respond within 1 minute and coordinate resources for failure recovery and review preparation.</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="03--cloud-doris">03  Cloud-Doris<a href="#03--cloud-doris" class="hash-link" aria-label="Direct link to 03  Cloud-Doris" title="Direct link to 03  Cloud-Doris">â€‹</a></h2><p>cloud-Doris is a data collection component developed by Xiaomi for the internal Doris service. Its main capability is to detect the availability of the Doris service and collect the cluster indicator data of internal concern.For example, Cloud-Doris can periodically simulate users reading and writing to the Doris system to detect the availability of services.If the cluster has abnormal availability, it will be alerted through Falcon.Collect user&#x27;s read and write data, and then generate user bill.Collect information such as table-level data volume, unhealthy copies, and oversized Tablets, and send alarms to abnormal information through Falcon.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="04-qingzhou-inspection">04 QingZhou inspection<a href="#04-qingzhou-inspection" class="hash-link" aria-label="Direct link to 04 QingZhou inspection" title="Direct link to 04 QingZhou inspection">â€‹</a></h2><p>For chronic hidden dangers such as capacity, user growth, resource allocation, etc., we use the unified QingZhou big data service inspection platform for inspection and reporting.The inspection generally consists of two parts:Service-specific inspections and basic indicator inspections.Among them, the service-specific inspection refers to the indicators that are unique to each big data service and cannot be used universally.For Doris, it mainly includes: Quota, number of shard copies, number of single table columns, number of table partitions, etc.By increasing the inspection method, the chronic hidden dangers that are difficult to be alarmed in advance can be well avoided, which provides support for the failure-free major festivals.</p><h1>Failure Recovery</h1><p>When an online cluster fails, the first principle should be to quickly restore services.If the cause of the failure is clear, handle it according to the specific cause and restore the service.If the cause of the failure is not clear, you should try restarting the process as soon as you keep the snapshot to restore the service.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="01-access-failures-handling">01 Access Failures Handling<a href="#01-access-failures-handling" class="hash-link" aria-label="Direct link to 01 Access Failures Handling" title="Direct link to 01 Access Failures Handling">â€‹</a></h2><p>Doris uses Xiaomi LVS as the access layer, which is similar to the LB service of open source or public cloud, and provides layer 4 or layer 7 traffic load scheduling capability.After Doris binds a reasonable port,Generally speaking, if an abnormality occurs in a single FE node, it will be automatically kicked out, and the service can be restored without the user&#x27;s perception, and an alarm will be issued for the abnormal node.Of course, for FE faults that cannot be processed in a short time, we will first adjust the weight of the faulty node to 0 or delete the abnormal node from LVS first to prevent unpredictable problems caused by process detection exceptions.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="02-node-failure-handling">02 Node Failure Handling<a href="#02-node-failure-handling" class="hash-link" aria-label="Direct link to 02 Node Failure Handling" title="Direct link to 02 Node Failure Handling">â€‹</a></h2><p>For FE node failures, if the cause of the failure cannot be quickly located, it is generally necessary to keep thread snapshots and memory snapshots and restart the process.</p><div class="language-undefined codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-undefined codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">jstack è¿›ç¨‹ID &gt;&gt; å¿«ç…§æ–‡ä»¶å.jstack</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>Save a memory snapshot of FE with the command:</p><div class="language-undefined codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-undefined codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">jmap -dump:live,format=b,file=å¿«ç…§æ–‡ä»¶å.heap è¿›ç¨‹ID</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>In the case of version upgrade or some unexpected scenarios, the image of the FE node may have abnormal metadata, and the abnormal metadata may be synchronized to other FE, resulting in all FE not working.Once a failed image is discovered, the fastest recovery option is to use Recovery mode to stop FE elections and replace the failed image with the backup image.Of course, it is not easy to backup images all the time.Since this failure is common in cluster upgrades, we recommend adding simple local image backup logic to the cluster upgrade procedure.Ensure that a copy of the current and latest image data will be retained before each upgrade starts the FE process.For BE node failure, if the process crashes, a core file will be generated, and minos will automatically pull the process;If the task is stuck, you need to restart the process after retaining the thread snapshot with the following command:</p><div class="language-undefined codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-undefined codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">pstack è¿›ç¨‹ID &gt;&gt; å¿«ç…§æ–‡ä»¶å.pstack</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><h1>Concluding Remarks</h1><p>Apache Doris has been widely used by Xiaomi since the first use of open source software Apache Doris by Xiaomi Group in September 2019.At present, it has served dozens of businesses of Xiaomi, with dozens of clusters and hundreds of nodes, and a set of data ecology with Apache Doris as the core has been formed within Xiaomi.In order to improve the efficiency of operation and maintenance, Xiaomi has also developed a complete set of automated management and operation and maintenance systems around Doris.With the increasing number of services, Doris also exposed some problems. For example, there was no better resource isolation mechanism in the past version, and services would affect each other. In addition, system monitoring needs to be further improved.With the rapid development of the community, more and more small partners have participated in the community construction, the vectorized engine has been transformed, the transformation of the query optimizer is in full swing, and Apache Doris is gradually maturing.</p></div></article><nav class="pagination-nav" aria-label="Blog list page navigation"></nav></main></div></div></div></div><div class="footer"><div class="container"><div class="footer-box"><div class="left"><img src="/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">Resource</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/download">Download</a></li><li class="footer__item"><a class="footer__link-item" href="/learning">Docs</a></li></ul></div><div class="col footer__col"><div class="footer__title">ASF</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">Foundation<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="footer__link-item">License<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">Events<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Sponsorship<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Privacy<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">Thanks<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">Language</div><ul class="footer__items clean-list"><li class="footer__item"><a href="/blog/tags/best-practice" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">English</a></li><li class="footer__item"><a href="/zh-CN/blog/tags/best-practice" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">ç®€ä½“ä¸­æ–‡</a></li></ul></div></div></div><div class="right"><div class="footer__title">Follow</div><div class="social-list"><div class="social"><a href="mailto:dev@doris.apache.org" title="mail" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5.6003 6H26.3997C27.8186 6 28.982 7.10964 29 8.46946L16.0045 15.454L3.01202 8.47829C3.02405 7.11258 4.1784 6 5.6003 6ZM3.01202 11.1508L3 23.5011C3 24.8756 4.16938 26 5.6003 26H26.3997C27.8306 26 29 24.8756 29 23.5011V11.145L16.3111 17.8028C16.1157 17.9058 15.8813 17.9058 15.6889 17.8028L3.01202 11.1508V11.1508Z" fill="currentColor"></path></svg></a><a href="https://github.com/apache/doris" title="github" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M16.0001 2.66675C8.63342 2.66675 2.66675 8.63341 2.66675 16.0001C2.66524 18.7991 3.54517 21.5276 5.1817 23.7983C6.81824 26.0691 9.12828 27.7668 11.7841 28.6508C12.4508 28.7668 12.7001 28.3668 12.7001 28.0161C12.7001 27.7001 12.6828 26.6508 12.6828 25.5334C9.33342 26.1508 8.46675 24.7174 8.20008 23.9668C8.04942 23.5828 7.40008 22.4001 6.83342 22.0828C6.36675 21.8334 5.70008 21.2161 6.81608 21.2001C7.86675 21.1828 8.61608 22.1668 8.86675 22.5668C10.0668 24.5828 11.9841 24.0161 12.7494 23.6668C12.8668 22.8001 13.2161 22.2174 13.6001 21.8841C10.6334 21.5508 7.53342 20.4001 7.53342 15.3001C7.53342 13.8494 8.04942 12.6507 8.90008 11.7161C8.76675 11.3827 8.30008 10.0161 9.03342 8.18275C9.03342 8.18275 10.1494 7.83342 12.7001 9.55075C13.7855 9.2495 14.907 9.09787 16.0334 9.10008C17.1668 9.10008 18.3001 9.24942 19.3668 9.54942C21.9161 7.81608 23.0334 8.18408 23.0334 8.18408C23.7668 10.0174 23.3001 11.3841 23.1668 11.7174C24.0161 12.6507 24.5334 13.8334 24.5334 15.3001C24.5334 20.4174 21.4174 21.5508 18.4508 21.8841C18.9334 22.3001 19.3508 23.1001 19.3508 24.3508C19.3508 26.1334 19.3334 27.5668 19.3334 28.0174C19.3334 28.3668 19.5841 28.7828 20.2508 28.6494C22.8975 27.7558 25.1973 26.0547 26.8266 23.7856C28.4559 21.5165 29.3327 18.7936 29.3334 16.0001C29.3334 8.63341 23.3668 2.66675 16.0001 2.66675V2.66675Z" fill="currentColor"></path></svg></a><a href="https://twitter.com/doris_apache" title="twitter" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M27.1493 10.8313C27.1493 10.5687 27.1442 10.3091 27.1326 10.0512C28.2554 9.21471 29.2295 8.1676 30 6.96938C28.9537 7.44012 27.8403 7.74469 26.7 7.8721C27.8868 7.14188 28.7969 5.97403 29.227 4.57256C28.1158 5.24638 26.8867 5.72811 25.5798 5.97912C24.5326 4.78777 23.0383 4.02895 21.3864 4.00076C18.2137 3.94831 15.6418 6.62904 15.6418 9.98754C15.6418 10.4649 15.6918 10.9279 15.7909 11.3749C11.0133 11.0675 6.77972 8.58103 3.9478 4.8326C3.45367 5.73067 3.17017 6.78025 3.17017 7.90503C3.17017 10.0324 4.18438 11.9232 5.72536 13.039C4.78198 12.9963 3.89827 12.7106 3.12316 12.2422V12.3204C3.12316 15.2937 5.10395 17.7849 7.73223 18.3661C7.2504 18.5032 6.74218 18.5745 6.2195 18.5719C5.85634 18.57 5.49437 18.5304 5.13941 18.4536C5.86973 20.8902 7.99227 22.6703 10.5051 22.7297C8.53846 24.3601 6.06106 25.334 3.37133 25.3272C2.90757 25.3272 2.44929 25.2961 2 25.2397C4.54329 26.9841 7.56224 28 10.8076 28C21.3719 28.0025 27.1493 18.8084 27.1493 10.8313V10.8313Z" fill="currentColor"></path></svg></a></div><div class="social"><a href="https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-1x7x8fger-F7NoshFQn~djlvGdnEtxUQ" title="slack" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_125_278)"><path d="M12.5875 16.6906C11.0844 16.6906 9.86562 17.9094 9.86562 19.4125V26.2375C9.86562 26.9594 10.1524 27.6517 10.6628 28.1622C11.1733 28.6726 11.8656 28.9594 12.5875 28.9594C13.3094 28.9594 14.0017 28.6726 14.5122 28.1622C15.0226 27.6517 15.3094 26.9594 15.3094 26.2375V19.4531C15.3094 17.9094 14.0906 16.6906 12.5875 16.6906ZM3 19.4531C3 20.175 3.28677 20.8673 3.79722 21.3778C4.30767 21.8882 4.99999 22.175 5.72187 22.175C6.44376 22.175 7.13608 21.8882 7.64653 21.3778C8.15698 20.8673 8.44375 20.175 8.44375 19.4531V16.7312H5.7625C4.25938 16.6906 3 17.9094 3 19.4531ZM12.5875 3C11.8656 3 11.1733 3.28677 10.6628 3.79722C10.1524 4.30767 9.86562 4.99999 9.86562 5.72187C9.86562 6.44376 10.1524 7.13608 10.6628 7.64653C11.1733 8.15698 11.8656 8.44375 12.5875 8.44375H15.3094V5.72187C15.3094 4.21875 14.0906 3 12.5875 3ZM5.72187 15.3094H12.5469C13.2688 15.3094 13.9611 15.0226 14.4715 14.5122C14.982 14.0017 15.2688 13.3094 15.2688 12.5875C15.2688 11.8656 14.982 11.1733 14.4715 10.6628C13.9611 10.1524 13.2688 9.86562 12.5469 9.86562H5.72187C4.99999 9.86562 4.30767 10.1524 3.79722 10.6628C3.28677 11.1733 3 11.8656 3 12.5875C3 13.3094 3.28677 14.0017 3.79722 14.5122C4.30767 15.0226 4.99999 15.3094 5.72187 15.3094ZM26.2375 9.86562C24.7344 9.86562 23.5156 11.0844 23.5156 12.5875V15.3094H26.2375C26.9594 15.3094 27.6517 15.0226 28.1622 14.5122C28.6726 14.0017 28.9594 13.3094 28.9594 12.5875C28.9594 11.8656 28.6726 11.1733 28.1622 10.6628C27.6517 10.1524 26.9594 9.86562 26.2375 9.86562ZM16.6906 5.72187V12.5875C16.6906 13.3094 16.9774 14.0017 17.4878 14.5122C17.9983 15.0226 18.6906 15.3094 19.4125 15.3094C20.1344 15.3094 20.8267 15.0226 21.3372 14.5122C21.8476 14.0017 22.1344 13.3094 22.1344 12.5875V5.72187C22.1344 4.99999 21.8476 4.30767 21.3372 3.79722C20.8267 3.28677 20.1344 3 19.4125 3C18.6906 3 17.9983 3.28677 17.4878 3.79722C16.9774 4.30767 16.6906 4.99999 16.6906 5.72187ZM22.1344 26.2781C22.1344 24.775 20.9156 23.5562 19.4125 23.5562H16.6906V26.2781C16.6906 27 16.9774 27.6923 17.4878 28.2028C17.9983 28.7132 18.6906 29 19.4125 29C20.1344 29 20.8267 28.7132 21.3372 28.2028C21.8476 27.6923 22.1344 27 22.1344 26.2781ZM26.2781 16.6906H19.4125C18.6906 16.6906 17.9983 16.9774 17.4878 17.4878C16.9774 17.9983 16.6906 18.6906 16.6906 19.4125C16.6906 20.1344 16.9774 20.8267 17.4878 21.3372C17.9983 21.8476 18.6906 22.1344 19.4125 22.1344H26.2375C27.7406 22.1344 28.9594 20.9156 28.9594 19.4125C29 17.9094 27.7812 16.6906 26.2781 16.6906Z" fill="currentColor"></path></g><defs><clipPath id="clip0_125_278"><rect width="26" height="26" fill="currentColor" transform="translate(3 3)"></rect></clipPath></defs></svg></a><a href="https://space.bilibili.com/362350065" title="bilibili" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M23.0533 11.5412H9.14591C8.72353 11.5412 8.36784 11.8633 8.36784 12.296V21.5162C8.36784 21.9489 8.72349 22.2665 9.14591 22.2665H23.0533C23.4757 22.2665 23.7928 21.949 23.7928 21.5162V12.296C23.7928 11.8633 23.4757 11.5412 23.0533 11.5412ZM10.0928 14.9479L14.0122 14.1975L14.3083 15.6685L10.4284 16.4188L10.0928 14.9479ZM16.1348 19.4301C14.9303 20.7431 13.6667 19.0155 13.6667 19.0155L14.3084 18.6008C14.3084 18.6008 15.1673 20.1508 16.125 18.0973C17.0432 20.0916 18.06 18.6206 18.06 18.6304L18.6426 19.0057C18.6426 19.0057 17.5565 20.7431 16.1348 19.4301ZM21.9498 16.4189L18.06 15.6686L18.3661 14.1976L22.2756 14.948L21.9498 16.4189Z" fill="currentColor"></path><path d="M16 2C8.26801 2 2 8.26805 2 16C2 23.7319 8.26806 30 16 30C23.7319 30 30 23.7319 30 16C30 8.26801 23.732 2 16 2ZM23.3727 24.1329C22.3941 24.1019 22.0644 24.1329 22.0644 24.1329C22.0644 24.1329 21.9923 25.2558 21.0343 25.2764C20.0659 25.2867 19.9216 24.4934 19.8907 24.1947C19.3035 24.1947 12.2467 24.2255 12.2467 24.2255C12.2467 24.2255 12.1231 25.266 11.165 25.266C10.1967 25.266 10.1451 24.4006 10.0833 24.2255C9.45486 24.2255 8.6101 24.2049 8.6101 24.2049C8.6101 24.2049 6.48791 23.7621 6.20978 21.0012C6.24067 18.2402 6.20978 12.7801 6.20978 12.7801C6.20978 12.7801 6.01404 10.2356 8.54836 9.50415C9.33118 9.4733 11.0208 9.46293 12.9781 9.46293L11.1753 7.71159C11.1753 7.71159 10.8971 7.36131 11.371 6.96986C11.8553 6.57846 11.8757 6.73797 12.0406 6.85128C12.2055 6.96456 14.7295 9.45229 14.7295 9.45229H14.3895C15.3579 9.45229 16.3572 9.46799 17.3152 9.46799C17.686 9.09711 19.798 7.02903 19.9422 6.92612C20.107 6.82309 20.1378 6.64927 20.6118 7.04068C21.0857 7.43213 20.8075 7.78309 20.8075 7.78309L19.0459 9.48322C21.4668 9.50386 23.3315 9.51423 23.3315 9.51423C23.3315 9.51423 25.7214 10.0398 25.7833 12.78C25.7524 15.5203 25.7936 21.0319 25.7936 21.0319C25.7936 21.0319 25.6598 23.7104 23.3727 24.1329Z" fill="currentColor"></path></svg></a><a class="item wechat"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M20.7578 11.5169C21.0708 11.5169 21.3795 11.5398 21.6851 11.573C20.8524 7.73517 16.7052 4.88306 11.9718 4.88306C6.67951 4.88306 2.34412 8.45283 2.34412 12.9854C2.34412 15.6013 3.78679 17.7498 6.19667 19.4161L5.2339 22.2827L8.59917 20.6122C9.80411 20.8478 10.7698 21.0906 11.9718 21.0906C12.2738 21.0906 12.5728 21.0759 12.8703 21.0523C12.682 20.4159 12.5728 19.7485 12.5728 19.0566C12.5728 14.8947 16.1847 11.5169 20.7578 11.5169ZM15.5822 8.9335C16.3072 8.9335 16.7871 9.40601 16.7871 10.1229C16.7871 10.8369 16.3072 11.3153 15.5822 11.3153C14.8601 11.3153 14.1365 10.8369 14.1365 10.1229C14.1365 9.40601 14.8601 8.9335 15.5822 8.9335ZM8.84429 11.3153C8.12218 11.3153 7.3942 10.8368 7.3942 10.1229C7.3942 9.40597 8.12218 8.93346 8.84429 8.93346C9.56559 8.93346 10.0463 9.40597 10.0463 10.1229C10.0463 10.8369 9.56559 11.3153 8.84429 11.3153ZM29.5453 18.9422C29.5453 15.1332 25.6935 12.0285 21.3677 12.0285C16.7871 12.0285 13.1797 15.1332 13.1797 18.9422C13.1797 22.7567 16.7871 25.8547 21.3677 25.8547C22.326 25.8547 23.2932 25.6169 24.2559 25.3777L26.897 26.8086L26.1726 24.4282C28.1056 22.993 29.5453 21.0906 29.5453 18.9422ZM18.7126 17.7498C18.2335 17.7498 17.7499 17.278 17.7499 16.7966C17.7499 16.3219 18.2335 15.8442 18.7126 15.8442C19.4406 15.8442 19.9176 16.3219 19.9176 16.7966C19.9176 17.278 19.4406 17.7498 18.7126 17.7498ZM24.0079 17.7498C23.5324 17.7498 23.0518 17.278 23.0518 16.7966C23.0518 16.3219 23.5324 15.8442 24.0079 15.8442C24.73 15.8442 25.2128 16.3219 25.2128 16.7966C25.2128 17.278 24.73 17.7498 24.0079 17.7498Z" fill="currentColor"></path></svg><div class="wechat-dropdown"><img src="https://cdnd.selectdb.com/assets/images/wechat-31a2eb3bbefef7171acfae2906dc777c.png" alt=""></div></a></div></div></div></div><div class="footer__copyright">Copyright Â© 2023 The Apache Software Foundation,Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License, Version 2.0</a>. Apache, Doris, Apache Doris, the Apache feather logo and the Apache Doris logo are trademarks of The Apache Software Foundation.</div></div></div></div>
<script src="https://cdnd.selectdb.com/assets/js/runtime~main.772191fd.js"></script>
<script src="https://cdnd.selectdb.com/assets/js/main.1e662d49.js"></script>
</body>
</html>