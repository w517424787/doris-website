<!doctype html>
<html lang="zh-CN" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no">
<meta name="generator" content="Docusaurus v2.0.0-beta.21">
<link rel="alternate" type="application/rss+xml" href="/zh-CN/blog/rss.xml" title="Apache Doris RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-CN/blog/atom.xml" title="Apache Doris Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DT7W9E9722"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DT7W9E9722",{anonymize_ip:!0})</script>






<link rel="icon" href="/zh-CN/images/logo-only.png">
<link rel="manifest" href="/zh-CN/manifest.json">
<meta name="theme-color" content="#FFFFFF">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/zh-CN/img/docusaurus.png">
<link rel="mask-icon" href="/zh-CN/img/docusaurus.svg" color="rgb(37, 194, 160)">
<meta name="msapplication-TileImage" content="/zh-CN/img/docusaurus.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:500">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400"><title data-rh="true">10 篇博文 含有标签「最佳实践」 - Apache Doris</title><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doris.apache.org/zh-CN/blog/tags/最佳实践"><meta data-rh="true" name="docusaurus_locale" content="zh-CN"><meta data-rh="true" name="docsearch:language" content="zh-CN"><meta data-rh="true" property="og:title" content="10 篇博文 含有标签「最佳实践」 - Apache Doris"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/zh-CN/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://doris.apache.org/zh-CN/blog/tags/最佳实践"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/最佳实践" hreflang="en"><link data-rh="true" rel="alternate" href="https://doris.apache.org/zh-CN/blog/tags/最佳实践" hreflang="zh-CN"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/最佳实践" hreflang="x-default"><link rel="stylesheet" href="https://cdn-tencent.selectdb.com/zh-CN/assets/css/styles.ee243de9.css">
<link rel="preload" href="https://cdn-tencent.selectdb.com/zh-CN/assets/js/runtime~main.276d6eed.js" as="script">
<link rel="preload" href="https://cdn-tencent.selectdb.com/zh-CN/assets/js/main.826cf4ca.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_fXgn">跳到主要内容</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-CN/"><div class="navbar__logo"><img src="https://cdn-tencent.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--light_HNdA"><img src="https://cdn-tencent.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/zh-CN/">首页</a><a class="navbar__item navbar__link" href="/zh-CN/docs/dev/summary/basic-summary">文档</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-CN/blog">博客</a><a class="navbar__item navbar__link" href="/zh-CN/community/team">社区</a><a class="navbar__item navbar__link" href="/zh-CN/users">用户</a><a href="https://doris-summit.selectdb.com" target="_blank" rel="noopener noreferrer" class="navbar__item navbar__link">Doris Summit 2022</a></div><div class="navbar__items navbar__items--right"><div class="locale-box"><a href="/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link">EN</a><a href="/zh-CN/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link dropdown__link--active">中文</a></div><a class="navbar__item navbar__link header-right-button-primary navbar-download-mobile" href="/zh-CN/download">下载</a><div class="navbar-search searchBox_ZlJk"><div class="navbar__search searchBarContainer_PzyC"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing__K5d searchBarLoadingRing_e2f0"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_m7ml"><kbd class="searchHint_zuPL">ctrl</kbd><kbd class="searchHint_zuPL">K</kbd></div></div></div><span class="github-btn desktop header-right-button-github"><span class="github-btn github-btn-large"><a class="gh-btn" href="//github.com/apache/doris/" target="_blank"><span class="gh-ico" aria-hidden="true"></span><span class="gh-text">Star</span></a><a class="gh-count" target="_blank" href="//github.com/apache/doris/stargazers/"></a></span></span><a class="header-right-button-primary navbar-download-desktop" href="/zh-CN/download">下载</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper"><div class="container margin-vert--lg blog-container"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>10 篇博文 含有标签「最佳实践」</h1><a href="/zh-CN/blog/tags">查看所有标签</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/LY">应用实践：数仓体系效率全面提升！同程数科基于 Apache Doris 的数据仓库建设</a></h2><div class="blog-info"><time datetime="2022-12-19T00:00:00.000Z" itemprop="datePublished">2022年12月19日</time><span class="split-line"></span><span class="authors"><span class="s-author">王星</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>导读：同程数科成立于 2015 年，是同程集团旗下的旅游产业金融服务平台。2020 年，同程数科由于看到了 Apache Doris 丰富的数据接入方式、优异的并行运算能力和极简运维的特性，引入了Apache Doris 进行数仓架构改造。本文详细讲述了同程数科数仓架构从1.0 到 2.0 的演进过程及使用Doris过程中的应用实践。希望对大家有所帮助。</p></blockquote><blockquote><p>作者｜同程数科大数据高级工程师 王星</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-fb77e142257a98bea6656a33a626b310.png" width="900" height="383" class="img_ev3q"></p><h1>业务背景</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务介绍">业务介绍<a class="hash-link" href="#业务介绍" title="标题的直接链接">​</a></h2><p>同程数科是同程集团旗下的旅游金融服务平台，其前身是同程金服。正式成立于 2015 年，同程数科以“数字科技引领旅游产业”为愿景，坚持以科技的力量，赋能我国旅游产业。
目前，同程数科的业务涵盖金融服务、消费金融服务、金融科技及数字科技等板块，累计服务覆盖超过千万用户和 76 座城市。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务需求">业务需求<a class="hash-link" href="#业务需求" title="标题的直接链接">​</a></h2><p>包含四大类：</p><ul><li>看板类：包括实时驾驶舱以及 T+1 业务看板等。</li><li>预警类：包括风控熔断、资金异常以及流量监控等。</li><li>分析类：包括及时性数据查询分析以及临时取数等。</li><li>财务类：包括清算以及支付对账需求。</li></ul><h1>架构演进之 1.0</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="工作流程">工作流程<a class="hash-link" href="#工作流程" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="1" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/1-bb0a3cc8027796049893fcf4d60dd5ab.png" width="1080" height="608" class="img_ev3q"></p><p>我们最初的数仓架构沿袭了前几年非常流行的SteamSets 和 Apache Kudu 组合的第一代架构。该架构中，Binlog 通过StreamSets后，通过实时采集后写入 Apache Kudu 中，最后通过 Apache Impala 和可视化工具进行查询和使用。</p><p>不足：</p><ul><li>组件引入过多，维护成本随之增加</li><li>多种技术架构和过长的开发链路，提高了数仓研发人员的学习成本，数仓人员需要在不同组件之间进行开发，导致开发效率降低。</li><li>Apache Kudu 在大表关联 Join 方面性能差强人意。</li><li>由于数仓使用了 CDH组件搭建，离线和实时集群并未进行分离，形成资源之间的相互竞争；在离线数据批量处理时对 IO 或磁盘消耗较大，无法保证实时数据的及时性。</li><li>虽然 SteamSets 配备了预警能力，但作业恢复能力仍相对欠缺。在配置多个任务时， JVM 的消耗较大，导致恢复速度较慢。</li></ul><h1>架构演进之 2.0</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="调研过程">调研过程<a class="hash-link" href="#调研过程" title="标题的直接链接">​</a></h2><p>由于缺点众多，我们不得不放弃了数仓1.0的架构。在 2020年中，我们对市面上流行的数仓进行了深度调研。</p><p>在调研过程中，我们集中对比了Click house和Apache Doris。ClickHouse 对 CPU 的利用率较高，所以在单表查询时表现比较优秀，但是在多查询高 QPS 的情况下则表现欠佳。反观Doris不仅单节点最高可支持上千QPS，而且得益于分区分桶裁剪的功能，可以支持QPS万级别的高并发查询；再者，ClickHouse的扩容缩容复杂且繁琐，目前做不到自动在线操作，需要自研工具支持。Doris支持集群的在线动态扩缩容，且可以随着业务的发展水平扩展。</p><p>在调研中，ApacheDoris脱颖而出。Doris高并发的查询能力非常吸引我们，而且灵活的扩缩容能力也也更适合我们灵活多变的广告业务。因此我们选择了 Apache Doris。</p><p><img loading="lazy" alt="2" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/2-11b7311ef2c06a545dbdb54e01787f6d.png" width="1080" height="608" class="img_ev3q"></p><p>引入 Apache Doris 后，我们对整个数仓进行了改造：</p><ul><li>通过Canal 采集MySQL Binlog 进入 Kafka中。因为Apache Doris 与 Kafka 的契合度较高，可以便捷地使用 Routine Load 对数据加载和导入。</li><li>我们对原有离线计算的数据链路进行了细微调整。对于存储在 Hive 中的数据，Apahce Doris 可以通过 Broker Load 将 Hive 中的数据导入。这样一来离线集群的数据就可以直接加载到 Doris。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="选择-doris">选择 Doris<a class="hash-link" href="#选择-doris" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="3" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/3-1a638414ccc0a8decbd99b24160973a8.png" width="1080" height="608" class="img_ev3q"></p><p>Apache Doris 整体表现令人深刻：</p><ul><li>数据接入：提供了丰富的数据导入方式，能够支持众多数据源的接入。</li><li>数据连接：Doris 支持 JDBC 与 ODBC 等方式连接。Doris对 BI 工具的可视化展示比较友好，能够便捷地与 BI 工具进行连接。另外Doris 采用MySQL 协议进行通信，用户可以通过各类 Client 工具直接访问 Doris。</li><li>SQL 语法：Doris 采用MySQL 协议，高度兼容MySQL 语法，支持标准SQL，对于数仓开发人员来说学习成本较低；</li><li>MPP 并行计算：Doris 基于 MPP 架构，提供了非常优秀的并行计算能力。在复杂Join和大表Join的场景下Doris优势非常明显；</li><li>文档健全：Doris 官方文档非常健全，对于新用户上手非常友好。（我们最看重的一点）</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris-实时系统架构">Doris 实时系统架构<a class="hash-link" href="#doris-实时系统架构" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="4" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/4-eeefe38e4d58d7a253a72ca3c98ecace.png" width="1080" height="608" class="img_ev3q"></p><ul><li><p>数据源：在实时系统架构中，数据源来自产业金融、消费金融、风控数据等业务线，通过 Canal 和 API 接口进行采集。</p></li><li><p>数据采集：通过 Canal- Admin 进行数据采集后，Canal将数据发送到 Kafka 消息队列之中。之后，数据再通过 Routine Load 接入到 Doris 集群。</p></li><li><p>Doris 数仓：由Doris 集群组成了了数据仓库的三级分层，分别是：使用了 Unique 模型的 DWD 明细层 、 Aggregate 模型的 DWS 汇总层以及 ADS 应用层。</p></li><li><p>数据应用：数据应用于实时看板、数据及时性分析以及数据服务三方面。</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris-新数仓特点">Doris 新数仓特点<a class="hash-link" href="#doris-新数仓特点" title="标题的直接链接">​</a></h2><p>数据导入方式简便，根据不同场景采用 3 种不同的导入方式：</p><ul><li>Routine Load：当我们提交 Rountine Load 任务时，Doris 内部会有一个常驻进程实时消费 Kafka ，不断从 Kafka 中读取数据并导入进 Doris中。</li><li>Broker Load：维度表及历史数据等离线数据有序地导入Doris。</li><li>Insert Into：用于定时批式计算任务，负责处理DWD 层数据，从而形成 DWS 层以及 ADS 层。
Doris 的良好数据模型，提升了我们的开发效率：</li><li>Unique 模型在 DWD 层接入时使用，可以有效防止重复消费数据。</li><li>Aggregate 模型用作聚合。在 Doris 中，Aggregate 支持如 Sum、Replace、Min 、Max 4 种方式的聚合模型，聚合的过程中使用 Aggregate 底层模型可以减少很大部分 SQL 代码量，不再人工手写Sum、Min、Max 等代码。
Doris 查询效率高：</li><li>支持物化视图与 Rollup 物化索引。物化视图底层类似 Cube 的概念与预计算的过程，与 Kylin 中以空间换时间的方式类似，均是在底层生成特殊的表，在查询中命中物化视图并快速响应。</li></ul><h1>新架构的收益</h1><ul><li>数据接入：在最初的架构中，通过 SteamSets 进行数据接入的过程中需要手动建立 Kudu 表。由于缺乏工具，整个建表和创建任务的过程需要 20-30 分钟。如今可以通过平台与快速构建语句实现数据快速接入，每张表的接入过程从之前的20-30分钟缩短到现在的 3-5 分钟，性能提升了 5-6 倍。</li><li>数据开发：使用 Doris之后，我们可以直接使用 Doris 中自带的 Unique、Aggregate 等数据模型及可以很好支持日志类场景的 Duplicate 模型，在 ETL 过程中大幅度加快开发过程。</li><li>查询分析：Doris 底层带有物化视图及 Rollup 物化索引等功能。物化视图底层类似 Cube 的概念与预计算的过程，与 Kylin 中以空间换时间的方式类似，均是在底层生成特殊的表，在查询中命中物化视图并快速响应。同时 Doris 底层对于大表关联进行了诸多优化，如 Runtime Filter 以及其他 Join 和自定义优化。相较于 Doris，Apache Kudu 则需要经过更为深入和复杂的优化才能更好地使用。</li><li>数据报表：我们最初使用 Kudu 报表查询需要 1-2 分钟才能够完成渲染，而 Doris 则是秒级甚至是毫秒级的响应速度。</li><li>便捷运维：Doris 没有 Hadoop 生态系统的复杂度，维护成本远低于 Hadoop。尤其是在集群迁移过程中，Doris 的运维便捷性尤为突出。3 月份，我们的机房进行了搬迁，12 台 Doris 节点机器在三天内全部迁移完成。整体操作较为简单，除了机器上架下架和搬移外，FE 扩容与缩容时只运用了 Add 与 Drop 等简单指令，并未消耗太长时间。</li></ul><h1>未来展望</h1><ul><li>实现基于 Flink CDC 的数据接入：当前，优化后的数据库架构中并没有没有引入 Flink CDC ，而是继续沿用了 数据经Canal 采集到 Kafka 后再采集到 Doris 中的模式，链路相对来说较长。使用Flink CDC 虽然可以继续精简整体架构，但是还需要写一定量的代码，这对于数据分析师直接使用感受并不友好。我们希望数据分析师只需要写简单SQL 或在页面上直接操作。在未来的规划中，我们计划引入 Flink CDC 功能并对上层应用进行扩充。</li><li>紧跟社区迭代计划：我们正在使用的 Doris 版本相对较老，现在的最新版本 Apache Doris V1.2.0在全面向量化、multi-catalog多元数据目录、light schema change轻量表结构变更方面有了较大幅度的提升。我们将紧跟社区迭代节奏对集群进行升级并充分利用新特性。</li><li>强化建设相关体系：我们现在的指标体系管理如报表元数据、业务元数据等维护与管理水平依旧有待提高。在数据质量监控方面，虽然目前包含了数据质量监控功能，但对于整个平台监控与数据自动化监控方面还需要强化与改善。</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/BestPractice_Kwai">Doris on Es在快手商业化的最佳实践</a></h2><div class="blog-info"><time datetime="2022-12-14T00:00:00.000Z" itemprop="datePublished">2022年12月14日</time><span class="split-line"></span><span class="authors"><span class="s-author">贺祥</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>作者：贺祥，数据架构高级工程师，快手商业化团队</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-846e4e39fd88e1e34d2474b23690d9b2.png" width="900" height="383" class="img_ev3q"></p><h1>1 关于快手</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-快手">1.1 快手<a class="hash-link" href="#11-快手" title="标题的直接链接">​</a></h2><p>快手（HKG: 1024）是一个短视频和潮流社交网络。发现有趣的短片，通过生活中的录音、视频、玩日常挑战或喜欢最好的动效模版和视频来为虚拟社区做出贡献。用短视频分享生活，并从数十种神奇的效果和滤镜中选择喜欢的方式。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="12-快手商业化报表引擎">1.2 快手商业化报表引擎<a class="hash-link" href="#12-快手商业化报表引擎" title="标题的直接链接">​</a></h2><p>快手商业化报表引擎为外部广告主提供广告投放效果的实时多维分析报表在线查询服务，以及为内部各商业化系统提供多维分析报表查询服务，致力于解决多维分析报表场景的高性能、高并发、高稳定的查询问题。</p><h1>2 初期架构</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-需求背景">2.1 需求背景<a class="hash-link" href="#21-需求背景" title="标题的直接链接">​</a></h2><p>传统 OLAP 引擎应对多维分析时更多是以预建模的方式，通过构建数据立方体（Cube）对事实数据进行下钻、上卷、切片、切块等操作。现代 OLAP 分析引入了关系模型的理念，在二维关系表中描绘数据。而在建模过程中，往往有两种建模方式，一是采用宽表模型、将多张表的数据通过 Join 写入进一张宽表中，另一种方式是采用星型模型、将数据表区分为事实表和维度表、查询时对事实表与维度表进行 Join 。
以上两种方案各有部分优缺点：</p><p>宽表模型：</p><p>采取空间换时间的思路，理论上都是维表主键为唯一 ID 来填充所有维度，冗余存储了多条维度数据。其优势在于查询时非常方便，无需关联额外维表，性能表现更佳。其弊端在于如果有维度数据变化，需要对全表数据进行重刷，无法支撑高频的 Update。</p><p>星型模型：</p><p>维度数据与事实数据完全分离，维度数据往往用专门的引擎存储 (如 MySQL、Elasticsearch 等)，查询时通过主键关联查询维度数据，其优势在于维度数据变化不影响事实数据、可支持高频 Update 操作。其弊端在于查询逻辑相对更复杂，且多表 Join 可能导致性能受损。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-业务需求">2.2 业务需求<a class="hash-link" href="#22-业务需求" title="标题的直接链接">​</a></h2><p>在快手的业务场景中，商业化报表引擎承载了外部广告主实时查询广告投放效果的需求，在构建报表引擎时，我们期望可以满足如下要求：</p><ul><li>超大数据量：单表原始数据每天增量百亿</li><li>查询高 QPS：平均 QPS千级别</li><li>高稳定性要求：在线服务要求稳定性4个9
最为重要的是，由于维度数据经常发生变更，维度表需要支持高达上千 QPS 的 Update 操作，同时还要进一步支持模糊匹配、分词检索等需求。
基于以上需求，我们选择了星型模型来建模，并以 Apache Druid 和 Elasticsearch 为核心构建了早期的报表引擎架构。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-初期架构基于apache-druid的架构">2.3 初期架构：基于Apache Druid的架构<a class="hash-link" href="#23-初期架构基于apache-druid的架构" title="标题的直接链接">​</a></h2><p>我们选择了引擎结合的方式，用Elasticsearch适配Druid引擎来实现。在数据写入阶段，我们通过Flink对数据进行分钟级预聚合，利用Kafka对数据进行小时级别的数据预聚合。在数据查询中，App端发起查询需求，对RE Front统一接口进行查询，Re Query根据引擎适配，向维表引擎（Elasticsearch和MySQL）及扩展引擎分别发起查询。</p><p>Druid则是一款基于时序的查询引擎，支持数据实时摄入，用来存储和查询大量的事实数据。而选用Elasticsearch作为维度数据存储引擎，主要是因为如下原因：</p><ul><li>支持高频实时更新，可以支撑上千 QPS的 Update操作</li><li>支持分词模糊检索，适用于快手的业务</li><li>支持量级较高的维表数据，不用像MySQL数据库一样做分库分表才能满足</li><li>支持数据同步监控，同时拥有检查和恢复的服务</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-报表引擎">2.4 报表引擎<a class="hash-link" href="#24-报表引擎" title="标题的直接链接">​</a></h2><p>报表引擎架构整体分为REFront 和 REQuery两层，REMeta为独立的元数据管理模块。报表引擎在REQuery内部实现MEM Join。支持Druid引擎中的事实数据与ES引擎中的维度数据做关联查询。为上层业务提供虚拟的cube表查询。屏蔽复杂的跨引擎管理查询逻辑。</p><p><img loading="lazy" alt="1" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/1-ca39b5dcdd3d69d8fee822dc1b74a2e5.png" width="864" height="885" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-基于apache-doris的架构">3 基于Apache Doris的架构<a class="hash-link" href="#3-基于apache-doris的架构" title="标题的直接链接">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-架构遗留的问题">3.1 架构遗留的问题<a class="hash-link" href="#31-架构遗留的问题" title="标题的直接链接">​</a></h2><p>首先，我们在使用报表引擎时，发现了这样的一个问题。Mem Join是单机实现与串行执行，到单次从ES中拉取的数据量超过10W时，响应时间已经接近10s，用户体验差。而且单节点实现大规模数据Join处理，内存消耗大，有Full GC风险。</p><p>其次，Druid的Lookup Join了功能不够完善是一个较大的问题，不能完全满足真实业务需求。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-选型调研">3.2 选型调研<a class="hash-link" href="#32-选型调研" title="标题的直接链接">​</a></h2><p>于是我们对业界常见的 OLAP 数据库进行了调研，其中最具代表性的为 Apache Doris和 Clickhouse。在进一步的调研中我们发现，Apache Doris在大宽表Join的能力更强。ClickHouse能够支持 Broadcast 基于内存的Join，但是对于大数据量千万级以上大宽表的Join，ClickHouse 的性能表现不好。Doris 和 Clickhouse 都支持明细数据存储，但Clickhouse支持的并发度较低，相反Doris支持高并发低延时的查询服务，单机最高支持上千QPS。在并发增加时，线性扩充FE和BE即可支持。而Clickhouse的数据导入没有事务支持功能，无法实现exactly once语义，对标准sql的支持也是有限的。相比之下，Doris提供了数据导入的事务支持和原子性，Doris 自身能够保证不丢不重的订阅 Kafka 中的消息，即 Exactly-Once 消费语义。ClickHouse使用门槛高、运维成本高和分布式能力弱，需要较多的定制化和较深的技术实力也是另一个难题，Doris则不同，只有FE、BE两个核心组件，外部依赖也比较少，运维快捷简单。我们还发现，由于Doris 更加接近 MySQL协议，比起Clickhouse更加便捷，在迁移时的成本并不大。在横向扩容方面，Doris 的扩缩容也能够做到自平衡，大大优于Clickhouse。</p><p>由此看来Doris可以比较好的提升Join的性能，在迁移成本、横向扩容、并发程度等其他方面也比较优秀。不过在高频Update上，Elasticsearch具有先天的优势。</p><p>通过 Doris 创建 ES 外表的方式来同时应对高频Upate和Join性能问题，会是比较理想的解决方案。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-dorisdoris-on-es完美配合">3.3 Doris+Doris on ES完美配合<a class="hash-link" href="#33-dorisdoris-on-es完美配合" title="标题的直接链接">​</a></h2><p>Doris on ES 的查询性能究竟如何呢？</p><p>首先，Apache Doris 是一个基于MPP 架构的实时分析型数据库，性能强劲、横向扩展能力能力强。Doris on ES构建在这个能力之上，并且对查询做了大量的优化。其次，在这些之上，融合Elasticsearch的能力之后，我们还对查询功能做出了大量的优化：</p><ul><li>Shard级别并发</li><li>行列扫描自动适配，优先列式扫描</li><li>顺序读取，提前终止</li><li>两阶段查询变为一阶段查询</li><li>Join场景使用Broadcast Join，对于小批量数据Join特别友好</li></ul><p><img loading="lazy" alt="2" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/2-b0e578721df866bb977d80072c559f32.png" width="864" height="800" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-基于doris-on-elasticsearch的架构实现">3.4 基于Doris on Elasticsearch的架构实现<a class="hash-link" href="#34-基于doris-on-elasticsearch的架构实现" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="341-数据链路升级">3.4.1 数据链路升级<a class="hash-link" href="#341-数据链路升级" title="标题的直接链接">​</a></h3><p>数据链路的升级适配比较简单。第一步，由Doris构建新的Olap表，配置好物化视图。第二步，基于之前事实数据的kafka topic启动routine load，导入实时数据。第三步，从Hive中通broker load导入离线数据。最后一步，通过Doris创建Es外表。</p><p><img loading="lazy" alt="3" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/3-1c0a381d13453a0e975d97ffab096981.png" width="864" height="629" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="342-报表引擎适配升级">3.4.2 报表引擎适配升级<a class="hash-link" href="#342-报表引擎适配升级" title="标题的直接链接">​</a></h3><p><img loading="lazy" alt="4" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/4-3fca61e78e95bd16fad48de37ee7124f.png" width="864" height="722" class="img_ev3q"></p><p>注：上图关联的mysql维表是基于未来规划，目前主要是ES做维表引擎</p><p>报表引擎适配</p><ul><li>抽象基于Doris的星型模型虚拟cube表</li><li>适配cube表查询解析，智能下推</li><li>支持灰度上线</li></ul><h1>4  线上表现</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-查询响应时间">4.1 查询响应时间<a class="hash-link" href="#41-查询响应时间" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="411-事实表查询表现对比">4.1.1 事实表查询表现对比<a class="hash-link" href="#411-事实表查询表现对比" title="标题的直接链接">​</a></h3><p>Druid</p><p><img loading="lazy" alt="5" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/5-4ce2b0396e8e14ac9e536befcf11cfd0.png" width="864" height="200" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="6" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/6-80fe6a32cf89065b0323afca7038f0ba.png" width="864" height="195" class="img_ev3q"></p><p>99分位耗时Druid大概为270ms，Doris为150ms，延时下降45%</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="412-join场景下cube表查询表现对比">4.1.2 Join场景下cube表查询表现对比<a class="hash-link" href="#412-join场景下cube表查询表现对比" title="标题的直接链接">​</a></h3><p>Druid</p><p><img loading="lazy" alt="7" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/7-30f72edbee75326a65e006652a846e63.png" width="864" height="197" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="8" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/8-b9485387d33cb96f36fdcaf09999ce2a.png" width="864" height="193" class="img_ev3q"></p><p>99分位耗时Druid大概为660ms，Doris为440ms，延时下降33%</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="413-收益总结">4.1.3 收益总结<a class="hash-link" href="#413-收益总结" title="标题的直接链接">​</a></h3><ul><li>P99整体耗时下降35%左右</li><li>资源节省50%左右</li><li>去除报表引擎内部Mem Join的复杂逻辑，下沉至Doris通过DOE实现，在大查询场景下(维表结果超过10W，性能提升超过10倍，10s-&gt;1s)</li><li>更丰富的查询语义(原本Mem Join实现比较简单，不支持复杂的查询)</li></ul><h1>5  总结与未来规划</h1><p>在快手商业化业务里面，维度数据与事实数据Join查询是非常普遍的。使用Doris 之后，查询变得简单。我们仅需要按天同步事实表和维表，在查询的同时 Join即可。通过Doris替代Druid、Clickhouse的方案，基本覆盖了我们使用Druid 时的所有场景，大大提高了海量数据的聚合分析能力。在Apache Doris的使用过程中，我们还发现了一些意想不到的收益：例如，Routine Load和 Broker Load的导入方式较为简单，提升了查询速度；数据占用空间大幅降低；Doris支持MySQL协议，方便了数据分析师自助取数绘图等。</p><p>尽管Doris on ES的解决方案比较成功的满足了我们的报表业务，ES外表映射仍然需要手工建表。但Apache Doris于近日完成了最新版本V1.2.0的发布，新版本功能新增了Multi-Catlog，提供了无缝接入Hive、ES、Hudi、Iceberg 等外部数据源的能力。用户可以通过 CREATE CATALOG 命令连接到外部数据源，Doris 会自动映射外部数据源的库、表信息。如此一来，以后我们就不需要再手动创建Es外表完成映射，大大节省了开发的时间成本，提升了研发效率。而全面向量化、Ligt Schema Change、Merge-on-Write、Java UDF等其他新功能的实现，也让我们对Apache Doris有了全新的期待。祝福Apache Doris！</p><h1>联系我们</h1><p>官网：<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Github：<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>dev邮件组：<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/xiaomi_vector">最佳实践: Apache Doris 在小米数据场景的应用实践与优化</a></h2><div class="blog-info"><time datetime="2022-12-08T00:00:00.000Z" itemprop="datePublished">2022年12月8日</time><span class="split-line"></span><span class="authors"><span class="s-author">魏祚</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>导读：小米集团于 2019 年首次引入了 Apache Doris ，目前 Apache Doris 已经在小米内部数十个业务中得到广泛应用，并且在小米内部已经形成一套以 Apache Doris 为核心的数据生态。本篇文章转录自 Doris 社区线上 Meetup 主题演讲，旨在分享 Apache Doris 在小米数据场景的落地实践与优化实践。</p></blockquote><blockquote><p>作者｜魏祚 小米 OLAP 引擎研发工程师 </p></blockquote><p><img loading="lazy" alt="kv" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-b27d71e34981d9850785329cea2cb610.png" width="900" height="383" class="img_ev3q"></p><h1>关于小米</h1><p>小米公司（“小米”或“集团”；HKG：1810），一家消费电子和智能制造公司，其智能手机和智能硬件通过物联网 (IoT) 平台连接。 2021年，小米总收入达到人民币3283亿元（4722.3131.62亿美元），同比增长33.5%；调整后净利润为人民币 220 亿元（316,451.08 万美元），同比增长 69.5%。</p><p>因分析业务的增长，小米集团于 2019 年首次引入了 Apache Doris 。经过三年时间的发展，目前 Apache Doris 已经在广告投放、新零售、增长分析、数据看板、用户画像、天星数科、小米有品、等小米内部数十个业务和品牌中得到广泛应用，并且在小米内部已经围绕 Apache Doris 为核心建设了数据生态。</p><p><img loading="lazy" alt="1" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/1-1ba7f77a03c987c9397cedee505fe819.png" width="1080" height="600" class="img_ev3q"></p><p>当前 Apache Doris 在小米内部已经具有数十个集群、总体达到数百台 BE 节点的规模，其中单集群最大规模达到近百台节点，拥有数十个实时数据同步任务，每日单表最大增量 120 亿、支持 PB 级别存储，单集群每天可以支持 2W 次以上的多维分析查询。</p><h1>架构演进</h1><p>小米引入 Apache Doris 的初衷是为了解决内部的用户行为分析中所遇到的问题。随着小米互联网业务的发展，利用用户行为数据进行增长分析的需求越来越强烈。如果每个业务产品线都自己搭建一套增长分析系统，不仅成本高昂，效率也不高。因此如果能有一款产品能够帮助他们不用关心底层的复杂技术细节，让相关业务人员能够专注于自己的技术工作，可以极大提高工作效率。所以，小米大数据和云平台联合开发了增长分析系统 Growing Analytics（下文中简称 GA)，旨在提供一个灵活的多维实时查询和分析平台，可以统一管理数据接入和查询方案，帮助业务线做好精细化运营。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="历史架构">历史架构<a class="hash-link" href="#历史架构" title="标题的直接链接">​</a></h2><p>增长分析平台立项于 2018 年年中，当时基于开发时间和成本，技术栈等因素的考虑，小米复用了现有各种大数据基础组件（HDFS, Kudu, SparkSQL 等），搭建了一套基于 Lamda 架构的增长分析查询系统。GA 系统初代版本的架构如下图所示，包含了以下几个方面：</p><ul><li>数据源：数据源是前端的埋点数据以及用户行为数据。</li><li>数据接入层：对埋点数据进行统一的清洗后写入小米内部自研的消息队列中，并通过 Spark Streaming 将数据导入Kudu 中。</li><li>存储层：在存储层中进行冷热数据分离。热数据存放在 Kudu 中，冷数据则会存放在 HDFS 上。同时在存储层中进行分区，当分区单位为天时，每晚会将一部分数据转冷并存储到 HDFS 上。</li><li>计算层/查询层：在查询层中，使用 SparkSQL 对 Kudu 与 HDFS 上数据进行联邦查询，最终把查询结果显示在前端页面。</li></ul><p><img loading="lazy" alt="2" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/2-f6f2fe0acf61bc2e3aefb9f853931c27.png" width="1080" height="603" class="img_ev3q"></p><p>在当时的历史背景下，初代版本的增长分析平台帮助我们解决了一系列用户运营过程中的问题，但同时在历史架构中也存在了两个问题：</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第一个问题组件分散">第一个问题：组件分散<a class="hash-link" href="#第一个问题组件分散" title="标题的直接链接">​</a></h3><p>由于历史架构是基于 SparkSQL + Kudu + HDFS 的组合，依赖的组件过多导致运维成本较高。原本的设计是各个组件都使用公共集群的资源，但是实践过程中发现执行查询作业的过程中，查询性能容易受到公共集群其他作业的影响，容易发生查询抖动，尤其在读取 HDFS 公共集群的数据时，有时较为缓慢。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第二个问题资源占用高">第二个问题：资源占用高<a class="hash-link" href="#第二个问题资源占用高" title="标题的直接链接">​</a></h3><p>通过 SparkSQL 进行查询时，延迟相对较高。SparkSQL 是基于批处理系统设计的查询引擎，在每个 Stage 之间交换数据 Shuffle 的过程中依然需要进行落盘，完成 SQL 查询的时延较高。为了保证 SQL 查询不受资源的影响，我们通过添加机器来保证查询性能，但是实践过程中发现，性能提升的空间有限，这套解决方案并不能充分地利用机器资源来达到高效查询的目的，存在一定的资源浪费。</p><p>针对上述两个问题，我们的目标是寻求一款计算、存储一体的 MPP 数据库来替代我们目前的存储计算层的组件，在通过技术选型后，最终我们决定使用 Apache Doris 替换老一代历史架构。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="重新选型">重新选型<a class="hash-link" href="#重新选型" title="标题的直接链接">​</a></h2><p>MPP架构的查询引擎，如Impala,Presto等能够高效地支持SQL查询，但是仍然需要依赖Kudu, HDFS, Hive Metastore等组件, 运维成本依然比较高。同时，由于计算存储分离，查询引擎不能很好地及时感知存储层的数据变化，就无法做更细致的查询优化。如想在SQL层做缓存就无法保证查询的结果是最新的。</p><p>Doris是Apache基金会顶级项目，主要定位是高性能的、支持实时的分析型数据库， 主要用于解决报表和多维分析。它主要集成了 Google Mesa 和 Cloudera Impala 技术。我们对Doris进行了内部的性能测试并多次和社区沟通交流，确定了用Doris替换原来的计算存储组件的解决方案。我们新的架构如下图所示：</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="基于-apache-doris-的新版架构">基于 Apache Doris 的新版架构<a class="hash-link" href="#基于-apache-doris-的新版架构" title="标题的直接链接">​</a></h2><p>新版架构从数据源获取埋点数据后，数据接入后写入 Apache Doris 后可以直接查询结果并在前端进行显示。真正实现了通过Doris统一了计算、存储，和资源管理yarn相关工具。</p><p><img loading="lazy" alt="3" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/3-266579e567d5c09c8931d7044813c707.png" width="1080" height="598" class="img_ev3q"></p><p>我们选择 Doris 原因：</p><ul><li>Doris 具有优秀的查询性能，能够满足业务需求。</li><li>Doris 支持标准 SQL ，用户使用与学习成本较低。</li><li>Doris 不依赖于其他的外部系统，运维简单。</li><li>Doris 社区拥有很高活跃度，版本迭代快。开发者规模大，有利于后续系统的维护升级。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="新旧架构性能对比">新旧架构性能对比<a class="hash-link" href="#新旧架构性能对比" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="4" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/4-c98f04af8754b651217aa474e7178d39.png" width="1061" height="546" class="img_ev3q"></p><p>我们选取了日均数据量大约 10 亿的业务，分别在不同场景下对Doris进行了性能测试，其中包含 6 个事件分析场景，3 个留存分析场景以及 3 个漏斗分析场景。经过与【SparkSQL+Kudu+HDFS】的旧方案对比后，我们发现：</p><ul><li>在事件分析的场景下，平均查询所耗时间降低了 85%。</li><li>在留存分析和漏斗分析场景下，平均查询所耗时间降低了 50%。</li></ul><h1>应用实践</h1><p>下面将介绍我们在Apache Doris应用中数据导入、数据查询、A/B测试的经验。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据导入">数据导入<a class="hash-link" href="#数据导入" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="5" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/5-adfeb633824992e5692635b13cfdfb78.png" width="1080" height="607" class="img_ev3q"></p><p>小米内部主要通过 Stream Load 与 Broker Load 以及少量 Insert 方式来导入数据到Doris。数据一般会先写入到消息队列中，分为实时数据和离线数据两个部分。
实时数据如何写入到Apache Doris 中：一部分实时数据通过 Flink数据处理 后， 并通过 Doris 社区提供的 Flink Doris Connector 组件写入到 Doris 中。另一部分数据通过 Spark Streaming 组件写入。这两种写入方式的底层都依赖的是社区提供的 Stream Load。
离线数据如何写入到Apache Doris 中：离线数据部分写入 Hive 后，通过小米的数据工场将数据导入到 Doris 中。用户可以直接在数据工场提交 Broker Load 任务并将数据直接导入 Doris 中，也可以通过 Spark SQL 将数据导入 。Spark SQL 方式则是依赖了 Doris 社区提供的 Spark Doris Connector 组件，其底层为 Doris 的 Stream Load 的封装。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据查询">数据查询<a class="hash-link" href="#数据查询" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="6" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/6-5ec3ef0ad093ee68dd7297622a064096.png" width="1080" height="596" class="img_ev3q"></p><p>用户通过数据工场将数据导入至 Doris 后即可进行查询。在小米内部，可以通过自研的数鲸平台进行查询的。用户可以通过数鲸平台对 Doris 进行可视化的查询，并展开用户行为分析和用户画像分析。其中，为帮助业务进行事件分析、留存分析、漏斗分析、路径分析等行为分析，我们为 Doris 添加了相应的 UDF （User Defined Function）和 UDAF (User Defined Aggregate Function)。
在即将发布的 1.2 版本中，Doris添加了外表元数据同步的功能，支持 Hive/Hudi/Iceberg 外表并增加了 Multi Catalog。查询外部表提升了性能，接入外表大幅增加了易用性。在未来，我们考虑直接通过 Doris 查询 Hive 与 Iceberg 数据，构建湖仓一体的架构。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ab测试">A/B测试<a class="hash-link" href="#ab测试" title="标题的直接链接">​</a></h2><p>小米的 A/B 实验平台对 Apache Doris 查询性能的提升有着迫切的需求，因此我们选择优先在小米的 A/B 实验平台上线 Apache Doris 向量化版本，也就是 1.1.2 版本。</p><p>小米的 A/B 实验平台是一款通过 A/B 测试的方式，借助实验分组、流量拆分与科学评估等手段来辅助完成科学的业务决策，最终实现业务增长的一款运营工具产品。在实际业务中，为了验证一个新策略的效果，通常需要准备原策略 A 和新策略 B 两种方案。随后在总体用户中取出一小部分，将这部分用户完全随机地分在两个组中，使两组用户在统计角度无差别。将原策略 A 和新策略 B 分别展示给不同的用户组，一段时间后，结合统计方法分析数据，得到两种策略生效后指标的变化结果，并以此来判断新策略 B 是否符合预期。</p><p>小米的 A/B 实验平台有几类典型的查询应用：用户去重、指标求和、实验协方差计算等，查询类型会涉及较多的 Count(distinct)、Bitmap计算、Like语句等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="上线前验证">上线前验证<a class="hash-link" href="#上线前验证" title="标题的直接链接">​</a></h3><p>我们基于 Apache Doris 1.1.2 版本搭建了一个和小米线上 Apache Doris 0.13 版本在机器配置和机器规模上完全相同的测试集群，用于向量化版本上线前的验证。验证测试分为两个方面：单 SQL 串行查询测试和批量 SQL 并发查询测试。在这两种测试中，我们在保证两个集群数据完全相同的条件下，分别在 Doris 1.1.2 测试集群和小米线上 Doris 0.13 集群执行相同的查询 SQL 来做性能对比。我们的目标是，Doris 1.1.2 版本在小米线上 Doris 0.13 版本的基础上有 1 倍的查询性能提升。
两个集群配置完全相同，具体配置信息如下：</p><ul><li>集群规模：3 FE + 89 BE</li><li>BE节点CPU:  Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz 16核 32线程 × 2</li><li>BE节点内存：256GB</li><li>BE节点磁盘：7.3TB × 12 HDD</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="单-sql-串行查询测试">单 SQL 串行查询测试<a class="hash-link" href="#单-sql-串行查询测试" title="标题的直接链接">​</a></h4><p>在该测试场景中，我们选取了小米A/B 实验场景中 7 个典型的查询 Case，针对每一个查询 Case，我们将扫描的数据时间范围分别限制为 1 天、7 天和 20 天进行查询测试，其中单日分区数据量级大约为 31 亿（数据量大约 2 TB），测试结果如图所示：</p><p><img loading="lazy" alt="7" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/7-8f6f2d02c9688f713ef48c8221c25158.png" width="750" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="8" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/8-0ee361fa5acabc282382a20b61f5baaa.png" width="749" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="9" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/9-f28d9b3dc18ad2f8314faaf514c5dc69.png" width="750" height="450" class="img_ev3q"></p><p>根据以上小米 A/B 实验场景下的单 SQL 串行查询测试结果所示，Doris 1.1.2 版本相比小米线上 Doris 0.13 版本至少有 3~5 倍的性能提升，效果显著。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="调优测试结果">调优测试结果<a class="hash-link" href="#调优测试结果" title="标题的直接链接">​</a></h3><p>我们基于小米的 A/B实验场景对 Apache Doris 1.1.2 版本进行了一系列调优，并将调优后的 Doris 1.1.2 版本与小米线上 Doris 0.13 版本分别进行了并发查询测试。测试情况如下：</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-1">测试 1<a class="hash-link" href="#测试-1" title="标题的直接链接">​</a></h4><p>我们选择了 A/B 实验场景中一批典型的用户去重、指标求和以及协方差计算的查询 Case（SQL 总数量为 3245）对两个版本进行并发查询测试，测试表的单日分区数据大约为 31 亿（数据量大约 2 TB），查询的数据范围会覆盖最近一周的分区。测试结果如图所示，Doris 1.1.2 版本相比 Doris0.13版本，总体的平均延迟降低了大约 48%，P95 延迟降低了大约 49%。在该测试中，Doris 1.1.2 版本相比 Doris0.13 版本的查询性能提升了接近 1 倍。</p><p><img loading="lazy" alt="10" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/10-c9499045fecce0f0eae927ba3e0ac883.png" width="1080" height="338" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-2">测试 2<a class="hash-link" href="#测试-2" title="标题的直接链接">​</a></h4><p>我们选择了 A/B实验场景下的 7 份 A/B 实验报告对两个版本进行测试，每份 A/B 实验报告对应小米 A/B实验平台页面的两个模块，每个模块对应数百或数千条查询 SQL。每一份实验报告都以相同的并发向两个版本所在的集群提交查询任务。测试结果如图所示，Doris 1.1.2 版本相比 Doris 0.13 版本，总体的平均延迟降低了大约 52%。在该测试中，Doris 1.1.2 版本相比 Doris 0.13 版本的查询性能提升了超过 1 倍。</p><p><img loading="lazy" alt="11" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/11-164d7e9bca3b81ccb6bae88a0048be41.png" width="750" height="450" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-3">测试 3<a class="hash-link" href="#测试-3" title="标题的直接链接">​</a></h4><p>为了验证调优后的 Apache Doris 1.1.2 版本在小米 A/B 实验场景之外的性能表现，我们选取了小米用户行为分析场景进行了 Doris 1.1.2 版本和 Doris 0.13 版本的并发查询性能测试。我们选取了 2022年10月24日、25日、26日和 27日这 4 天的小米线上真实的行为分析查询 Case 进行对比查询，测试结果如图所示，Doris 1.1.2 版本相比 Doris 0.13 版本，总体的平均延迟降低了大约7 7%，P95 延迟降低了大约 83%。在该测试中，Doris 1.1.2 版本相比 Doris 0.13 版本的查询性能有 4~6 倍的提升。</p><p><img loading="lazy" alt="12" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/12-125f05fc3d7544d5f15edf2ab41184e8.png" width="1080" height="338" class="img_ev3q"></p><h1>总结</h1><p>自从 Apache Doris 从 2019 年上线第一个业务至今，目前 Apache Doris 已经在小米内部服务了数十个业务及子品牌、集群数量达到数十个、节点规模达到数百台。每天完成数万次用户在线分析查询，承担了包括增长分析和报表查询等绝大多数在线分析的需求。</p><p>经过一个多月的性能调优和测试，Apache Doris 1.1.2 版本在查询性能和稳定性方面已经达到了小米 A/B实验平台的上线要求，在某些场景下的查询性能甚至超过了我们的预期，希望本次分享可以给有需要的朋友一些可借鉴的经验参考。</p><p>与此同时，在以上小米的实践中，已有部分功能在 Apache Doris 1.0 或 1.1 版本中发布，部分 PR 已经合入社区 Master，将在不久后发布的 1.2 新版本中与大家见面。随着社区的快速发展，有越来越多小伙伴参与到社区建设中，社区活跃度有了极大的提升。Apache Doris 已经变得越来越成熟，并开始从单一计算存储一体的分析型 MPP 数据库走向湖仓一体的道路，相信在未来，还会有更多的数据分析场景被探索和实现。</p><h1>联系我们</h1><p>官网：<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Github：<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>dev邮件组：<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/JD_OLAP">Apache Doris 在京东搜索实时 OLAP 探索与实践</a></h2><div class="blog-info"><time datetime="2022-12-02T00:00:00.000Z" itemprop="datePublished">2022年12月2日</time><span class="split-line"></span><span class="authors"><span class="s-author">李哲</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>京东搜索实时 OLAP 探索与实践</h1><p><img loading="lazy" alt="kv" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-e94fd46c1522a3383d161daec2249d18.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>前言
本文讨论了京东搜索在实时流量数据分析方面，利用Apache Flink和Apache Doris进行的探索和实践。流式计算在近些年的热度与日俱增：从Google Dataflow论文的发表，到Apache Flink计算引擎逐渐站到舞台中央，再到Apache Druid等实时分析型数据库的广泛应用，流式计算引擎百花齐放。但不同的业务场景，面临着不同的问题，没有哪一种引擎是万能的。我们希望京东搜索业务在流计算的应用实践，能够给到大家一些启发，也欢迎大家多多交流，给我们提出宝贵的建议。</p></blockquote><blockquote><p>作者：李哲，京东搜推数据开发工程师，曾就职于美团点评，主要从事离线数据开发、流计算开发以及OLAP多维查询引擎的应用开发。</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="京东与京东搜索">京东与京东搜索<a class="hash-link" href="#京东与京东搜索" title="标题的直接链接">​</a></h2><p>京东集团（NASDAQ：JD）中国领先的电商企业，2021年全年净收入达到9516亿元人民币。京东集团旗下设有京东零售、京东国际、京东科技、京东物流、京东云等。 京东集团于2014年5月在美国纳斯达克证券交易所正式挂牌上市。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="搜索业务对实时数据分析的需求">搜索业务对实时数据分析的需求<a class="hash-link" href="#搜索业务对实时数据分析的需求" title="标题的直接链接">​</a></h2><p>京东搜索作为电商平台的入口，为众多商家与用户提供连接的纽带。京东搜索发挥着导流的作用，给用户提供表达需求的入口；为了正确理解用户意图，将用户的需求进行高效的转化，线上同时运行着多个AB实验算法，遍及POP形态与自营形态的多个商品。而这些商品所属的品类、所在的组织架构以及品牌店铺等属性，都需要在线进行监控，以衡量转化的效果和承接的能力。目前搜索上层应用业务对实时数据的需求，主要包含三部分内容：
1、 搜索整体数据的实时分析。
2、 AB实验效果的实时监控。
3、 热搜词的Top榜单以反映舆情的变化。
这三部分数据需求，都需要进行深度的下钻，维度细化需要到SKU粒度。同时我们也承担着搜索实时数据平台的建设任务，为下游用户输出不同层次的实时流数据。
我们的用户包括搜索的运营、产品、算法以及采销人员。虽然不同用户关心的数据粒度不同、时间频率不同、维度也不同，但是我们希望能够建立统一的实时OLAP数据仓库，并提供一套安全、可靠的、灵活的实时数据服务。
目前每日新增的曝光日志达到几亿条记录，而拆分到SKU粒度的日志则要翻10倍，再细拆到AB实验的SKU粒度时，数据量则多达上百亿记录，多维数据组合下的聚合查询要求秒级响应时间，这样的数据量也给团队带来了不小的挑战。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时技术架构演进">实时技术架构演进<a class="hash-link" href="#实时技术架构演进" title="标题的直接链接">​</a></h2><p>我们之前的方案是以Apache Storm引擎进行点对点的数据处理，这种方式在业务需求快速增长的阶段，可以快速的满足实时报表的需求。但是随着业务的不断发展、数据量逐渐增加以及需求逐渐多样化，弊端随之产生。例如灵活性差、数据一致性无法满足、开发效率较低、资源成本增加等。</p><p><img loading="lazy" alt="page_2-zh" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/page_2-zh-3261d16224daf63c18e91b2644108848.png" width="1684" height="801" class="img_ev3q"></p><p>为解决之前架构出现的问题，我们首先进行了架构升级，将storm引擎替换为Apache Flink，用以实现高吞吐、exactly once的处理语义。同时根据搜索数据的特点，将实时数据进行分层处理，构建出PV流明细层、SKU流明细层和AB实验流明细层，期望基于不同明细层的实时流，构建上层的实时OLAP层。
OLAP层的在技术选型时，需要满足以下几点：
1：数据延迟在分钟级，查询响应时间在秒级
2：标准SQL交互引擎，降低使用成本
3：支持join操作，方便以维度级别增加属性信息
4：流量数据可以近似去重，但订单行要精准去重
5：高吞吐，每分钟数据量在千万级记录，每天数百亿条新增记录
6：前端业务较多，查询并发度不能太低
通过对比目前业界广泛使用的支持实时导入的OLAP引擎，我们在druid、ES、clickhouse和doris之间做了横向比较：</p><p><img loading="lazy" alt="page_3-zh" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/page_3-zh-bb25c0ea2faa03912dea231b8b207d3e.png" width="2315" height="758" class="img_ev3q"></p><p>通过对比开源的几款实时OLAP引擎，我们发现doris和clickhouse能够满足我们的需求，但是clickhouse的并发度太低是个潜在的风险，而且clickhouse的数据导入没有事务支持，无法实现exactly once语义，对标准sql的支持也是有限的。
最终，我们选定doris作为聚合层，用于实时OLAP分析。对于流量数据，使用聚合模型建表；对于订单行，我们使用Uniq模型，保证同一个订单最终只会存储一条记录，从而达到订单行精准去重的目的。在flink处理时，我们也将之前的任务拆解，将反复加工的逻辑封装，每一次处理都生成新的topic流，明细层细分了不同粒度的实时流。新方案如下：</p><p><img loading="lazy" alt="page_4-zh" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/page_4-zh-87f0c1b0fea8992d98b23ad9a02b3d1e.png" width="3004" height="1571" class="img_ev3q"></p><p>目前的技术架构中，flink的任务是非常轻的。基于生产的数据明细层，我们直接使用了doris来充当聚合层的功能，将原本可以在flink中实现的窗口计算，下沉到doris中完成。利用doris的routine load消费实时数据，虽然数据在导入前是明细粒度，但是基于聚合模型，导入后自动进行异步聚合。而聚合度的高低，完全根据维度的个数与维度的基数决定。通过在base表上建立rollup，在导入时双写或多写并进行预聚合操作，这有点类似于物化视图的功能，可以将数据进行高度的汇总，以提升查询性能。
在明细层采用kafka直接对接到doris，还有一个好处就是这种方式天然的支持数据回溯。数据回溯简单说就是当遇到实时数据的乱序问题时，可以将“迟到”的数据进行重新计算，更新之前的结果。这是因为我们导入的是明细数据，延迟的数据无论何时到达都可以被写入到表中，而查询接口只需要再次进行查询即可获得最新的计算结果。最终方案的数据流图如下：</p><p><img loading="lazy" alt="page_5-zh" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/page_5-zh-248c2ca88f12afd922abf431162b289c.png" width="1137" height="729" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris在大促期间的优化">Doris在大促期间的优化<a class="hash-link" href="#doris在大促期间的优化" title="标题的直接链接">​</a></h2><p>上文提到我们在doris中建立了不同粒度的聚合模型，包括PV粒度、SKU粒度以及AB实验粒度。我们这里以每日生产数据量最大的曝光AB实验模型为例，阐述在doris中如何支持大促期间每日新增百亿条记录的查询的。
AB实验的效果监控，业务上需要10分钟、30分钟、60分钟以及全天累计等四个时间段，同时需要根据渠道、平台和一二三级品类等维度进行下钻分析，观测的指标则包含曝光PV、UV、曝光SKU件次、点击PV、点击UV等基础指标，以及CTR等衍生指标。
在数据建模阶段，我们将曝光实时数据建立聚合模型，其中K空间包含日期字段、分钟粒度的时间字段、渠道、平台、一二三级品类等，V空间则包含上述的指标列，其中UV和PV进行HLL近似计算，而SKU件次则采用SUM函数，每到来一条新记录则加1。由于AB实验数据都是以AB实验位作为过滤条件，因此将实验位字段设置为分桶字段，查询时能够快速定位tablet分片。值得注意的是，HLL的近似度在目前PV和UV的基数下，实际情况误差在0.8%左右，符合预期。
目前doris的集群共30+台BE，存储采用的是支持NVMe协议的SSD硬盘。AB实验曝光topic的分区数为40+，每日新增百亿条数据。在数据导入阶段，我们主要针对导入任务的三个参数进行优化：最大时间间隔、最大数据量以及最大记录数。当这3个指标中任何一个达到设置的阈值时，任务都会触发导入操作。为了更好的了解任务每次触发的条件，达到10分钟消费6亿条记录的压测目标，我们通过间隔采样的方法，每隔3分钟采样一次任务的情况，获取Statistic信息中的receivedBytes、cimmittedTaskNum、loadedRows以及taskExecuteTimeMs数值。通过对上述数值在前后2个时间段的差值计算，确定每个任务触发的条件，并调整参数，以在吞吐和延迟之间进行平衡，最终达到压测的要求。
为了实现快速的多维数据查询，基于base表建立了不同的rollup，同时每个rollup的字段顺序，也要遵循过滤的字段尽可能放到前面的原则，充分利用前缀索引的特性。这里并不是rollup越多越好，因为每个rollup都会有相应的物理存储，每增加一个rollup，在写入时就会增加一份IO。最终我们在此表上建立了2个rollup，在要求的响应时间内尽可能多的满足查询需求。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结与展望">总结与展望<a class="hash-link" href="#总结与展望" title="标题的直接链接">​</a></h2><p>京东搜索是在2020年5月份引入doris的，规模是30+台BE，线上同时运行着10+个routine load任务，每日新增数据条数在200亿+，已经成为京东体量最大的doris用户。从结果看，用doris替换flink的窗口计算，既可以提高开发效率，适应维度的变化，同时也可以降低计算资源，用doris充当实时数据仓库的聚合层，并提供统一的接口服务，保证了数据的一致性和安全性。
我们在使用中也遇到了查询相关的、任务调度相关的bug，也在推动京东OLAP平台升级到最新版本。接下来待版本升级后，我们计划使用bitmap功能来支持UV等指标的精准去重操作，并将推荐实时业务应用doris实现。除此之外，为了完善实时数仓的分层结构，为更多业务提供数据输入，我们也计划使用适当的flink窗口开发聚合层的实时流，增加数据的丰富度和完整度。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Netease">Apache Doris 助力网易严选打造精细化运营 DMP 标签系统</a></h2><div class="blog-info"><time datetime="2022-11-30T00:00:00.000Z" itemprop="datePublished">2022年11月30日</time><span class="split-line"></span><span class="authors"><span class="s-author">刘晓东</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>应用实践｜Apache Doris 助力网易严选打造精细化运营 DMP 标签系统</h1><p><img loading="lazy" alt="1280X1280" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-a63c2e8908df91d10704f971aa636fa6.png" width="900" height="383" class="img_ev3q"></p><p><strong>导读</strong>:如果说互联网的上半场是粗狂运营，那么在下半场，精细化运营将是长久的主题，有数据分析能力才能让用户得到更好的体验。当下比较典型的分析方式是构建用户标签系统，本文将由网易严选分享 DMP 标签系统的建设以及 Apache Doris 在其中的应用实践。</p><p>作者<strong>｜</strong>刘晓东 网易严选资深开发工程师</p><p>如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题，有数据分析能力才能让用户得到更好的体验。当下比较典型的分析方式是构建用户标签系统，从而精准地生成用户画像，提升用户体验。今天分享的主题是网易严选 DMP 标签系统建设实践，<strong>主要围绕下面五点展开：</strong></p><ul><li>平台总览</li><li>标签生产 ：标签圈选&amp;生产链路</li><li>标签存储：存储方式&amp;存储架构演进</li><li>高性能查询</li><li>未来规划</li></ul><h1>平台总览</h1><p>DMP 作为网易严选的数据中台，向下连接数据，向上赋能业务，承担着非常重要的基石角色。</p><p><strong>DMP 的数据来源主要包括三大部分：</strong></p><ul><li>自营平台的 APP、小程序、PC 端等各端的业务日志</li><li>网易集团内部共建的一些基础数据</li><li>京东、淘宝、抖音等第三方渠道店铺的数据</li></ul><p>通过收集、清洗，将以上数据形成数据资产沉淀下来。DMP 在数据资产基础上形成了一套自己的标签产出、人群圈选和用户画像分析体系，从而为业务提供支撑，包括：智能化的选品、精准触达以及用户洞察等。总的来说，<strong>DMP 系统就是构建以数据为核心的标签体系和画像体系，从而辅助业务做一系列精细化的运营。</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1YstQ5zmLCicnNlrUrnjtPrwhGwZP8icqQLu19auVgPylzxdNHfxHcALQ/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><img loading="lazy" alt="1280X1280" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/kv-a63c2e8908df91d10704f971aa636fa6.png" width="900" height="383" class="img_ev3q"></p><p>了解 DMP 系统，先从以下几个概念开始。</p><ul><li><strong>标签</strong>: 对于实体（用户、设备、手机号等）特征的描述，是一种面向业务的数据组织形式，比如使用：年龄段、地址、偏好类目等对用户实体进行刻画。</li><li><strong>人群圈选</strong>: 通过条件组合从全体用户中圈选出一部分用户，具体就是指定一组用户标签和其对应的标签值，得到符合条件的用户人群。</li><li><strong>画像分析</strong>: 对于人群圈选结果，查看该人群的行为情况、标签分布。例如查看【城市为杭州，且性别为女性】的用户在严选 APP 上的行为路径、消费模型等。</li></ul><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1K1YX0UHuJFDsg6SR8SYS2ksIb3ZwDgnkxImicjl64Utr0RicVwRwIEwg/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>严选标签系统对外主要提供两大核心能力：</p><ol><li><p>标签查询：查询特定实体指定标签的能力，常用于基本信息的展示。</p></li><li><p>人群圈选：分为实时和离线圈选。<strong>圈选结果主要用于：</strong></p></li></ol><ul><li>分组判断：判读用户是否在指定的一个或多个分组，资源投放、触点营销等场景使用较多。</li><li>结果集拉取：拉取指定的人群数据到业务方系统中，进行定制化开发。</li><li>画像分析：分析特定人群的行为数据，消费模型等，进行更精细的运营。</li></ul><p><strong>整体的业务流程如下：</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1vwvAZ2kkh6QQaLLv45tWrxhVXdVyG4nRkPibjZicZlhib6NABc7drVI6w/640?wx_fmt=png" alt="img" class="img_ev3q"></p><ul><li>首先定义标签和人群圈选的规则；</li><li>定义出描述业务的 DSL 之后，便可以将任务提交到 Spark 进行计算；</li><li>计算完成之后，<strong>将计算结果存储到 Hive 和 Doris</strong>；</li><li>之后业务方便可以根据实际业务需求<strong>从 Hive 或</strong> <strong>Doris</strong> <strong>中查询使用数据</strong>。</li></ul><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPguh0t1dZwCVibOYEiamXy6y66P5JicLjFKuuddtibznqP5e9Zn7zhCjCzYlMqq22MqGbvliaQ909IBD0w/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>DMP 平台整体分为计算存储层、调度层、服务层、和元数据管理四大模块。</strong></p><p>所有的标签元信息存储在源数据表中；调度层对业务的整个流程进行任务调度：数据处理、聚合转化为基础标签，基础标签和源表中的数据通过 DSL 规则转化为可用于数据查询的 SQL 语义，由调度层将任务调度到计算存储层的 Spark 进行计算，<strong>并将计算结果存储到 Hive 和 Doris 中。</strong>服务层由标签服务、实体分组服务、基础标签数据服务、画像分析服务四部分组成。</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1jkd1Of63dlmia0lBtMgaJibzo0I2B59oeCEbj8m41TQybJAvgPM8DYAg/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>标签的生命周期包含5个阶段：</strong></p><ul><li><strong>标签需求</strong>: 在此阶段，运营提出标签的需求和价值预期，产品评估需求合理性以及紧迫性。</li><li><strong>排期生产</strong>: 此阶段需要数据开发梳理数据，从 ods 到 dwd 到 dm 层整个链路，根据数据建立模型，同时数据开发需要做好质量监控。 </li><li><strong>人群圈选</strong>: 标签生产出来之后进行应用，圈选出标签对应的人群。</li><li><strong>精准营销</strong>: 对圈选出来的人群进行精准化营销。</li><li><strong>效果评估</strong>: 最后产品、数据开发和运营对标签使用率、使用效果进行效果评估来决定后续对标签进行改进或降级。</li></ul><p>总的来说，就是以业务增长为目标，围绕标签的生命周期，投入合理的资源，最大化运营效果。</p><h1>标签生产</h1><p><strong>接下来介绍标签生产的整个过程。</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1R5yhhuMUqfab8L0ib57cd0Nuev7G6onY4TiaIiadCGsia7wJ1FvDYSico3w/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>标签的数据分层：</strong></p><ul><li>最下层是 ods 层，包括用户登录日志、埋点记录日志、交易数据以及各种数据库的 Binlog 数据。</li><li>对 ods 层处理后的数据到达 dwd 明细层，包括用户登录表、用户活动表、订单信息表等。</li><li>dwd 层数据聚合后到 dm 层，标签全部基于 dm 层数据实现。</li></ul><p>目前我们从原始数据库到 ods 层数据产出已经完全自动化，从 ods 层到 dwd 层实现了部分自动化，从 dwd 到 dm 层有一部分自动化操作，但自动化程度还不高，这部分的自动化操作是我们接下来的工作重点。</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1iacicmmGMQpvkibkCayaEfibwnz5yatmTpbBFXpIxMvMwfrzfYEZFUzsNQ/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>标签根据时效性分为</strong>：离线标签、近实时标签和实时标签。</p><p><strong>根据聚合粒度分为</strong>：聚合标签和明细标签。</p><p>通过类别维度可将标签分为：账号属性标签、消费行为标签、活跃行为标签、用户偏好标签、资产信息标签等。</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1nlRv6HFeBhWR9icFFJYS3anmyEUVZVHIic6Fo6YxGF0RSMFNSlP3c88A/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>直接将 dm 层的数据不太方便拿来用，原因在于：</strong></p><p>基础数据比较原始，抽象层次有所欠缺、使用相对繁琐。通过对基础数据进行与、或、非的组合，形成业务标签供业务方使用，可以降低运营的理解成本，降低使用难度。</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1LfWal1W06JHzQcdcXocU12S1r811v3DfIpbKHduTvxhWZnmNv3UNgQ/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>标签组合之后需要对标签进行具体业务场景应用，如人群圈选。配置如上图左侧所示，支持离线人群包和实时行为（需要分开配置）。</p><p>配置完后，生成上图右侧所示的 DSL 规则，以 Json 格式表达，对前端比较友好，也可以转成存储引擎的查询语句。</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1mSV0ghMXHMOl0GlurpVgDJ2OcX4KiaibQlZJdEVpcQN3HEEyjo9Xo7iag/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1ibxI3tWSJM5tpic4EyfxlBblicRV6WiaIqFwK9H927MenkqRZicXEeJdrKA/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>标签有一部分实现了自动化。在人群圈选部分自动化程度比较高。比如分组刷新，每天定时刷新；高级计算，如分组与分组间的交/并/差集；数据清理，及时清理过期失效的实体集。</p><h1>标签存储</h1><p><strong>下面介绍一下我们在标签存储方面的实践。</strong></p><p>严选 DMP 标签系统需要承载比较大的 C端流量，对实时性要求也比较高。</p><p>我们对存储的要求包括：</p><ul><li>支持高性能查询，以应对大规模 C端流量</li><li>支持 SQL，便于应对数据分析场景</li><li>支持数据更新机制</li><li>可存储大数据量</li><li>支持扩展函数，以便处理自定义数据结构</li><li>和大数据生态结合紧密</li></ul><p>目前还没有一款存储能够完全满足要求。</p><p><strong>我们第一版的存储架构如下图所示：</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1zFrMrNdNRI5GDcmBnQo3fApLIkjmFOUia2o7jyfdNsoAeHU69ialQusw/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>离线数据大部分存储在 Hive 中，小部分存储在 Hbase（主要用于基础标签的查询）。实时数据一部分存储在 Hbase 中用于基础标签的查询，部分双写到 KUDU 和 ES 中，用于实时分组圈选和数据查询。离线圈选的数据通过 impala 计算出来缓存在 Redis 中。</p><p><strong>这一版本的缺点包括：</strong></p><ul><li>存储引擎过多。</li><li>双写有数据质量隐患，可能一方成功一方失败，导致数据不一致。</li><li>项目复杂，可维护性较差。</li></ul><p>为了减少引擎和存储的使用量，提高项目可维护性，在版本一的基础上改进实现了版本二。</p><p><strong>我们第二版的存储架构如下图所示：</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1mlDkbiaNBOZ3NdP7Wbe10pA59RTS2rTRdXs4HIx545N98IrzKSK96HA/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>存储架构版本二引入了 Apache Doris</strong>，离线数据主要存储在 Hive 中，同时将基础标签导入到 Doris，实时数据也存储在 Doris，基于 Spark 做 Hive 加 Doris 的联合查询，并将计算出来的结果存储在 Redis 中。经过此版改进后，实时离线引擎存储得到了统一，性能损失在可容忍范围内（Hbase 的查询性能比 Doris 好一些，能控制在 10ms 以内，Doris 目前是 1.0 版本，p99，查询性能能控制在 20ms 以内，p999，能控制在 50ms 以内）；<strong>项目简化，降低了运维成本。</strong></p><p><strong>在大数据领域，各种存储计算引擎有各自的适用场景，如下表所示：</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1ZxBxNx3JrBUDT1jZ6ViaqMfQPt7z6Cugd7GOFS6F33L1PHC5HXI5ciaA/640?wx_fmt=png" alt="img" class="img_ev3q"></p><h1>高性能查询</h1><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj19cGc43b0KbcEnCFDb58XCCibCbCiassbkFB5c1WSE4WMetVJSicZpXNRg/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>分组存在性判断：判断用户是否在指定的一个分组或者多个分组。包括两大部分：</p><ul><li>第一部分为静态人群包，提前进行预计算，存入 Redis 中（Key 为实体的 ID，Value 为结果集 ID），采用 Lua 脚本进行批量判断，提升性能；</li><li>第二部分为实时行为人群，需要从上下文、API 和 Apache Doris 中提取数据进行规则判断。性能提升方案包括，异步化查询、快速短路、查询语句优化、控制 Join表数量等。</li></ul><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1LcNHF6380ct4bSv6r7MtNayJptnUUp7jsD27HXf7IjjicHPibJA9xkSQ/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>还有一个场景是人群分析：人群分析需要将人群包数据同多个表进行联合查询，分析行为路径。目前 Doris 还不支持路径分析函数，因此我们开发了 DorisUDF 来支持此业务。<strong>Doris 的计算模型对自定义函数的开发还是很友好的，能够比较好地满足我们的性能需要。</strong></p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1DZ8XPKg3icGpw3FzerppSjcXIcgz6hWED863oPiaMh0POvBbKuuib6bAQ/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>Apache Doris 在网易严选中已应用于点查、批量查询、路径分析、人群圈选等场景。在实践中具备以下优势：</strong></p><ul><li>在点查和少量表的联合查询性能 QPS 超过万级，RT99&lt;50MS。</li><li>水平扩展能力很强，运维成本相对比较低。</li><li>离线数据和实时数据相统一，降低标签模型复杂度。</li></ul><p>不足之处在于大量小数据量的导入任务资源占用较多，待 Doris 1.1.2 版本正式发布后我们也会及时同步升级。不过此问题已经在 Doris 1.1 版本中进行了优化，<strong>Doris 在 1.1 中大幅增强了数据 Compaction 能力，对于新增数据能够快速完成聚合，避免分片数据中的版本过多导致的 -235 错误以及带来的查询效率问题。</strong></p><p><strong>具体可以参考：</strong><a href="http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247500848&amp;idx=1&amp;sn=a667665ed4ccf4cf807a47be7c264f69&amp;chksm=cf2fca37f85843219e2f74d856478d4aa24d381c1d6e7f9f6a64b65f3344ce8451ad91c5af97&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">Apache Doris 1.1 特性揭秘：Flink 实时写入如何兼顾高吞吐和低延时</a></p><h1>未来规划</h1><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/zHbzQPKIBPgtennfZnKiaXyYX6AtIRWj1AOgeyT6rKS4Amy9BwZM6RJubVlW2X0CLOkTvUVAib08uxQ8H4TJ3q2g/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p><strong>提升存储&amp;计算性能</strong>: Hive 和 Spark 逐渐全部转向 Apache Doris。</p><p><strong>优化标签体系：</strong></p><ul><li>建立丰富准确的标签评价体系</li><li>提升标签质量和产出速度</li><li>提升标签覆盖率</li></ul><p><strong>更精准的运营</strong></p><ul><li>建立丰富的用户分析模型</li><li>从使用频次和用户价值两个方面提升用户洞察模型评价体系</li><li>建立通用化画像分析能力，辅助运营智能化决策</li></ul><h1>资料下载</h1><p>关注公众号「<strong>SelectDB</strong>」，后台回复【<strong>网易严选</strong>】获取本次演讲 <strong>PPT 资料</strong>！</p><p><img loading="lazy" src="https://mmbiz.qpic.cn/mmbiz_png/Uecg6b8kbSYAsL4vHMicGdVmylG5uibxhN3XOBT5xjTQVpyojBwHjhJgsRgD318FicEzjw0t3rK6F6HAZkDnDuq2A/640?wx_fmt=png" alt="img" class="img_ev3q"></p><p>SelectDB 是一家开源技术公司，致力于为 Apache Doris 社区提供一个由全职工程师、产品经理和支持工程师组成的团队，繁荣开源社区生态，打造实时分析型数据库领域的国际工业界标准。基于 Apache Doris 研发的新一代云原生实时数仓 SelectDB，运行于多家云上，为用户和客户提供开箱即用的能力。</p><p><strong>相关链接：</strong></p><p>SelectDB 官方网站：</p><p><a href="https://selectdb.com" target="_blank" rel="noopener noreferrer">https://selectdb.com</a> </p><p>Apache Doris 官方网站：</p><p><a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Apache Doris Github：</p><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>Apache Doris 开发者邮件组：</p><p><a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/NIO">Apache Doris 在蔚来汽车的应用</a></h2><div class="blog-info"><time datetime="2022-11-28T00:00:00.000Z" itemprop="datePublished">2022年11月28日</time><span class="split-line"></span><span class="authors"><span class="s-author">唐怀东</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>Apache Doris 在蔚来汽车的应用</h1><p><img loading="lazy" alt="NIO" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/NIO_kv-7601d71a49c7ecd7fb42f03de600ae6c.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>导读：本次分享的题目是Apache Doris在蔚来汽车的应用，主要包括以下几大部分：</p><ol><li>蔚来</li><li>OLAP在蔚来的发展</li><li>Doris作为统一OLAP数仓</li><li>Doris在运营平台上的实践</li><li>经验总结</li></ol></blockquote><p>作者：唐怀东，蔚来汽车 数据团队负责人</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="蔚来">蔚来<a class="hash-link" href="#蔚来" title="标题的直接链接">​</a></h2><p>蔚来（纽约证券交易所代码：NIO）是设计高端智能电动汽车市场的领先公司。 NIO 成立于 2014 年 11 月，设计、开发、联合制造和销售高端智能电动汽车，并不断推动自动驾驶、数字技术、电动动力总成和电池领域的创新。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="olap在蔚来的发展">OLAP在蔚来的发展<a class="hash-link" href="#olap在蔚来的发展" title="标题的直接链接">​</a></h2><p>首先，让我们来一起回顾OLAP在蔚来汽车的发展。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-2017年引入apache-druid">1. 2017年引入Apache Druid<a class="hash-link" href="#1-2017年引入apache-druid" title="标题的直接链接">​</a></h3><p>在当时可选择的OLAP存储和查询引擎并不多，比较常见的有Apache Druid、Apache Kylin。我们优先引入Druid的原因是以前有使用经验，而Kylin预计算虽然具有极高的查询效率优势，但是：</p><ul><li><p>Kylin底层最合适和最优的存储是HBase，之前公司并未引入，会额外增加运维的工作。</p></li><li><p>Kylin对各种维度和指标进行预计算，如果维度和维度取值非常多，会有维度爆炸的问题，对存储造成非常大的压力。</p></li></ul><p>Druid的优势很明显，支持实时和离线数据接入，列式存储，高并发，查询效率非常高。其缺点也比较明显：</p><ul><li>未使用标准协议例如JDBC，使用门槛高</li><li>Join的支持较弱</li><li>精确去重的效率低，性能会随之下降。整体性能要分场景去考虑，这也是我们后期去选型其他OLAP的原因</li><li>运维成本高，不同的组件有不同的安装方式和不同的依赖；数据导入还要考虑和Hadoop集成以及JAR包的依赖</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-2019年引入tidb">2. 2019年引入TiDB<a class="hash-link" href="#2-2019年引入tidb" title="标题的直接链接">​</a></h3><p><strong>TiDB是一个OLTP+OLAP的成熟引擎，同样是优点、缺点分明：</strong></p><p>优势：</p><ul><li>OLTP数据库，更新友好。</li><li>支持明细和聚合，有指标计算和数据看板展示，还支持明细数据查询</li><li>支持标准SQL，使用成本低</li><li>运维成本低</li></ul><p>劣势：</p><ul><li>它不是一个独立的OLAP。TiFlash依赖于OLTP，会增加存储。其OLAP能力稍显不足</li><li>整体性能要分场景去衡量</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-2021年引入doris">3. 2021年引入Doris<a class="hash-link" href="#3-2021年引入doris" title="标题的直接链接">​</a></h3><p>自2021年起，我们正式引入了Apache Doris。在系统选型过程中，产品的性能、SQL语法、系统兼容性、学习以及运维成本等多方面因素是我们最为关心的部分。经过深入调研、层层对比以下几个系统后，我们得出了如下结论：</p><p><strong>我们重点关注的Doris，其优点完全满足我们的诉求：</strong></p><ul><li>支持高并发查询（我们最关心的一点）</li><li>同时支持实时和离线数据</li><li>支持明细和聚合</li><li>Uniq模型支持更新</li><li>物化视图的能力能极大的加速查询效率</li><li>兼容MySQL协议，所以开发和使用成本比较低</li><li>性能完全满足我们的要求</li><li>运维成本比较低</li></ul><p><strong>Clickhouse，我们之前也调研过，也尝试想去使用它。其单机性能极强，但是缺点明显:</strong></p><ul><li>我们明确需要的场景下，它的多表join支持的稍微差一些</li><li>并发度比较低</li><li>运维成本极高</li></ul><p>凭借多种性能优势，Apache Doris比较理想地替代了Druid和TiDB。而Clickhouse在我们的业务上并不能很好的适配，让我们最终走向了Apache Doris。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris作为统一olap数仓">Doris作为统一OLAP数仓<a class="hash-link" href="#doris作为统一olap数仓" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="NIO" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/olap-96ad3bb86cebd92a200a0581f0418d3c.png" width="1018" height="669" class="img_ev3q"></p><p>这张图基本上就是从数据源到数据接入、数据计算、数据仓库、数据服务以及应用。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-数据源">1. 数据源<a class="hash-link" href="#1-数据源" title="标题的直接链接">​</a></h3><p>蔚来的场景下，数据源不仅仅指业务系统的数据，还有埋点数据、设备数据、车辆数据等等。数据会通过一种接入方式接入到大数据平台。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-数据接入">2. 数据接入<a class="hash-link" href="#2-数据接入" title="标题的直接链接">​</a></h3><p>对于一些业务系统的数据，可以开启CDC捕捉变化的数据，然后转换成一个数据流存储到Kafka，接续再进行流式的计算。某些只能通过批量的方式的数据会直接进入到我们的分布式存储。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-数据计算">3. 数据计算<a class="hash-link" href="#3-数据计算" title="标题的直接链接">​</a></h3><p>我们没有采用流批一体，采用的是Lambda架构。
我们本身的业务决定了我们的Lambda架构是离线和实时分成了两条路径：</p><ul><li>部分数据是流式的。</li><li>部分数据能够存储到数据流里，一些历史数据不会存储到Kafka。</li><li>有些场景数据要求高精准度。为了保证数据的准确性，一个离线的pipeline将会把整个数据重新计算和刷新。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-数据仓库">4. 数据仓库<a class="hash-link" href="#4-数据仓库" title="标题的直接链接">​</a></h3><p>数据计算到数仓，这两条线路我们没有采用Flink或Spark Doris Connector。我们用Routine Load来连接Apache Doris和Flink，用Broker Load连接Doris和Spark。 由Spark批量生成的数据，会备份到Hive供其他场景使用。这样每计算一次，就同时供多个场景去使用，大大提升了效率。Flink的情况也诸如此类。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-数据服务">5. 数据服务<a class="hash-link" href="#5-数据服务" title="标题的直接链接">​</a></h3><p>Doris后面是One Service。通过注册数据源或灵活配置的方式，自动生成API，对API进行流量的控制和权限的控制，灵活性大大提高。并借助于k8s serverless方案，整个服务非常灵活和丰富。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-数据应用">6. 数据应用<a class="hash-link" href="#6-数据应用" title="标题的直接链接">​</a></h3><p>应用层中我们主要是部署一些报表应用和其他的一些服务。</p><p>我们主要有两类使用场景：</p><ul><li>面向用户，类似于互联网，我们有很多用户的场景，包括看板和指标</li><li>面向车，车的数据通过这种方式进入到Doris，通过一定的聚合之后，Doris数据体量在几十亿级别。但总体性能仍然可以满足我们的要求。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris在运营平台上的实践">Doris在运营平台上的实践<a class="hash-link" href="#doris在运营平台上的实践" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-cdp-architecture">1. CDP Architecture<a class="hash-link" href="#1-cdp-architecture" title="标题的直接链接">​</a></h3><p><img loading="lazy" alt="NIO" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/cdp-3d65926e741a2837759b07514e914bbf.png" width="1471" height="422" class="img_ev3q"></p><p>接下来我来介绍Doris在运营平台上的实践。这是我们的真实使用场景。如今互联网公司普遍会做自己的CDP，它一般包括几个模块：</p><ul><li>标签，是最基础的部分。</li><li>圈人，基于标签，按照一定逻辑将人圈选出来。</li><li>洞察，针对圈定的人群，了解人群分布、特点。</li><li>触达，利用例如短信、电话、声音、APP通知、IM等方式触达到用户，并配合流量控制。</li><li>效果分析，提升运营平台的完整性，有动作、有效果、有反馈。</li></ul><p>Doris在这里面起到了最重要的作用，包括：标签存储、人群存储、效果分析。
标签分为基础标签和用户行为的基础数据，在此基础之上，我们可以灵活自定义其他标签。从实效性来看，标签还分为实时的标签和离线的标签。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-cdp存储选型的考量点">2. CDP存储选型的考量点<a class="hash-link" href="#2-cdp存储选型的考量点" title="标题的直接链接">​</a></h3><p>我们从5个维度去考量CDP存储的选型。</p><p>**(1) 离线和实时统一
如前所述标签有离线标签，有实时标签。目前我们是准实时的场景。对于有些数据，准实时已足够满足我们的需求，大量的标签还是离线的标签，采用的方式就是Doris的Routine Load和Broker Load。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>实时标签</td><td>数据实时更新</td><td>Routine Load</td></tr><tr><td>离线标签</td><td>高效大批量导入</td><td>Broker Load</td></tr><tr><td>流批统一</td><td>实时历险数据存储统一</td><td>Routine Load 和 Broker Load 更新同一张表的不同列</td></tr></tbody></table><p>另外同一张表上，不同列更新的频率也是不一样的。例如用户的基础标签，我们对用户的身份需要实时的更新，因为用户的身份是时刻变化的。T+1的更新不能满足我们的需求。有些标签离线，例如用户的性别、年龄等基础标签，T+1更新足以满足我们的标准。基础用户的原子标签放在一张表中带来的维护成本很低。当后期自定义标签时，表的数量会大大减少，这样对于整体性能的提升有极大好处。</p><p><strong>(2) 高效圈选</strong></p><p>用户运营有了标签，第二步就是圈人，圈选就是根据标签的不同组合，把符合标签条件的所有人筛选出来，这时会有不同标签条件组合的查询、这个查询在Doris引入向量化之后有比较明显的提升。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>复杂条件圈选</td><td>高效的支持多条件圈选</td><td>SIMD的优化</td></tr></tbody></table><p><strong>(3) 高效聚合</strong></p><p>前面提到的用户洞察或群体洞察以及效果分析统计，需要对数据做统计分析，并不是单一的按用户ID获取标签的这种简单场景。其读取的数据量和查询效率，对我们这个标签的分布、群体的分布、效果分析的统计都有很大的影响。在这里，体现到的Doris的功能特点是：</p><ul><li>第一是数据分片，我们按时间把数据分片，分析统计就会极大的减少数据量，可以极大的加速查询和分析的效率。</li><li>第二是节点聚合，然后再收集做统一的聚合。</li><li>第三是向量化加速，向量化引擎对性能提升非常显著。</li></ul><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>标签值的分布</td><td>每天都需要更新所有标签，需要快速高效统计</td><td>数据分片，减少数据传输和计算</td></tr><tr><td>群体的分布</td><td>同上</td><td>存算统一，每个节点先聚合</td></tr><tr><td>效果分析的统计值</td><td>同上</td><td>SIMD提速</td></tr></tbody></table><p><strong>(4) 多表关联</strong></p><p>我们的CDP可能和业内常见的CDP场景不太一样，因为有些场景的CDP标签是提前预估完成的，不存在自定义标签。只做原子标签，或者说用户基础行为数据的统计，这样可以把灵活性留给使用CDP的用户，根据自己的业务场景去自定义标签。底层的数据是分散在不同的数据库表里，如果做自定义的标签的建设，势必需要做表的关联。
我们选择Doris一个非常重要的原因，就是多表关联的能力。通过性能测试，Doris目前能够满足我们的要求。而且Doris为用户提供了非常强大的能力。因为标签是动态的。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>群体的特征分布</td><td>统计群体在某个特征下的分布</td><td>多表关联</td></tr><tr><td>Single Tag</td><td>Display tags</td><td></td></tr></tbody></table><p><strong>(5) 联邦查询</strong></p><p>用户触达成功与否我们会记录到TiDB。用户运营中的通知，可能只影响用户体验，如果涉及到钱例如发放积分或优惠券，任务执行就要做到不重不漏，这种OLTP场景用TiDB比较合适。
做效果分析，需要了解运营计划执行到什么程度，是否达成目标，其分布情况等等。需要把任务执行情况和人群圈选相结合才能进行分析，就会用到Doris和TiDB的关联，外表关联进行查询。
我们设想标签体量比较小，保存到es可能比较合适，然而ES不能满足我们的需求，后面会解释其原因。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>效果分析关联任务执行明细</td><td>Doris数据关联TiDB数据</td><td>关联外表进行查询</td></tr><tr><td>人群标签关联行为聚合数据</td><td>Doris数据关联Elasticsearch数据</td><td></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="经验和总结">经验和总结<a class="hash-link" href="#经验和总结" title="标题的直接链接">​</a></h2><ol><li><p><strong>bitmap</strong>. 我们的体量无法充分发挥其效率。如果体量达到一定程度，用bitmap会有很好的性能提升。例如计算UV场景，Id全集大于5000万，可以考虑bitmap聚合。</p></li><li><p><strong>ES外表。单表查询下效率比较理想。</strong> </p></li><li><p><strong>分批更新列</strong>. 为了减少表的数量和提升join表的性能，设计表尽量精简尽量聚合，相同类型的事实都放在一起。但相同类型的字段可能更新频率不同，有些字段需要天级更新，有些字段可能需要小时级更新，单独更新某一列就是一个明显的诉求。Doris聚合模型单独更新某些列的解决方案是使用REPLACE_IF_NOT_NULL。注意:用null替换原来的非null值是做不到的,可以把所有的null替换成有意义的默认值，例如unknown。</p></li><li><p><strong>在线服务</strong>. Doris同一份数据同时服务在线离线场景，对资源隔离的要求比较高，目前还存在进一步优化的空间。</p></li></ol></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/xiaomi">Apache Doris 在小米数据场景的应用实践与优化</a></h2><div class="blog-info"><time datetime="2022-08-15T00:00:00.000Z" itemprop="datePublished">2022年8月15日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>背景</h1><p>因增长分析业务需要，小米集团于 2019 年首次引入了 Apache Doris 。经过三年时间的发展，目前 Apache Doris 已经在广告投放、新零售、增长分析、数据看板、天星数科、小米有品、用户画像等小米内部数十个业务中得到广泛应用 <strong>，并且在小米内部已经形成一套以 Apache Doris 为核心的数据生态。</strong>
<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/25d7c2c45acd4e1c8c1a1742016fc6b9~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">
当前 Apache Doris 在小米内部已经具有<strong>数十个</strong>集群、总体达到<strong>数百台</strong> BE 节点的规模，其中单集群最大规模达到<strong>近百台节点</strong>，拥有<strong>数十个</strong>流式数据导入产品线，每日单表最大增量 <strong>120 亿</strong>、支持 <strong>PB 级别</strong>存储，单集群每天可以支持 <strong>2W 次以上</strong>的多维分析查询。</p><h1>架构演进</h1><p>小米引入 Apache Doris 的初衷是为了解决内部进行用户行为分析时所遇到的问题。随着小米互联网业务的发展，各个产品线利用用户行为数据对业务进行增长分析的需求越来越迫切。让每个业务产品线都自己搭建一套增长分析系统，不仅成本高昂，也会导致效率低下。因此能有一款产品能够帮助他们屏蔽底层复杂的技术细节，让相关业务人员能够专注于自己的技术领域，可以极大提高工作效率。基于此，小米大数据和云平台联合开发了增长分析系统 Growing Analytics（下文中简称 GA )，旨在提供一个灵活的多维实时查询和分析平台，统一数据接入和查询方案，帮助业务线做精细化运营。（此处内容引用自：<a href="https://mp.weixin.qq.com/s?__biz=MzUxMDQxMDMyNg==&amp;mid=2247486817&amp;idx=1&amp;sn=99fbef15b4d6f6059c3affbc77517e6e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">基于Apache Doris的小米增长分析平台实践</a>）</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/897a0453e1a540ae88cdf05ee9188b56~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>分析、决策、执行是一个循环迭代的过程，在对用户进行行为分析后，针对营销策略是否还有提升空间、是否需要在前端对用户进行个性化推送等问题进行决策，帮助小米实现业务的持续增长。这个过程是对用户行为进行<strong>分析-决策-优化执行-再分析-再决策-再优化执行</strong>的迭代过程。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="历史架构">历史架构<a class="hash-link" href="#历史架构" title="标题的直接链接">​</a></h3><p>增长分析平台立项于 2018 年年中，当时基于开发时间和成本，技术栈等因素的考虑，小米复用了现有各种大数据基础组件（HDFS, Kudu, SparkSQL 等），搭建了一套基于 Lamda 架构的增长分析查询系统。<strong>GA 系统初代版本的架构如下图所示，包含了以下几个方面：</strong></p><ul><li>数据源：数据源是前端的埋点数据以及可能获取到的用户行为数据。</li><li>数据接入层：对埋点数据进行统一的清洗后打到小米内部自研的消息队列 Talos 中，并通过 Spark Streaming 将数据导入存储层 Kudu 中。</li><li>存储层：在存储层中进行冷热数据分离。热数据存放在 Kudu 中，冷数据则会存放在 HDFS 上。同时在存储层中进行分区，当分区单位为天时，每晚会将一部分数据转冷并存储到 HDFS 上。</li><li>计算层/查询层：在查询层中，使用 SparkSQL 对 Kudu 与 HDFS 上数据进行联合视图查询，最终把查询结果在前端页面上进行显示。</li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9039c4f9ef8a4a3cbfd092b21233e831~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>在当时的历史背景下，初代版本的增长分析平台帮助我们解决了一系列用户运营过程中的问题，但同时在历史架构中也存在了两个问题：</strong></p><p><strong>第一个问题：</strong> 由于历史架构是基于 SparkSQL + Kudu + HDFS 的组合，依赖的组件过多导致运维成本较高。原本的设计是各个组件都使用公共集群的资源，但是实践过程中发现执行查询作业的过程中，查询性能容易受到公共集群其他作业的影响，容易抖动，尤其在读取 HDFS 公共集群的数据时，有时较为缓慢。</p><p><strong>第二个问题：</strong> 通过 SparkSQL 进行查询时，延迟相对较高。SparkSQL 是基于批处理系统设计的查询引擎，在每个 Stage 之间交换数据 Shuffle 的过程中依然需要落盘操作，完成 SQL 查询的时延较高。为了保证 SQL 查询不受资源的影响，我们通过添加机器来保证查询性能，但是实践过程中发现，性能提升的空间有限，这套解决方案并不能充分地利用机器资源来达到高效查询的目的，存在一定的资源浪费。 <strong>（此处内容引用自：<a href="https://mp.weixin.qq.com/s?__biz=MzUxMDQxMDMyNg==&amp;mid=2247486817&amp;idx=1&amp;sn=99fbef15b4d6f6059c3affbc77517e6e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">基于Apache Doris的小米增长分析平台实践</a>）</strong></p><p>针对上述两个问题，我们的目标是寻求一款计算存储一体的 MPP 数据库来替代我们目前的存储计算层的组件，<strong>在通过技术选型后，最终我们决定使用 Apache Doris 替换老一代历史架构。</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="基于-apache-doris-的新版架构">基于 Apache Doris 的新版架构<a class="hash-link" href="#基于-apache-doris-的新版架构" title="标题的直接链接">​</a></h3><p>当前架构从数据源获取前端埋点数据后，通过数据接入层打入 Apache Doris 后可以直接查询结果并在前端进行显示。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/540f5fa779af4b629869e54b793ea273~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>选择 Doris 原因：</strong></p><ul><li><p>Doris 具有优秀的查询性能，能够满足业务需求。</p></li><li><p>Doris 支持标准 SQL ，用户使用与学习成本较低。</p></li><li><p>Doris 不依赖于其他的外部系统，运维简单。</p></li><li><p>Doris 社区拥有很高活跃度，有利于后续系统的维护升级。</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新旧架构性能对比">新旧架构性能对比<a class="hash-link" href="#新旧架构性能对比" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ada8246b409a4cb6b11ffd2454aa2b06~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们选取了日均数据量大约 10 亿的业务，分别在不同场景下进行了性能测试，其中包含 6 个事件分析场景，3 个留存分析场景以及 3 个漏斗分析场景。<strong>经过对比后，得出以下结论：</strong></p><ul><li>在事件分析的场景下，平均查询所耗时间<strong>降低了 85%</strong> 。</li><li>在留存分析和漏斗分析场景下，平均查询所耗时间<strong>降低了 50%</strong> <strong>。</strong></li></ul><h1>应用实践</h1><p>随着接入业务的增多和数据规模的增长，让我们也遇到不少问题和挑战，下面我们将介绍在<strong>使用 Apache Doris 过程中沉淀出来的一些实践经验</strong>。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据导入">数据导入<a class="hash-link" href="#数据导入" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8afce198933f4ca4b2c97d4cf85b27de~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">小米内部主要通过 Stream Load 与 Broker Load 以及少量 Insert 方式来进行 Doris 的数据导入。数据一般会先打到 Talos 消息队列中，并分为实时数据和离线数据两个部分。</p><p><strong>实时数据写入 Apache Doris 中：</strong></p><p> 一部分业务在通过 Flink 对数据进行处理后，会通过 Doris 社区提供的 Flink Doris Connector 组件写入到 Doris 中，底层依赖于 Doris Stream Load 数据导入方式。也有一部分会通过 Spark Streaming 封装的 Stream Load 将数据导入到 Doris 中。</p><p><strong>离线数据写入</strong> <strong>Apache Doris 中：</strong></p><p>离线数据部分则会先写到 Hive 中，再通过小米的数据工场将数据导入到 Doris 中。用户可以直接在数据工场提交 Broker Load 任务并将数据直接导入 Doris 中，也可以通过 Spark SQL 将数据导入 Doris 中。Spark SQL 方式则是依赖了 Doris 社区提供的 Spark Doris Connector 组件，底层也是对 Doris 的 Stream Load 数据导入方式进行的封装。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据查询">数据查询<a class="hash-link" href="#数据查询" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c1cd3554e854dbe99aba27499e28118~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>用户通过数据工场将数据导入至 Doris 后即可进行查询，在小米内部是通过小米自研的数鲸平台来做查询的。用户可以通过数鲸平台对 Doris 进行查询可视化，并实现用户行为分析（为满足业务的事件分析、留存分析、漏斗分析、路径分析等行为分析需求，我们为 Doris 添加了相应的 UDF 和 UDAF ）和用户画像分析。</p><p>虽然目前依然需要将 Hive 的数据导过来，但 Doris 社区也正在支持湖仓一体能力，在后续实现湖仓一体能力后，我们会考虑直接通过 Doris 查询 Hive 与 Iceberg 外表。<strong>值得一提的是，Doris 1.1 版本已经实现支持查询 Iceberg 外表能力。</strong> 同时在即将发布的 <strong>1.2 版本</strong>中，还将支持 Hudi 外表并增加了 Multi Catalog ，可以实现外部表元数据的同步，无论是查询外部表的性能还是接入外表的易用性都有了很大的提升。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="compaction-调优">Compaction 调优<a class="hash-link" href="#compaction-调优" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/92ad4ea90c564af2b720080b449c6edf~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>Doris 底层采用类似 LSM-Tree 方式，支持快速的数据写入。每一次的数据导入都会在底层的 Tablet 下生成一个新的数据版本，每个数据版本内都是一个个小的数据文件。单个文件内部是有序的，但是不同的文件之间又是无序的。为了使数据有序，在 Doris 底层就会存在 Compaction 机制，异步将底层小的数据版本合并成大的文件。Compaction 不及时就会造成版本累积，增加元数据的压力，并影响查询性能。由于 Compaction 任务本身又比较耗费机器CPU、内存与磁盘资源，如果 Compaction 开得太大就会占用过多的机器资源并影响到查询性能，同时也可能会造成 OOM。<strong>针对以上问题，我们一方面从业务侧着手，通过以下方面引导用户：</strong></p><ul><li>通过引导业务侧进行合理优化，对表设置<strong>合理的分区和分桶</strong>，避免生成过多的数据分片。</li><li>引导用户尽量<strong>降低数据的导入频率</strong> <strong>，</strong> <strong>增大单次数据导入的量</strong>，降低 Compaction 压力。</li><li>引导用户<strong>避免过多使用会在底层生成 Delete 版本的 Delete 操作</strong>。在 Doris 中 Compaction 分为 Base Compaction 与 Cumulative Compaction。Cumulative Compaction 会快速的把大量新导入的小版本进行快速的合并，在执行过程中若遇到 Delete 操作就会终止并将当前 Delete 操作版本之前的所有版本进行合并。由于 Cumulative Compaction 无法处理 Delete 版本，在合并完之后的版本会和当前版本一起放到 Base Compaction 中进行。当 Delete 版本特别多时， Cumulative Compaction 的步长也会相应变短，只能合并少量的文件，导致 Cumulative Compaction 不能很好的发挥小文件合并效果。</li></ul><p><strong>另一方面我们从运维侧着手：</strong></p><ul><li><strong>针对不同的业务集群配置不同的 Compaction 参数。</strong> 部分业务是实时写入数据的，需要的查询次数很多，我们就会将 Compaction 开的大一点以达到快速合并目的。而另外一部分业务只写今天的分区，但是只对之前的分区进行查询，在这种情况下，我们会适当的将 Compaction 放的小一点，避免 Compaction 占用过大内存或 CPU 资源。到晚上导入量变少时，之前导入的小版本能够被及时合并，对第二天查询效率不会有很大影响。</li><li><strong>适当降低 Base Compaction 任务优先级并增加 Cumulative Compaction 优先级。</strong> 根据上文提到的内容，Cumulative Compaction 能够快速合并大量生成的小文件，而 Base Compaction 由于合并的文件较大，执行的时间也会相应变长，读写放大也会比较严重。所以我们希望 Cumulative Compaction 优先、快速的进行。</li><li><strong>增加版本积压报警。</strong> 当我们收到版本积压报警时，动态调大 Compaction 参数，尽快消耗积压版本。</li><li><strong>支持手动触发指定表与分区下数据分片的 Compaction 任务。</strong> 由于 Compaction 不及时，部分表在查询时版本累积较多并需要能够快速进行合并。所以，我们支持对单个表或单个表下的某个分区提高 Compaction 优先级。</li></ul><p><strong>目前 Doris 社区针对以上问题已经做了</strong> <strong>一系列的优化</strong> <strong>，在 1.1 版本中</strong> <strong>大幅增强了数据 Compaction 能力，对于新增数据能够快速完成聚合，避免分片数据中的版本过多导致的 -235 错误以及带来的查询效率问题。</strong>\
<strong>首先</strong>，在 Doris 1.1 版本中，引入了 QuickCompaction，增加了主动触发式的 Compaction 检查，在数据版本增加的时候主动触发 Compaction。同时通过提升分片元信息扫描的能力，快速的发现数据版本多的分片，触发 Compaction。通过主动式触发加被动式扫描的方式，彻底解决数据合并的实时性问题。</p><p><strong>同时</strong>，针对高频的小文件 Cumulative Compaction，实现了 Compaction 任务的调度隔离，防止重量级的 Base Compaction 对新增数据的合并造成影响。</p><p><strong>最后</strong>，针对小文件合并，优化了小文件合并的策略，采用梯度合并的方式，每次参与合并的文件都属于同一个数据量级，防止大小差别很大的版本进行合并，逐渐有层次的合并，减少单个文件参与合并的次数，能够大幅的节省系统的 CPU 消耗。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd2f0a547d6e4ddcb027715c4a544c5a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"><strong>在社区 1.1 新版本的测试结果中，不论是Compaction 的效率、CPU 的资源消耗，还是高频导入时的查询抖动，效果都有了大幅的提升。</strong></p><p><strong>具体可以参考：</strong> <a href="http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247500848&amp;idx=1&amp;sn=a667665ed4ccf4cf807a47be7c264f69&amp;chksm=cf2fca37f85843219e2f74d856478d4aa24d381c1d6e7f9f6a64b65f3344ce8451ad91c5af97&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">Apache Doris 1.1 特性揭秘：Flink 实时写入如何兼顾高吞吐和低延时</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="监控报警">监控报警<a class="hash-link" href="#监控报警" title="标题的直接链接">​</a></h3><p>Doris 的监控主要是通过 Prometheus 以及 Grafana 进行。对于 Doris 的报警则是通过 Falcon 进行。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3fbe6b44f1124a91bf5ee17608f302d5~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">小米内部使用 Minos 进行集群部署。Minos 是小米内部自研并开源的大数据服务进程管理工具。在完成 Doris 集群部署后会更新至小米内部的轻舟数仓中。在轻舟数仓中的节点注册到 ZooKeeper 后，Prometheus 会监听 ZooKeeper 注册的节点，同时访问对应端口，拉取对应 Metrics 。在这之后，Grafana 会在面板上对监控信息进行显示，若有指标超过预设的报警阈值，Falcon 报警系统就会在报警群内报警，同时针对报警级别较高或某些无法及时响应的警告，可直接通过电话呼叫值班同学进行报警。</p><p>另外，小米内部针对每一个 Doris 集群都有 Cloud - Doris 的守护进程。Could - Doris 最大功能是可以对 Doris 进行可用性探测。比如我们每一分钟对 Doris 发送一次 select current timestamp(); 查询，若本次查询 20 秒没有返回，我们就会判断本次探测不可用。小米内部对每一个集群的可用性进行保证，通过上述探测方法，可以在小米内部输出 Doris可用性指标。</p><h1>小米对Apache Doris的优化实践</h1><p>在应用 Apache Doris 解决业务问题的同时，我们也发现了 Apache Doris 存在的一些优化项，因此在与社区进行沟通后我们开始深度参与社区开发，解决自身问题的同时也及时将开发的重要 Feature 回馈给社区，具体包括 Stream Load 两阶段提交（2PC）、单副本数据导入、Compaction 内存限制等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stream-load-两阶段提交2pc">Stream Load 两阶段提交（2PC)<a class="hash-link" href="#stream-load-两阶段提交2pc" title="标题的直接链接">​</a></h3><p><strong>遇到的问题</strong></p><p>在 Flink 和 Spark 导入数据进 Doris 的过程中，当某些异常状况发生时可能会导致如下问题：</p><p><strong>Flink 数据重复导入</strong> <strong>：</strong> Flink 通过周期性 Checkpoint 机制处理容错并实现 EOS，通过主键或者两阶段提交实现包含外部存储的端到端 EOS。Doris-Flink-Connector 1.1 之前 UNIQUE KEY 表通过唯一键实现了EOS，非 UNIQUE KEY 表不支持 EOS。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e7750384cac44a569c8edf6c5de61744~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p> <strong>Spark SQL 数据部分导入</strong> <strong>：</strong> 通过 SparkSQL 从 Hive 表中查出的数据并写入 Doris 表中的过程需要使用到 Spark Doris Connector 组件，会将 Hive 中查询的数据通过多个 Stream Load 任务写入 Doris 中，出现异常时会导致部分数据导入成功，部分导入失败。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/936ffd500f364f838a9976584727ed42~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Stream Load 两阶段提交设计</strong></p><p>以上两个问题可以通过导入支持两阶段提交解决，第一阶段完成后确保数据不丢且数据不可见，这就能保证第二阶段发起提交时一定能成功，也能够保证第二阶段发起取消时一定能成功。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50e59f3a78f74ba6a8dd2d7960497adb~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Doris 中的写入事务分为三步：</strong></p><ol><li>在  FE 上开始事务，状态为 Prepare ；</li><li>数据写入 BE；</li><li>多数副本写入成功的情况下，提交事务，状态变成 Committed，并且 FE 向 BE 下发 Publish Version 任务，让数据立即可见。</li></ol><p>引入两阶段提交之后，第 3 步变为状态修改为 Pre Commit，Publish Version 在第二阶段完成。用户在第一阶段完成后（事务状态为 Pre Commit ），可以选择在第二阶段放弃或者提交事务。</p><p><strong>支持 Flink Exactly-Once 语义</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ef5e0a81b441487ba7c3b3fa22e8c85d~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">Doris-Flink-Connector 1.1 使用两阶段 Stream Load 并支持 Flink 两阶段提交实现了 EOS，只有全局的 Checkpoint 完成时，才会发起 Sream Load 的第二阶段提交，否则发起第二阶段放弃。</p><p><strong>解决 SparkSQL 数据部分导入</strong></p><p>Doris-Spark-Connector 使用两阶段 Stream Load 之后，成功的 Task 通过 Stream Load 第一阶段将写入数据到 Doris （Pre Commit 状态，不可见），当作业成功后，发起所有 Stream Load 第二阶段提交，作业失败时，发起所有 Stream Load 第二阶段取消。这就确保了不会有数据部分导入的问题。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/26b11a29566946c99b53ef90e01665ef~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="单副本数据导入优化">单副本数据导入优化<a class="hash-link" href="#单副本数据导入优化" title="标题的直接链接">​</a></h3><p><strong>单副本数据导入设计</strong></p><p><strong>Doris 通过多副本机制确保数据的高可靠以及系统高可用。</strong> 写入任务可以按照使用的资源分为计算和存储两类：排序、聚合、编码、压缩等使用的是 CPU 和内存的计算资源，最后的文件存储使用存储资源，三副本写入时计算和存储资源会占用三份。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0012b34b7404e5482700c281f6c206f~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>那能否只写一份副本数据在内存中，待到单副本写入完成并生成存储文件后，将文件同步到另外两份副本呢？答案是可行的，因此针对三副本写入的场景，我们做了单副本写入设计。<strong>单副本数据在内存中做完排序、聚合、编码以及压缩后，将文件同步至其他两个副本，这样很大程度上可以节省出 CPU 和内存资源。</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3528e0d75184068aa3b50384cb548d1~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>性能对比测试</strong></p><p><strong>Broker Load 导入 62G 数据性能对比</strong>
<strong>导入时间：</strong> 三副本导入耗时 33 分钟，单副本导入耗时 31 分钟。</p><p><strong>内存使用：</strong> 内存使用上优化效果十分明显，三副本数据导入的内存使用是单副本导入的三倍。单副本导入时只需要写一份内存，但是三副本导入时需要写三份内存，内存优化达到了 3 倍。</p><p><strong>CPU 消耗对比：</strong> 三副本导入的 CPU 消耗差不多是单副本的三倍。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cbe6bb648e8d47d09c556eed4ffcdfa9~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>并发场景性能对比</strong></p><p>测试中向  100 个表并发导入数据，每个表有 50 个导入任务，任务总数为 5000 个。单个 Stream Load 任务导入的数据行是 200 万行，约为 90M 的数据。测试中开了 128 个并发，<strong>将</strong> <strong>单副本导入和三副本导入进行了对比：</strong></p><p><strong>导入时间：</strong> 3 副本导入耗时 67 分钟，而后单副本耗时 27 分钟完成。导入效率相当提升两倍以上。</p><p><strong>内存使用：</strong> 单副本的导入会更低。</p><p><strong>CPU消耗对比：</strong> 由于都已经是开了并发在导入，CPU开销都比较高，但是单副本导入吞吐提升明显。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5a4f5533c4184f8caab39c38d951e410~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Compaction 内存限制</strong></p><p>之前 Doris 在单机磁盘一次导入超过 2000 个 Segment 的情况下，Compaction 有内存 OOM 的问题。对于当天写入但不查当天数据而是查询之前的数据业务场景，我们会把 Compaction 稍微放的小一点，避免占用太大的内存，导致进程 OOM。Doris 之前每个磁盘有固定的线程做存储在这个盘上的数据的 Compaction，没有办法在全局进行管控。因为我们要限制单个节点上面内存的使用，<strong>所以我们将该模式改成了生产者-消费者模式：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ede14473f9104bdc89213e82398ba32a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>生产者不停的从所有的磁盘上面生产任务，之后将生产任务提交到线程池中。我们可以很好的把控线程池的入口，达到对 Compaction 的限制。我们在合并时会把底层的小文件进行归并排序，之后在内存里给每一个文件开辟 Block，所以我们可以近似认为占用的内存量与文件的数量是相关的，从而可以通过对单节点上同时执行合并的文件数量做限制，来达到控制内存的效果。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00803f23d5a0427fb57abde4a2b1ec2d~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>我们增加了对单个 BE Compaction 合并的文件数量的限制。</strong> 若正在进行的 Compaction 的文件数量超过或等于当前限制时，后续提交上来的任务就需要等待，等到前面的 Compaction 任务做完并将指标释放出来后，后边提交进来的那些任务才可以进行。</p><p>通过这种方式，我们对某些业务场景做了内存的限制，很好的避免集群负载高时占用过多内存导致 OOM 的问题。</p><h1>总结</h1><p>自从 Apache Doris 从 2019 年上线第一个业务至今，<strong>目前 Apache Doris 已经在小米内部服务了数十个业务、集群数量达到数十个、节点规模达到数百台、每天完成数万次用户在线分析查询，承担了包括增长分析和报表查询等场景绝大多数在线分析的需求。</strong></p><p>与此同时，以上所列小米对于 Apache Doris 的优化实践，已经有部分功能已经在 Apache Doris 1.0 或 1.1 版本中发布，有部分 PR 已经合入社区 Master，在不久后发布的 1.2 新版本中应该就会与大家见面。随着社区的快速发展，有越来越多小伙伴参与到社区建设中，社区活跃度有了极大的提升。Apache Doris 已经变得越来越成熟，并开始从单一计算存储一体的分析型 MPP 数据库走向湖仓一体的道路，相信在未来还有更多的数据分析场景等待去探索和实现。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/jd">Apache Doris 在京东客服 OLAP 中的应用实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1><strong>引言</strong></h1><p>Apache Doris 是一款开源的 MPP 分析型数据库产品，不仅能够在亚秒级响应时间即可获得查询结果，有效的支持实时数据分析，而且支持 10PB 以上的超大的数据集。相较于其他业界比较火的 OLAP 数据库系统，Doris 的分布式架构非常简洁，支持弹性伸缩，易于运维，节省大量人力和时间成本。目前国内社区火热，也有美团、小米等大厂在使用。</p><p>本文主要讨论京东客服在人工咨询、客户事件单、售后服务单等专题的实时大屏，在实时和离线数据多维分析方面，如何利用 Doris 进行业务探索与实践。近些年来，随着数据量爆炸式的增长，以及海量数据联机分析需求的出现，MySQL、Oracle 等传统的关系型数据库在大数据量下遇到瓶颈，而 Hive、Kylin 等数据库缺乏时效性。于是 Apache Doris、Apache Druid、ClickHouse 等实时分析型数据库开始出现，不仅可以应对海量数据的秒级查询，更满足实时、准实时的分析需求。离线、实时计算引擎百花齐放。但是针对不同的场景，面临不同的问题，没有哪一种引擎是万能的。我们希望通过本文，对京东客服业务在离线与实时分析的应用与实践，能够给到大家一些启发，也希望大家多多交流，给我们提出宝贵的建议。</p><h1><strong>京东客服业务形态</strong></h1><p>京东客服作为集团服务的入口，为用户和商家提供了高效、可靠的保障。京东客服肩负着及时解决用户问题的重任，给用户提供详细易懂的说明与解释；为更好的了解用户的反馈以及产品的状况，需要实时的监控咨询量、接起率、投诉量等一系列指标，通过环比和同比，及时发现存在的问题，以更好的适应用户的购物方式，提高服务质量与效率，进而提高京东品牌的影响力。</p><h1><strong>Easy OLAP 设计</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-easyolap-doris-数据导入链路"><strong>01 EasyOLAP Doris 数据导入链路</strong><a class="hash-link" href="#01-easyolap-doris-数据导入链路" title="标题的直接链接">​</a></h3><p>EasyOLAP Doris 数据源主要是实时 Kafka 和离线 HDFS 文件。实时数据的导入依赖于 Routine Load 的方式；离线数据主要使用 Broker Load 和 Stream Load 的方式导入。</p><p><img loading="lazy" alt="EasyOLAP Doris 数据导入链路" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/jd03-00bd471f0fab2d98798f5e3148b35fce.png" width="1080" height="604" class="img_ev3q"></p><p>EasyOLAP Doris 数据导入链路</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-easyolap-doris-全链路监控"><strong>02 EasyOLAP Doris 全链路监控</strong><a class="hash-link" href="#02-easyolap-doris-全链路监控" title="标题的直接链接">​</a></h3><p>目前 EasyOLAP Doris 项目的监控，使用的是 Prometheus + Grafana 框架。其中 node_exporter 负责采集机器层面的指标，Doris 也会自动以 Prometheus 格式吐出 FE、BE 的服务层面的指标。另外，部署了 OLAP Exporter 服务用于采集 Routine Load 相关的指标，旨在第一时间发现实时数据流导入的情况，确保实时数据的时效性。</p><p><img loading="lazy" alt="EasyOLAP Doris monitoring link" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/jd04-8770adfb04ffe977f931d9eaff4cb534.png" width="1080" height="594" class="img_ev3q"></p><p>EasyOLAP Doris 监控链路</p><p><img loading="lazy" alt="640" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/jd01-47257e8bb0b14785f854db959cdfd931.png" width="871" height="600" class="img_ev3q"></p><p>EasyOLAP Doris 监控面板展示</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-easyolap-doris-主备双流设计"><strong>03 EasyOLAP Doris 主备双流设计</strong><a class="hash-link" href="#03-easyolap-doris-主备双流设计" title="标题的直接链接">​</a></h3><p>EasyOLAP Doris 为了保障 0 级业务在大促期间服务的稳定性，采取了主备集群双写的方式。当其中一个集群出现抖动或者数据存在延迟的情况，用户可以自主地快速切换到另一个集群，尽可能的减少集群抖动给业务带来的影响。</p><p><img loading="lazy" alt="03 EasyOLAP Doris Primary-Secondary Dual Stream Design" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/jd02-a6a4279c0c33a25862e89b56e7c986a7.png" width="1080" height="669" class="img_ev3q"></p><p>EasyOLAP Doris 主备双流设计</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-easyolap-doris-动态分区管理"><strong>04 EasyOLAP Doris 动态分区管理</strong><a class="hash-link" href="#04-easyolap-doris-动态分区管理" title="标题的直接链接">​</a></h3><p>京东 OLAP 团队分析需求之后，对 Doris 做了一定的定制化开发，其中就涉及到动态分区管理功能。尽管社区版本已经拥有动态分区的功能，但是该功能无法保留指定时间的分区。针对京东集团的特点，我们对指定时间的历史数据进行了留存，比如 618 和 11.11 期间的数据，不会因为动态分区而被删除。</p><p>动态分区管理功能能够控制集群中存储的数据量，而且方便了业务方的使用，无需手动或使用额外代码来管理分区信息。</p><h1><strong>Doris 缓存机制</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-需求场景"><strong>01 需求场景</strong><a class="hash-link" href="#01-需求场景" title="标题的直接链接">​</a></h3><p>致力于不断提升用户体验，京东客服的数据分析追求极致的时效性。离线数据分析场景是写少读多，数据写入一次，多次频繁读取；实时数据分析场景，一部分数据是不更新的历史分区，一部分数据是处于更新的分区。在大部分的分析应用中，存在下述几种场景：</p><ul><li>高并发场景：Doris 较好的支持高并发，但是过高的 QPS 会引起集群抖动，且单个节点无法承载太高的 QPS ；</li><li>复杂查询：京东客服实时运营平台监控根据业务场景需展示多维复杂指标，丰富指标展示对应多种不同的查询，且数据源来自于多张表，虽然单个查询的响应时间在毫秒级别，但是整体的响应时间可能会到秒级别；</li><li>重复查询：如果没有防重刷机制，由于延迟或手误，重复刷新页面会导致提交大量重复的查询；</li></ul><p>针对上述场景，在应用层有解决方案——将查询结果放入到 Redis 中，缓存会周期性的刷新或者由用户手动刷新，但是也会存在一些问题：</p><ul><li>数据不一致：无法立即对数据的更新作出响应，用户接收到的结果可能是旧数据；</li><li>命中率低：如果数据实时性强，缓存频繁失效，则缓存的命中率低且系统的负载无法得缓解；</li><li>额外成本：引入外部组件，增加系统复杂度，增加额外成本。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-缓存机制简介"><strong>02 缓存机制简介</strong><a class="hash-link" href="#02-缓存机制简介" title="标题的直接链接">​</a></h3><p>在 EasyOLAP Doris 中，一共有三种不同类型 Cache。根据适用场景的不同，分别为 Result Cache、SQL Cache 和 Partition Cache 。三种缓存都可以通过 MySQL 客户端指令控制开关。</p><p>这三种缓存机制是可以共存的，即可以同时开启。查询时，查询分析器首先会判断是否开启了 Result Cache ，在 Result Cache 开启的情况下先从 Result Cache 中查找该查询是否存在缓存，如果存在缓存，直接取缓存的值返回给客户端；如果缓存失效或者不存在，则直接进行查询并将结果写入到缓存。缓存放在各个 FE 节点的内存中，以便快速读取。</p><p>SQL Cache 按照 SQL 的签名、查询的表的分区的 ID 和分区最新版本号来存储和获取缓存。这三者一起作为缓存的条件，其中一者发生变化，如 SQL 语句变化、数据更新之后分区版本号变化，都会无法命中缓存。在多表 Join 的情况下，其中一张表的分区更新，也会导致无法命中缓存。SQL Cache 更适合 T+1 更新的场景。</p><p>Partition Cache 是更细粒度的缓存机制。Partition Cache 主要是将一个查询根据分区并行拆分，拆分为只读分区和可更新分区，只读分区缓存，更新分区不缓存，相应的结果集也会生成 n 个，然后再将各个拆分后的子查询的结果合并。因此，如果查询 N 天的数据，数据更新最近的 D 天，每天只是日期范围不一样但相似的查询，就可以利用 Partition Cache ，只需要查询 D 个分区即可，其他部分都来自缓存，可以有效降低集群负载，缩短查询响应时间。</p><p>一个查询进入到 Doris，系统先会处理查询语句，并将该查询语句作为 Key，在执行查询语句之前，查询分析器能够自动选择最适合的缓存机制，以确保在最优的情况下，利用缓存机制来缩短查询相应时间。然后检查 Cache 中是否存在该查询结果，如果存在就获取缓存中的数据返回给客户端；如果没有缓存，则正常查询，并将该查询结果以 Value 的形式和该查询语句 Key 存储到缓存中。Result Cache 可以在高并发场景下发挥其作用，也可以保护集群资源不受重复的大查询的侵占。SQL Cache 更加适合 T+1 的场景，在分区更新不频繁以及 SQL 语句重复的情况下，效果很好。Partition Cache 是粒度最小的缓存。在查询语句查询一个时间段的数据时，查询语句会被拆分成多个子查询。在数据只写一个分区或者部分分区的情况下，能够缩短查询时间，节省集群资源。</p><p>为了更好的观察缓存的效果，相关指标已经加入到 Doris 的服务指标中，通过 Prometheus 和 Grafana 监控系统获取直观的监控数据。指标有不同种类的 Cache 的命中数量、不同种类的 Cache 命中率、 Cache 的内存大小等指标。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-缓存机制效果"><strong>03 缓存机制效果</strong><a class="hash-link" href="#03-缓存机制效果" title="标题的直接链接">​</a></h3><p>京东客服 Doris 主集群，11.11 期间在没有开启缓存时，部分业务就导致 CPU 的使用率达到 100% ；在开启 Result Cache 的情况下，CPU 使用率在 30%-40% 之间。缓存机制确保业务在高并发场景下，能够快速的得到查询结果，并很好的保护了集群资源。</p><h1><strong>Doris 在 2020 年 11.11 大促期间的优化</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-导入任务优化"><strong>01 导入任务优化</strong><a class="hash-link" href="#01-导入任务优化" title="标题的直接链接">​</a></h3><p>实时数据的导入一直是一个挑战。其中，保证数据实时性和导入稳定性是最重要的。为了能够更加直观的观察实时数据导入的情况，京东 OLAP 团队自主开发了 OLAP Exporter ，用于采集实时数据导入相关的指标，如导入速度、导入积压和暂停的任务等。通过导入速度和导入积压，可以判断一个实时导入任务的状态，如发现任务有积压的趋势，可以使用自主开发的采样工具，对实时任务进行采样分析。实时任务主要有三个阈值来控制任务的提交，分别是每批次最大处理时间间隔、每批次最大处理条数和每批次最大处理数据量，一个任务只要达到其中一个阈值，该任务就会被提交。通过增加日志，发现 FE 中的任务队列比较繁忙，所以，参数的调整主要都是将每批次最大处理条数和每批次最大处理数据量调大，然后根据业务的需求，调整每批次最大处理时间间隔，以保证数据的延迟在每批次最大处理时间间隔的两倍之内。通过采样工具，分析任务，不仅保证了数据的实时性，也保证了导入的稳定性。另外，我们也设置了告警，可以及时发现实时导入任务的积压以及导入任务的暂停等异常情况。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-监控指标优化"><strong>02 监控指标优化</strong><a class="hash-link" href="#02-监控指标优化" title="标题的直接链接">​</a></h3><p>监控指标主要分为两个部分，一个是机器层面指标部分，一个是业务层面指标部分。在整个监控面板里，详细的指标带来了全面的数据的同时，也增加了获取重要指标的难度。所以，为了更好的观察所有集群的重要指标，单独设立一个板块—— 11.11 重要指标汇总板块。板块中有 BE CPU 使用率、实时任务消费积压行数、TP99、QPS 等指标。指标数量不多，但是可以观测到所有集群的情况，这样可以免去在监控中频繁切换的麻烦。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-周边工具支持"><strong>03 周边工具支持</strong><a class="hash-link" href="#03-周边工具支持" title="标题的直接链接">​</a></h3><p>除了上述说到的采样工具和 OLAP Exporter ，京东 OLAP 团队还开发了一系列的 Doris 维护工具。</p><ol><li><p>导入采样工具：导入采样工具不仅可以采集实时导入的数据，而且还支持调整实时导入任务的参数，或者在实时导入任务暂停状态下，生成创建语句（包括最新的位点等信息）用于任务的迁移等操作。</p></li><li><p>大查询工具：大查询不仅会造成集群 BE CPU 使用率的抖动，还会导致其他查询响应时间变长。在有大查询工具之前，发现集群 CPU 出现抖动，需要去检查所有 FE 上的审计日志，然后再做统计，不仅浪费时间，而且不够直观。大查询工具就是为了解决上述的问题。当监控侧发现集群有抖动，就可以使用大查询工具，输入集群名和时间点，就可以得到该时间点下，不同业务的查询总数，时间超过 5 秒、 10 秒、 20 秒的查询个数，扫描量巨大的查询个数等，方便我们从不同的维度分析大查询。大查询的详细情况也将被保存在中间文件中，可以直接获取不同业务的大查询。整个过程只需要几十秒到一分钟就可以定位到正在发生的大查询并获取相应的查询语句，大大节约了时间和运维成本。</p></li><li><p>降级与恢复工具：为了确保 11.11 大促期间， 0 级业务的稳定性，在集群压力超过安全位的时候，需要对其他非 0 级业务做降级处理，待度过高峰期后，再一键恢复到降级前的设置。降级主要是降低业务的最大连接数、暂停非 0 级的实时导入任务等。这大大增加了操作的便捷性，提高了效率。</p></li><li><p>集群巡检工具：在 11.11 期间，集群的健康巡检是极其重要的。常规巡检包括双流业务的主备集群一致性检查，为了确保业务在一个集群出现问题的时候可以快速切换到另一个集群，就需要保证两个集群上的库表一致、数据量差异不大等；检查库表的副本数是否为 3 且检查集群是否存在不健康的 Tablet ；检查机器磁盘使用率、内存等机器层面的指标等。</p></li></ol><h1><strong>总结与展望</strong></h1><p>京东客服是在 2020 年年初开始引入 Doris 的，目前拥有一个独立集群，一个共享集群，是京东 OLAP 的资深用户。</p><p>在业务使用中也遇到了例如任务调度相关的、导入任务配置相关的和查询相关等问题，这也在推动京东 OLAP 团队更深入的了解 Doris 。我们计划推广使用物化视图来进一步提升查询的效率；使用 Bitmap 来支持 UV 等指标的精确去重操作；使用审计日志，更方便的统计大查询、慢查询；解决实时导入任务的调度问题，使导入任务更加高效稳定。除此之外，我们也计划优化建表、创建优质 Rollup 或物化视图以提升应用的流畅性，加速更多业务向 OLAP 平台靠拢，以提升应用的影响力。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/meituan">Apache Doris 在美团外卖实时数仓建设中的实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong>本文主要介绍一种通用的实时数仓构建的方法与实践。实时数仓以端到端低延迟、SQL 标准化、快速响应变化、数据统一为目标。在实践中，我们总结的最佳实践是：一个通用的实时生产平台 + 一个通用交互式实时分析引擎相互配合同时满足实时和准实时业务场景。两者合理分工，互相补充，形成易于开发、易于维护、效率最高的流水线，兼顾开发效率与生产成本，以较好的投入产出比满足业务多样需求。</p><h1><strong>实时场景</strong></h1><p>实时数据在美团外卖的场景是非常多的，主要有以下几点：</p><ul><li><p>运营层面：比如实时业务变化，实时营销效果，当日营业情况以及当日实时业务趋势分析等。</p></li><li><p>生产层面：比如实时系统是否可靠，系统是否稳定，实时监控系统的健康状况等。</p></li><li><p>C 端用户：比如搜索推荐排序，需要实时了解用户的想法，行为、特点，给用户推荐更加关注的内容。</p></li><li><p>风控侧：在外卖以及金融科技用的是非常多的，实时风险识别，反欺诈，异常交易等，都是大量应用实时数据的场景</p></li></ul><h1><strong>实时技术及架构</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1实时计算技术选型"><strong>1.实时计算技术选型</strong><a class="hash-link" href="#1实时计算技术选型" title="标题的直接链接">​</a></h3><p>目前开源的实时技术比较多，比较通用的是 Storm、Spark Streaming 以及 Flink，具体要根据不同公司的业务情况进行选型。</p><p>美团外卖是依托美团整体的基础数据体系建设，从技术成熟度来讲，前几年用的是 Storm，Storm 当时在性能稳定性、可靠性以及扩展性上是无可替代的，随着 Flink 越来越成熟，从技术性能上以及框架设计优势上已经超越 Storm，从趋势来讲就像 Spark 替代 MR 一样，Storm 也会慢慢被 Flink 替代，当然从 Storm 迁移到 Flink 会有一个过程，我们目前有一些老的任务仍然在 Storm 上，也在不断推进任务迁移。</p><p>具体 Storm 和 Flink 的对比可以参考上图表格。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2实时架构">2.<strong>实时架构</strong><a class="hash-link" href="#2实时架构" title="标题的直接链接">​</a></h3><p><strong>① Lambda 架构</strong></p><p>Lambda 架构是比较经典的架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。Lambda 架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。这在业务应用中也是顺理成章采用的一种方式。</p><p>双路生产会存在一些问题，比如加工逻辑 double，开发运维也会 double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个 Kappa 架构。</p><p><strong>② Kappa 架构</strong></p><p>Kappa 架构从架构设计来讲比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用 Kappa 架构生产落地的案例不多见，且场景比较单一。这些问题在我们这边同样会遇到，我们也会有自己的一些思考，在后面会讲到。</p><h1><strong>业务痛点</strong></h1><p>在外卖业务上，我们也遇到了一些问题。</p><p>业务早期，为了满足业务需要，一般是拿到需求后 case by case 的先把需求完成，业务对于实时性要求是很高的，从时效性来说，没有进行中间层沉淀的机会，在这种场景下，一般是拿到业务逻辑直接嵌入，这是能想到的简单有效的方法，在业务发展初期这种开发模式比较常见。</p><p>如上图所示，拿到数据源后，会经过数据清洗，扩维，通过 Storm 或 Flink 进行业务逻辑处理，最后直接进行业务输出。把这个环节拆开来看，数据源端会重复引用相同的数据源，后面进行清洗、过滤、扩维等操作，都要重复做一遍，唯一不同的是业务的代码逻辑是不一样的，如果业务较少，这种模式还可以接受，但当后续业务量上去后，会出现谁开发谁运维的情况，维护工作量会越来越大，作业无法形成统一管理。而且所有人都在申请资源，导致资源成本急速膨胀，资源不能集约有效利用，因此要思考如何从整体来进行实时数据的建设。</p><h1><strong>数据特点与应用场景</strong></h1><p>那么如何来构建实时数仓呢？</p><p>首先要进行拆解，有哪些数据，有哪些场景，这些场景有哪些共同特点，对于外卖场景来说一共有两大类，日志类和业务类。</p><ul><li><p>日志类：数据量特别大，半结构化，嵌套比较深。日志类的数据有个很大的特点，日志流一旦形成是不会变的，通过埋点的方式收集平台所有的日志，统一进行采集分发，就像一颗树，树根非常大，推到前端应用的时候，相当于从树根到树枝分叉的过程（从 1 到 n 的分解过程），如果所有的业务都从根上找数据，看起来路径最短，但包袱太重，数据检索效率低。日志类数据一般用于生产监控和用户行为分析，时效性要求比较高，时间窗口一般是 5min 或 10min 或截止到当前的一个状态，主要的应用是实时大屏和实时特征，例如用户每一次点击行为都能够立刻感知到等需求。</p></li><li><p>业务类：主要是业务交易数据，业务系统一般是自成体系的，以 Binlog 日志的形式往下分发，业务系统都是事务型的，主要采用范式建模方式，特点是结构化的，主体非常清晰，但数据表较多，需要多表关联才能表达完整业务，因此是一个 n 到 1 的集成加工过程。</p></li></ul><p>业务类实时处理面临的几个难点：</p><ul><li><p>业务的多状态性：业务过程从开始到结束是不断变化的，比如从下单-&gt;支付-&gt;配送，业务库是在原始基础上进行变更的，Binlog 会产生很多变化的日志。而业务分析更加关注最终状态，由此产生数据回撤计算的问题，例如 10 点下单，13 点取消，但希望在 10 点减掉取消单。</p></li><li><p>业务集成：业务分析数据一般无法通过单一主体表达，往往是很多表进行关联，才能得到想要的信息，在实时流中进行数据的合流对齐，往往需要较大的缓存处理且复杂。</p></li><li><p>分析是批量的，处理过程是流式的：对单一数据，无法形成分析，因此分析对象一定是批量的，而数据加工是逐条的。</p></li></ul><p>日志类和业务类的场景一般是同时存在的，交织在一起，无论是 Lambda 架构还是 Kappa 架构，单一的应用都会有一些问题。因此针对场景来选择架构与实践才更有意义。</p><h1><strong>实时</strong>数仓架构设计</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-实时架构流批结合的探索"><strong>1. 实时架构：流批结合的探索</strong><a class="hash-link" href="#1-实时架构流批结合的探索" title="标题的直接链接">​</a></h3><p>基于以上问题，我们有自己的思考。通过流批结合的方式来应对不同的业务场景。</p><p>如上图所示，数据从日志统一采集到消息队列，再到数据流的 ETL 过程，作为基础数据流的建设是统一的。之后对于日志类实时特征，实时大屏类应用走实时流计算。对于 Binlog 类业务分析走实时 OLAP 批处理。</p><p>流式处理分析业务的痛点？对于范式业务，Storm 和 Flink 都需要很大的外存，来实现数据流之间的业务对齐，需要大量的计算资源。且由于外存的限制，必须进行窗口的限定策略，最终可能放弃一些数据。计算之后，一般是存到 Redis 里做查询支撑，且 KV 存储在应对分析类查询场景中也有较多局限。</p><p>实时 OLAP 怎么实现？有没有一种自带存储的实时计算引擎，当实时数据来了之后，可以灵活的在一定范围内自由计算，并且有一定的数据承载能力，同时支持分析查询响应呢？随着技术的发展，目前 MPP 引擎发展非常迅速，性能也在飞快提升，所以在这种场景下就有了一种新的可能。这里我们使用的是 Doris 引擎。</p><p>这种想法在业内也已经有实践，且成为一个重要探索方向。阿里基于 ADB 的实时 OLAP 方案等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-实时数仓架构设计"><strong>2. 实时数仓架构设计</strong><a class="hash-link" href="#2-实时数仓架构设计" title="标题的直接链接">​</a></h3><p>从整个实时数仓架构来看，首先考虑的是如何管理所有的实时数据，资源如何有效整合，数据如何进行建设。</p><p>从方法论来讲，实时和离线是非常相似的，离线数仓早期的时候也是 case by case，当数据规模涨到一定量的时候才会考虑如何治理。分层是一种非常有效的数据治理方式，所以在实时数仓如何进行管理的问题上，首先考虑的也是分层的处理逻辑，具体如下：</p><ul><li><p>数据源：在数据源的层面，离线和实时在数据源是一致的，主要分为日志类和业务类，日志类又包括用户日志，DB 日志以及服务器日志等。</p></li><li><p>实时明细层：在明细层，为了解决重复建设的问题，要进行统一构建，利用离线数仓的模式，建设统一的基础明细数据层，按照主题进行管理，明细层的目的是给下游提供直接可用的数据，因此要对基础层进行统一的加工，比如清洗、过滤、扩维等。</p></li><li><p>汇总层：汇总层通过 Flink 或 Storm 的简洁算子直接可以算出结果，并且形成汇总指标池，所有的指标都统一在汇总层加工，所有人按照统一的规范管理建设，形成可复用的汇总结果。</p></li></ul><p>总结起来，从整个实时数仓的建设角度来讲，首先数据建设的层次化要先建出来，先搭框架，然后定规范，每一层加工到什么程度，每一层用什么样的方式，当规范定义出来后，便于在生产上进行标准化的加工。由于要保证时效性，设计的时候，层次不能太多，对于实时性要求比较高的场景，基本可以走上图左侧的数据流，对于批量处理的需求，可以从实时明细层导入到实时 OLAP 引擎里，基于 OLAP 引擎自身的计算和查询能力进行快速的回撤计算，如上图右侧的数据流。</p><h1><strong>实时平台化建设</strong></h1><p>架构确定之后，后面考虑的是如何进行平台化的建设，实时平台化建设完全附加于实时数仓管理之上进行的。</p><p>首先进行功能的抽象，把功能抽象成组件，这样就可以达到标准化的生产，系统化的保障就可以更深入的建设，对于基础加工层的清洗、过滤、合流、扩维、转换、加密、筛选等功能都可以抽象出来，基础层通过这种组件化的方式构建直接可用的数据结果流。这其中会有一个问题，用户的需求多样，满足了这个用户，如何兼容其他的用户，因此可能会出现冗余加工的情况，从存储来讲，实时数据不存历史，不会消耗过多的存储，这种冗余是可以接受的，通过冗余的方式可以提高生产效率，是一种空间换时间的思想应用。</p><p>通过基础层的加工，数据全部沉淀到 IDL 层，同时写到 OLAP 引擎的基础层，再往上是实时汇总层计算，基于 Storm、Flink 或 Doris，生产多维度的汇总指标，形成统一的汇总层，进行统一的存储分发。</p><p>当这些功能都有了以后，元数据管理，指标管理，数据安全性、SLA、数据质量等系统能力也会逐渐构建起来。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1实时基础层功能">1.实时基础层功能<a class="hash-link" href="#1实时基础层功能" title="标题的直接链接">​</a></h3><p>实时基础层的建设要解决一些问题。</p><p>首先是一条流重复读的问题，一条 Binlog 打过来，是以 DB 包的形式存在的，用户可能只用其中一张表，如果大家都要用，可能存在所有人都要接这个流的问题。解决方案是可以按照不同的业务解构出来，还原到基础数据流层，根据业务的需要做成范式结构，按照数仓的建模方式进行集成化的主题建设。</p><p>其次要进行组件的封装，比如基础层的清洗、过滤、扩维等功能，通过一个很简单的表达入口，让用户将逻辑写出来。trans 环节是比较灵活的，比如从一个值转换成另外一个值，对于这种自定义逻辑表达，我们也开放了自定义组件，可以通过 Java 或 Python 开发自定义脚本，进行数据加工。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2实时特征生产功能">2.<strong>实时特征生产功能</strong><a class="hash-link" href="#2实时特征生产功能" title="标题的直接链接">​</a></h3><p>特征生产可以通过 SQL 语法进行逻辑表达，底层进行逻辑的适配，透传到计算引擎，屏蔽用户对计算引擎的依赖。就像对于离线场景，目前大公司很少通过代码的方式开发，除非一些特别的 case，所以基本上可以通过 SQL 化的方式表达。</p><p>在功能层面，把指标管理的思想融合进去，原子指标、派生指标，标准计算口径，维度选择，窗口设置等操作都可以通过配置化的方式，这样可以统一解析生产逻辑，进行统一封装。</p><p>还有一个问题，同一个源，写了很多 SQL，每一次提交都会起一个数据流，比较浪费资源，我们的解决方案是，通过同一条流实现动态指标的生产，在不停服务的情况下可以动态添加指标。</p><p>所以在实时平台建设过程中，更多考虑的是如何更有效的利用资源，在哪些环节更能节约化的使用资源，这是在工程方面更多考虑的事情。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3sla-建设">3.SLA 建设<a class="hash-link" href="#3sla-建设" title="标题的直接链接">​</a></h3><p>SLA 主要解决两个问题，一个是端到端的 SLA，一个是作业生产效率的 SLA，我们采用埋点+上报的方式，由于实时流比较大，埋点要尽量简单，不能埋太多的东西，能表达业务即可，每个作业的输出统一上报到 SLA 监控平台，通过统一接口的形式，在每一个作业点上报所需要的信息，最后能够统计到端到端的 SLA。</p><p>在实时生产中，由于链路非常长，无法控制所有链路，但是可以控制自己作业的效率，所以作业 SLA 也是必不可少的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-实时-olap-方案">4. 实时 OLAP 方案<a class="hash-link" href="#4-实时-olap-方案" title="标题的直接链接">​</a></h3><p>问题：</p><ul><li><p>Binlog 业务还原复杂：业务变化很多，需要某个时间点的变化，因此需要进行排序，并且数据要存起来，这对于内存和 CPU 的资源消耗都是非常大的。</p></li><li><p>Binlog 业务关联复杂：流式计算里，流和流之间的关联，对于业务逻辑的表达是非常困难的。</p></li></ul><p>解决方案：</p><p>通过带计算能力的 OLAP 引擎来解决，不需要把一个流进行逻辑化映射，只需要解决数据实时稳定的入库问题。</p><p>我们这边采用的是 Doris 作为高性能的 OLAP 引擎，由于业务数据产生的结果和结果之间还需要进行衍生计算，Doris 可以利用 unique 模型或聚合模型快速还原业务，还原业务的同时还可以进行汇总层的聚合，也是为了复用而设计。应用层可以是物理的，也可以是逻辑化视图。</p><p>这种模式重在解决业务回撤计算，比如业务状态改变，需要在历史的某个点将值变更，这种场景用流计算的成本非常大，OLAP 模式可以很好的解决这个问题。</p><h1>实时应用案例</h1><p>最后通过一个案例说明，比如商家要根据用户历史下单数给用户优惠，商家需要看到历史下了多少单，历史 T+1 的数据要有，今天实时的数据也要有，这种场景是典型的 Lambda 架构，可以在 Doris 里设计一个分区表，一个是历史分区，一个是今日分区，历史分区可以通过离线的方式生产，今日指标可以通过实时的方式计算，写到今日分区里，查询的时候进行一个简单的汇总。</p><p>这种场景看起来比较简单，难点在于商家的量上来之后，很多简单的问题都会变的复杂，因此后面我们也会通过更多的业务输入，沉淀出更多的业务场景，抽象出来形成统一的生产方案和功能，以最小化的实时计算资源支撑多样化的业务需求，这也是未来需要达到的目的。</p><p>今天的分享就到这里，谢谢大家。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="嘉宾介绍">嘉宾介绍：<a class="hash-link" href="#嘉宾介绍" title="标题的直接链接">​</a></h3><p>朱良，5 年以上传统行业数仓建设经验，6 年互联网数仓经验，技术方向涉及离线，实时数仓治理，系统化能力建设，OLAP 系统及引擎，大数据相关技术，重点跟进 OLAP，实时技术前沿发展趋势。业务方向涉及即席查询，运营分析，策略报告产品，用户画像，人群推荐，实验评估等。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/scenario">打造自助对话式数据分析场景，Apache Doris 在思必驰的应用实践｜最佳实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">赵伟</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>作者：赵伟，思必驰大数据高级研发，10年大数据开发和设计经验，负责大数据平台基础技术和OLAP分析技术开发。社区贡献：Doris-spark-connector 的实时读写和优化。</p></blockquote><h1>业务背景</h1><p>思必驰是国内专业的对话式人工智能平台公司，拥有全链路的智能语音语言技术，致力于成为全链路智能语音及语言交互的平台型企业，自主研发了新一代人机交互平台 DUI 和人工智能芯片 TH1520，为车联网、IoT 及政务、金融等众多行业场景合作伙伴提供自然语言交互解决方案。</p><p>思必驰于 2019 年首次引入 Apache Doris ，基于 Apache Doris 构建了实时与离线一体的数仓架构。相对于过去架构，Apache Doris 凭借其灵活的查询模型、极低的运维成本、短平快的开发链路以及优秀的查询性能等诸多方面优势，如今已经在实时业务运营、自助/对话式分析等多个业务场景得到运用，满足了 设备画像/用户标签、业务场景实时运营、数据分析看板、自助 BI、财务对账等多种数据分析需求。在这一过程中我们也积累了诸多使用上的经验，在此分享给大家。</p><h1>架构演进</h1><p>早期业务中离线数据分析是我们的主要需求，近几年，随着业务的不断发展，业务场景对实时数据分析的要求也越来越高，早期数仓架构逐渐力不从心，暴露出很多问题。为了满足业务场景对查询性能、响应时间及并发能力更高的要求，2019年正式引入 Apache Doris 构建实时离线一体的数仓架构。</p><p>以下将为大家介绍思必驰数仓架构的演进之路，早期数仓存在的优缺点，同时分享我们选择 Apache Doris 构建新架构的原因以及面临的新问题与挑战。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="早期数仓架构及痛点">早期数仓架构及痛点<a class="hash-link" href="#早期数仓架构及痛点" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="data_wharehouse_architecture_v1_0_git" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/data_wharehouse_architecture_v1_0_git-006b22817872b04ad8f909e54e8c1411.png" width="1953" height="1106" class="img_ev3q"></p><p>如上图所示，早期架构基于 Hive +Kylin 来构建离线数仓，实时数仓架基于 Spark+MySQL 来构建实时分析数仓。</p><p>我们业务场景的数据源主要分为三类，业务数据库如 MySQL，应用系统如 K8s 容器服务日志，还有车机设备终端的日志。数据源通过 MQTT/HTTP 协议、业务数据库 Binlog 、Filebeat日志采集等多种方式先写入 Kafka 。在早期架构中，数据经 Kafka 后将分为实时和离线两条链路，首先是实时部分，实时部分链路较短，经过 Kafka 缓冲完的数据通过 Spark 计算后放入 MySQL 中进行分析，对于早期的实时分析需求，MySQL 基本可以满足分析需求。而离线部分则由 Spark 进行数据清洗及计算后在 Hive 中构建离线数仓，并使用 Apache Kylin 构建 Cube，在构建 Cube 之前需要提前做好数据模型的的设计，包括关联表、维度表、指标字段、指标需要的聚合函数等，通过调度系统进行定时触发构建，最终使用 HBase 存储构建好的 Cube。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="早期架构的优势"><strong>早期架构的优势：</strong><a class="hash-link" href="#早期架构的优势" title="标题的直接链接">​</a></h3><ol><li><p>早期架构与 Hive 结合较好，无缝对接 Hadoop 技术体系。</p></li><li><p>离线数仓中基于 Kylin 的预计算、表关联、聚合计算、精确去重等场景，查询性能较高，在并发场景下查询稳定性也较高。</p></li></ol><p>早期架构解决了当时业务中较为紧迫的查询性能问题，但随着业务的发展，对数据分析要求不断升高，早期架构缺点也开始逐渐凸显出来。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="早期架构的痛点"><strong>早期架构的痛点：</strong><a class="hash-link" href="#早期架构的痛点" title="标题的直接链接">​</a></h3><ol><li><p>依赖组件多。Kylin 在 2.x、3.x 版本中强依赖 Hadoop 和 HBase ，应用组件较多导致开发链路较长，架构稳定性隐患多，维护成本比很高。</p></li><li><p>Kylin 的构建过程复杂，构建任务容易失败。Kylin 构建需要进行打宽表、去重列、生成字典，构建 Cube 等如果每天有 1000-2000 个甚至更多的任务，其中至少会有 10 个甚至更多任务构建失败，导致需要大量时间去写自动运维脚本。</p></li><li><p>维度/字典膨胀严重。维度膨胀指的是在某些业务场景中需要多个分析条件和字段，如果在数据分析模型中选择了很多字段而没有进行剪枝，则会导致 Cube 维度膨胀严重，构建时间变长。而字典膨胀指的是在某些场景中需要长时间做全局精确去重，会使得字典构建越来越大，构建时间也会越来越长，从而导致数据分析性能持续下降。</p></li><li><p>数据分析模型固定，灵活性较低。在实际应用过程中，如果对计算字段或者业务场景进行变更，则要回溯部分甚至全部数据。</p></li><li><p>不支持数据明细查询。早期数仓架构是无法提供明细数据查询的，Kylin 官方给的解决方法是下推给 Presto 做明细查询，这又引入了新的架构，增加了开发和运维成本。</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="架构选型">架构选型<a class="hash-link" href="#架构选型" title="标题的直接链接">​</a></h2><p>为解决以上问题，我们开始探索新的数仓架构优化方案，先后对市面上应用最为广泛的 Apache Doris、Clickhouse 等 OLAP 引擎进行选型调研。相较于 ClickHouse 的繁重运维、各种各样的表类型、不支持关联查询等，结合我们的 OLAP 分析场景中的需求，综合考虑，Apache Doris 表现较为优秀，最终决定引入 Apache Doris 。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="新数仓架构">新数仓架构<a class="hash-link" href="#新数仓架构" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="data_wharehouse_architecture_v2_0_git" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/data_wharehouse_architecture_v2_0_git-825df043f0abf0fda4a92b8dc5d10956.png" width="1993" height="1144" class="img_ev3q"></p><p>如上图所示，我们基于 Apache Doris 构建了实时+离线一体的新数仓架构，与早期架构不同的是，实时和离线的数据分别进行处理后均写入 Apache Doris 中进行分析。</p><p>因历史原因数据迁移难度较大，离线部分基本和早期数仓架构保持一致，在Hive上构建离线数仓，当然完全可以在Apache Doris 上直接构建离线数仓。</p><p>相对早期架构不同的是，离线数据通过 Spark 进行清洗计算后在 Hive 中构建数仓，然后通过 Broker Load 将存储在 Hive 中的数据写入到 Apache Doris 中。这里要说明的， Broker Load 数据导入速度很快，天级别 100-200G 数据导入到 Apache Doris 中仅需要 10-20 分钟。</p><p>实时数据流部分，新架构使用了 Doris-Spark-Connector 来消费 Kafka 中的数据并经过简单计算后写入 Apache Doris 。从架构图所示，实时和离线数据统一在 Apache Doris 进行分析处理，满足了数据应用的业务需求，实现了实时+离线一体的数仓架构。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新架构的收益"><strong>新架构的收益：</strong><a class="hash-link" href="#新架构的收益" title="标题的直接链接">​</a></h3><ol><li><p>极简运维，维护成本低，不依赖 Hadoop 生态组件。Apache Doris 的部署简单，只有 FE 和 BE 两个进程， FE 和 BE 进程都是可以横向扩展的，单集群支持到数百台机器，数十 PB 的存储容量，并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。在使用 Doris 三年时间中花费的运维时间非常少，相比于基于 Kylin 搭建的早期架构，新架构花费极少的时间去做运维。</p></li><li><p>链路短，开发排查问题难度大大降低。基于 Doris 构建实时和离线统一数仓，支持实时数据服务、交互数据分析和离线数据处理场景，这使得开发链路变的很短，问题排查难度大大降低。</p></li><li><p>支持 Runtime 形式的 Join 查询。Runtime 类似 MySQL 的表关联，这对数据分析模型频繁变更的场景非常友好，解决了早期结构数据模型灵活性较低的问题。</p></li><li><p>同时支持 Join、聚合、明细查询。解决了早期架构中部分场景无法查询数据明细的问题。</p></li><li><p>支持多种加速查询方式。支持上卷索引，物化视图，通过上卷索引实现二级索引来加速查询，极大的提升了查询响应时间。</p></li><li><p>支持多种联邦查询方式。支持对 Hive、Iceberg、Hudi 等数据湖和 MySQL、Elasticsearch 等数据库的联邦查询分析。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="问题和挑战"><strong>问题和挑战：</strong><a class="hash-link" href="#问题和挑战" title="标题的直接链接">​</a></h3><p>在建设新数仓架构过程中，我们遇到了一些问题：</p><ul><li><p>高并发场景对 Apache Doris 查询性能存在一定影响。我们分别在 Doris 0.12 和 Doris 1.1版本上进行测试，同一时间同样的 SQL，10 并发和 50 并发进行访问，性能差别较大。</p></li><li><p>在实时写入场景中，当实时写入的数据量比较大时，会使得 IO 比较密集，导致查询性能下降。</p></li><li><p>大数据量下字符串精确去重较慢。目前使用的是 count distinct 函数、Shuffle 和聚合算子去重，此方式算力比较慢。当前业内常见的解决方法一般是针对去重列构建字典，基于字典构建 Bitmap 索引后使用 Bitmap 函数去重。目前 Apache Doris 只支持数字类型的 Bitmap 索引，具有一定的局限性。</p></li></ul><h1>业务场景的应用</h1><p>Apache Doris 在思必驰最先应用在实时运营业务场景以及自助/对话式分析场景，本章节将介绍两个场景的需求及应用情况。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时运营业务场景">实时运营业务场景<a class="hash-link" href="#实时运营业务场景" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="real-time_operation_git" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/real-time_operation_git-87d6e8ede096ba1551cb290941741126.png" width="1977" height="1226" class="img_ev3q"></p><p>首先是实时运营业务场景，如上图所示，实时运营业务场景的技术架构和前文所述的新版数仓架构基本一致：</p><ul><li><p>数据源：数据源新版架构图中一致，包括 MySQL 中的业务数据，应用系统埋点数据以及设备和终端日志。</p></li><li><p>数据导入：离线数据导入使用 Broker Load，实时数据导入使用 Doris-Spark-Connector 。</p></li><li><p>数据存储与开发：几乎所有的实时数仓全部在 Apache Doris 构建，有一部分离线数据放在 Airflow 上执行 DAG 跑批任务。</p></li><li><p>数据应用：最上层是业务侧提出的业务分析需求，包括大屏展示，数据运营的实时看板、用户画像、BI 看板等。</p></li></ul><p><strong>在实时运营业务场景中，数据分析的需求主要有两方面：</strong></p><ul><li><p>由于实时导入数据量比较大，因此对实时数据的查询效率要求较高</p></li><li><p>在此场景中，有 20+ 人的团队在运营，需要同时开数据运营的看板，因此对实时写入的性能和查询并发会有比较高的要求。</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="自助对话式分析场景">自助/对话式分析场景<a class="hash-link" href="#自助对话式分析场景" title="标题的直接链接">​</a></h2><p>除以上之外，Apache Doris 在思必驰第二个应用是自助/对话式分析场景。</p><p><img loading="lazy" alt="ai_chatbots_git" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/ai_chatbots_git-f094d1221b56b522cb93ba3bc766e659.png" width="1953" height="1118" class="img_ev3q"></p><p>如上图所示，在一般的 BI 场景中，用户方比如商务、财务、销售、运营、项目经理等会提出需求给数据分析人员，数据分析人员在 BI 平台上做数据看板，最终把看板提供给用户，用户从 BI 看板上获取所需信息，但是有时候用户想要查看明细数据、定制化的看板需求，或者在某些场景需做任意维度的上卷或者下钻的分析，一般场景下 BI 看板是不支持的的，基于以上所述用户需求，我们打造了自助对话式 BI 场景来解决用户定制化的需求。</p><p>与一般 BI 场景不同的是，我们将自助/对话式 BI 场景从数据分析人员方下沉到用户方，用户方只需要通过打字，描述数据分析的需求。基于我们公司自然语言处理的能力，自助/对话式 BI 场景会将自然语言转换成SQL，类似 NL2SQL 技术，需要说明的是这里使用的是定制的自然语言解析，相对开源的 NL2SQL 命中率高、解析结果更精确。当自然语言转换成 SQL 后，将 SQL 给到 Apache Doris 查询得到分析结果。由此，用户通过打字就可以随时查看任意场景下的明细数据，或者任意字段的上卷、下钻。</p><p>相比 Apache Kylin、Apache Druid 等预计算的 OLAP 引擎，Apache Doris 符合以下几个特点：</p><ul><li><p>查询灵活，模型不固定，支持自由定制场景。</p></li><li><p>支持表关联、聚合计算、明细查询。</p></li><li><p>响应时间要快速。</p></li></ul><p>因此我们很顺利的运用 Apache Doris 实现了自助/对话式分析场景。同时，自助/对话式分析在我们公司多个数据分析场景应用反馈非常好。</p><h1>实践经验</h1><p>基于上面的两个场景，我们使用过程当中积累了一些经验和心得，分享给大家。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数仓-表设计"><strong>数仓</strong> <strong>表设计：</strong><a class="hash-link" href="#数仓-表设计" title="标题的直接链接">​</a></h3><ol><li><p>千万级(量级供参考，跟集群规模有关系)以下的数据表使用 Duplicate 表类型，Duplicate 表类型同时支持聚合、明细查询，不需要额外写明细表。</p></li><li><p>当数据量比较大时，使用 Aggregate 聚合表类型，在聚合表类型上做上卷索引，使用物化视图优化查询、优化聚合字段。由于 Aggregate 表类型是预计算表，会丢失明细数据，如有明细查询需求，需要额外写一张明细表。</p></li><li><p>当数据量又大、关联表又多时，可用 ETL 先写成宽表，然后导入到 Doris，结合 Aggregate 在聚合表类型上面做优化，也可以使用官方推荐Doris 的 Join 优化：<a href="https://doris.apache.org/zh-CN/docs/dev/advanced/join-optimization/doris-join-optimization" target="_blank" rel="noopener noreferrer">https://doris.apache.org/zh-CN/docs/dev/advanced/join-optimization/doris-join-optimization</a></p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="写入"><strong>写入：</strong><a class="hash-link" href="#写入" title="标题的直接链接">​</a></h3><ol><li><p>通过 Spark Connector 或 Flink Connector 替代 Routine Load： 最早我们使用的是 Routine Load 实时写入 BE 节点， Routine Load 的工作原理是通过 SQL 在 FE 节点起一个类似于 Task Manager 的管理，把任务分发给 BE 节点，在 BE 节点起 Routine Load 任务。在我们实时场景并发很高的情况下，BE 节点 CPU 峰值一般会达到 70% 左右，在这个前提下，Routine Load 也跑到 BE 节点，将严重影响 BE 节点的查询性能，并且查询 CPU 也将影响 Routine Load 导入， Routine Load 就会因为各种资源竞争死掉。面对此问题，目前解决方法是将 Routine Load 从 BE 节点拿出来放到资源调度上，用 Doris-Spark/Flink-Connector 替换 Routine Load。当时 Doris-spark-Connector 还没有实时写入的功能，我们根据业务需求进行了优化，并将方案贡献给社区。</p></li><li><p>通过攒批来控制实时写入频率：当实时写入频率较高时，小文件堆积过多、查询 IO 升高，小文件排序归并的过程将导致查询时间加长，进而出现查询抖动的情况。当前的解决办法是控制导入频次，调整 Compaction 的合并线程、间隔时间等参数，避免 Tablet 下小文件的堆积。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="查询">查询：<a class="hash-link" href="#查询" title="标题的直接链接">​</a></h3><ol><li><p>增加 SQL 黑名单，控制异常大查询。个别用户在查询时没有加 where 条件，或者查询时选择的时间范围较长，这种情况下 BE 节点的 SQL 会把磁盘的负载和 CPU 拉高，导致其他节点的 SQL 查询变慢，甚至出现 BE 节点宕机的情况。目前的解决方案是使用 SQL 黑名单禁止全表及大量分区实时表的查询。</p></li><li><p>使用 SQL Cache 和 SQL Proxy 实现高并发访问。同时使用 SQL Cache 和 SQL Proxy 的原因在于，SQL Cache的颗粒度到表的分区，如果数据发生变更， SQL Cache 将失效，因此 SQL Cache 缓存适合数据更新频次较低的场景（离线场景、历史分区等）。对于数据需要持续写到最新分区的场景， SQL Cache 则是不适用的。当 SQL Cache 失效时 Query 将全部发送到 Doris 造成重复的 Runtime 计算，而 SQL Proxy 可以设置一秒左右的缓存，可以避免相同条件的重复计算，有效提高集群的并发。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="存储">存储：<a class="hash-link" href="#存储" title="标题的直接链接">​</a></h3><p>使用 SSD 和 HDD 做热温数据存储周期的分离，近一年以内的数据存在 SSD，超过一年的数据存在 HDD。Apache Doris 支持对分区设置冷却时间，但只支持创建表分区时设置冷却的时间，目前的解决方案是设置自动同步逻辑，把历史的一些数据从 SSD 迁移到 HDD，确保 1年内的数据都放在 SSD 上。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="升级">升级：<a class="hash-link" href="#升级" title="标题的直接链接">​</a></h3><p>升级前一定要备份元数据，也可以使用新开集群的方式，通过 Broker 将数据文件备份到 S3 或 HDFS 等远端存储系统中，再通过备份恢复的方式将旧集群数据导入到新集群中。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="升级前后性能对比"><strong>升级前后性能对比</strong><a class="hash-link" href="#升级前后性能对比" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="doris_1_1_performance_test_git" src="https://cdn-tencent.selectdb.com/zh-CN/assets/images/doris_1_1_performance_test_git-ad375d6872f12ab1e3cca76d30caa1f6.png" width="1961" height="1126" class="img_ev3q"></p><p>思必驰最早是从 0.12 版本开始使用 Apache Doris 的，在今年我们也完成了从 0.15 版本到最新 1.1 版本的升级操作，并进行了基于真实业务场景和数据的性能测试。</p><p>从以上测试报告中可以看到，总共 13 个测试 SQL 中，前 3 个 SQL 升级前后性能差异不明显，因为这 3 个场景主要是简单的聚合函数，对 Apache Doris 性能要求不高，0.15 版本即可满足需求。而在 Q4 之后的场景中 ，SQL 较为复杂，Group By 有多个字段、多个字段聚合函数以及复杂函数，因此升级新版本后带来的性能提升非常明显，平均查询性能较 0.15 版本提升 2-3 倍。由此，非常推荐大家去升级到 Apache Doris 最新版本。</p><h1>总结和收益</h1><ol><li><p>Apache Doris 支持构建离线+实时统一数仓，一个 ETL 脚本即可支持实时和离线数仓，大大缩短开发周期，降低存储成本，避免了离线和实时指标不一致等问题。</p></li><li><p>Apache Doris 1.1.x 版本开始全面支持向量化计算，较之前版本查询性能提升 2-3 倍。经测试，Apache Doris 1.1.x 版本在宽表场景的查询性能已基本与 ClickHouse 持平。</p></li><li><p>功能强大，不依赖其他组件。相比 Apache Kylin、Apache Druid、ClickHouse 等，Apache Doris 不需要引入第 2 个组件填补技术空档。Apache Doris 支持聚合计算、明细查询、关联查询，当前思必驰超 90% 的分析需求已移步 Apache Doris实现。 得益于此优势，技术人员需要运维的组件减少，极大降低运维成本。</p></li><li><p>易用性极高，支持 MySQL 协议和标准 SQL，大幅降低用户学习成本。</p></li></ol><h1>未来计划</h1><ol><li><p>Tablet 小文件过多的问题。Tablet 是 Apache Doris 中读写数据最小的逻辑单元，当 Tablet 小文件比较多时会产生 2 个问题，一是 Tablet 小文件增多会导致元数据内存压力变大。二是对查询性能的影响，即使是几百兆的查询，但在小文件有几十万、上百万的情况下，一个小小的查询也会导致 IO 非常高。未来，我们将做一个 Tablet 文件数量/大小比值的监控，当比值在不合理范围内时及时进行表设计的修改，使得文件数量和大小的比值在合理的范围内。</p></li><li><p>支持基于 Bitmap 的字符串精确去重。业务中精确去重的场景较多，特别是基于字符串的 UV 场景，目前 Apache Doris 使用的是 Distinct 函数来实现的。未来我们会尝试的在 Apache Doris 中创建字典，基于字典去构建字符串的 Bitmap 索引。</p></li><li><p>Doris-Spark-Connector 流式写入支持分块传输。Doris-Spark-Connector 底层是复用的 Stream Load，工作机制是攒批，容易出现两个问题，一是攒批可能会会出现内存压力导致 OOM，二是当Doris-Spark-Connector 攒批时，Spark Checkpoint 没有提交，但 Buffer 已满并提交给 Doris，此时 Apacche Doris 中已经有数据，但由于没有提交 Checkpoint，假如此时任务恰巧失败，启动后又会重新消费写入一遍。未来我们将优化此问题，实现 Doris-Spark-Connector 流式写入支持分块传输。</p></li></ol></div></article><nav class="pagination-nav" aria-label="博文列表分页导航"></nav></main></div></div></div><div class="footer"><div class="container"><div class="footer-box"><div class="left"><img src="/zh-CN/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/zh-CN/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">资源</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh-CN/download">下载</a></li><li class="footer__item"><a class="footer__link-item" href="/zh-CN/learning">文档</a></li></ul></div><div class="col footer__col"><div class="footer__title">ASF</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">基金会<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="footer__link-item">版权<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">活动<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">捐赠<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">隐私<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">鸣谢<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_lCJq"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">语言</div><ul class="footer__items clean-list"><li class="footer__item"><a href="/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">English</a></li><li class="footer__item"><a href="/zh-CN/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">简体中文</a></li></ul></div></div></div><div class="right"><div class="footer__title">分享</div><div class="social-list"><div class="social"><a href="mailto:dev@doris.apache.org" title="mail" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5.6003 6H26.3997C27.8186 6 28.982 7.10964 29 8.46946L16.0045 15.454L3.01202 8.47829C3.02405 7.11258 4.1784 6 5.6003 6ZM3.01202 11.1508L3 23.5011C3 24.8756 4.16938 26 5.6003 26H26.3997C27.8306 26 29 24.8756 29 23.5011V11.145L16.3111 17.8028C16.1157 17.9058 15.8813 17.9058 15.6889 17.8028L3.01202 11.1508V11.1508Z" fill="currentColor"></path></svg></a><a href="https://github.com/apache/doris" title="github" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M16.0001 2.66675C8.63342 2.66675 2.66675 8.63341 2.66675 16.0001C2.66524 18.7991 3.54517 21.5276 5.1817 23.7983C6.81824 26.0691 9.12828 27.7668 11.7841 28.6508C12.4508 28.7668 12.7001 28.3668 12.7001 28.0161C12.7001 27.7001 12.6828 26.6508 12.6828 25.5334C9.33342 26.1508 8.46675 24.7174 8.20008 23.9668C8.04942 23.5828 7.40008 22.4001 6.83342 22.0828C6.36675 21.8334 5.70008 21.2161 6.81608 21.2001C7.86675 21.1828 8.61608 22.1668 8.86675 22.5668C10.0668 24.5828 11.9841 24.0161 12.7494 23.6668C12.8668 22.8001 13.2161 22.2174 13.6001 21.8841C10.6334 21.5508 7.53342 20.4001 7.53342 15.3001C7.53342 13.8494 8.04942 12.6507 8.90008 11.7161C8.76675 11.3827 8.30008 10.0161 9.03342 8.18275C9.03342 8.18275 10.1494 7.83342 12.7001 9.55075C13.7855 9.2495 14.907 9.09787 16.0334 9.10008C17.1668 9.10008 18.3001 9.24942 19.3668 9.54942C21.9161 7.81608 23.0334 8.18408 23.0334 8.18408C23.7668 10.0174 23.3001 11.3841 23.1668 11.7174C24.0161 12.6507 24.5334 13.8334 24.5334 15.3001C24.5334 20.4174 21.4174 21.5508 18.4508 21.8841C18.9334 22.3001 19.3508 23.1001 19.3508 24.3508C19.3508 26.1334 19.3334 27.5668 19.3334 28.0174C19.3334 28.3668 19.5841 28.7828 20.2508 28.6494C22.8975 27.7558 25.1973 26.0547 26.8266 23.7856C28.4559 21.5165 29.3327 18.7936 29.3334 16.0001C29.3334 8.63341 23.3668 2.66675 16.0001 2.66675V2.66675Z" fill="currentColor"></path></svg></a><a href="https://twitter.com/doris_apache" title="twitter" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M27.1493 10.8313C27.1493 10.5687 27.1442 10.3091 27.1326 10.0512C28.2554 9.21471 29.2295 8.1676 30 6.96938C28.9537 7.44012 27.8403 7.74469 26.7 7.8721C27.8868 7.14188 28.7969 5.97403 29.227 4.57256C28.1158 5.24638 26.8867 5.72811 25.5798 5.97912C24.5326 4.78777 23.0383 4.02895 21.3864 4.00076C18.2137 3.94831 15.6418 6.62904 15.6418 9.98754C15.6418 10.4649 15.6918 10.9279 15.7909 11.3749C11.0133 11.0675 6.77972 8.58103 3.9478 4.8326C3.45367 5.73067 3.17017 6.78025 3.17017 7.90503C3.17017 10.0324 4.18438 11.9232 5.72536 13.039C4.78198 12.9963 3.89827 12.7106 3.12316 12.2422V12.3204C3.12316 15.2937 5.10395 17.7849 7.73223 18.3661C7.2504 18.5032 6.74218 18.5745 6.2195 18.5719C5.85634 18.57 5.49437 18.5304 5.13941 18.4536C5.86973 20.8902 7.99227 22.6703 10.5051 22.7297C8.53846 24.3601 6.06106 25.334 3.37133 25.3272C2.90757 25.3272 2.44929 25.2961 2 25.2397C4.54329 26.9841 7.56224 28 10.8076 28C21.3719 28.0025 27.1493 18.8084 27.1493 10.8313V10.8313Z" fill="currentColor"></path></svg></a></div><div class="social"><a href="https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-1h153f1ar-sTJB_QahY1SHvZdtPFoIOQ" title="slack" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_125_278)"><path d="M12.5875 16.6906C11.0844 16.6906 9.86562 17.9094 9.86562 19.4125V26.2375C9.86562 26.9594 10.1524 27.6517 10.6628 28.1622C11.1733 28.6726 11.8656 28.9594 12.5875 28.9594C13.3094 28.9594 14.0017 28.6726 14.5122 28.1622C15.0226 27.6517 15.3094 26.9594 15.3094 26.2375V19.4531C15.3094 17.9094 14.0906 16.6906 12.5875 16.6906ZM3 19.4531C3 20.175 3.28677 20.8673 3.79722 21.3778C4.30767 21.8882 4.99999 22.175 5.72187 22.175C6.44376 22.175 7.13608 21.8882 7.64653 21.3778C8.15698 20.8673 8.44375 20.175 8.44375 19.4531V16.7312H5.7625C4.25938 16.6906 3 17.9094 3 19.4531ZM12.5875 3C11.8656 3 11.1733 3.28677 10.6628 3.79722C10.1524 4.30767 9.86562 4.99999 9.86562 5.72187C9.86562 6.44376 10.1524 7.13608 10.6628 7.64653C11.1733 8.15698 11.8656 8.44375 12.5875 8.44375H15.3094V5.72187C15.3094 4.21875 14.0906 3 12.5875 3ZM5.72187 15.3094H12.5469C13.2688 15.3094 13.9611 15.0226 14.4715 14.5122C14.982 14.0017 15.2688 13.3094 15.2688 12.5875C15.2688 11.8656 14.982 11.1733 14.4715 10.6628C13.9611 10.1524 13.2688 9.86562 12.5469 9.86562H5.72187C4.99999 9.86562 4.30767 10.1524 3.79722 10.6628C3.28677 11.1733 3 11.8656 3 12.5875C3 13.3094 3.28677 14.0017 3.79722 14.5122C4.30767 15.0226 4.99999 15.3094 5.72187 15.3094ZM26.2375 9.86562C24.7344 9.86562 23.5156 11.0844 23.5156 12.5875V15.3094H26.2375C26.9594 15.3094 27.6517 15.0226 28.1622 14.5122C28.6726 14.0017 28.9594 13.3094 28.9594 12.5875C28.9594 11.8656 28.6726 11.1733 28.1622 10.6628C27.6517 10.1524 26.9594 9.86562 26.2375 9.86562ZM16.6906 5.72187V12.5875C16.6906 13.3094 16.9774 14.0017 17.4878 14.5122C17.9983 15.0226 18.6906 15.3094 19.4125 15.3094C20.1344 15.3094 20.8267 15.0226 21.3372 14.5122C21.8476 14.0017 22.1344 13.3094 22.1344 12.5875V5.72187C22.1344 4.99999 21.8476 4.30767 21.3372 3.79722C20.8267 3.28677 20.1344 3 19.4125 3C18.6906 3 17.9983 3.28677 17.4878 3.79722C16.9774 4.30767 16.6906 4.99999 16.6906 5.72187ZM22.1344 26.2781C22.1344 24.775 20.9156 23.5562 19.4125 23.5562H16.6906V26.2781C16.6906 27 16.9774 27.6923 17.4878 28.2028C17.9983 28.7132 18.6906 29 19.4125 29C20.1344 29 20.8267 28.7132 21.3372 28.2028C21.8476 27.6923 22.1344 27 22.1344 26.2781ZM26.2781 16.6906H19.4125C18.6906 16.6906 17.9983 16.9774 17.4878 17.4878C16.9774 17.9983 16.6906 18.6906 16.6906 19.4125C16.6906 20.1344 16.9774 20.8267 17.4878 21.3372C17.9983 21.8476 18.6906 22.1344 19.4125 22.1344H26.2375C27.7406 22.1344 28.9594 20.9156 28.9594 19.4125C29 17.9094 27.7812 16.6906 26.2781 16.6906Z" fill="currentColor"></path></g><defs><clipPath id="clip0_125_278"><rect width="26" height="26" fill="currentColor" transform="translate(3 3)"></rect></clipPath></defs></svg></a><a href="https://space.bilibili.com/362350065" title="bilibili" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M23.0533 11.5412H9.14591C8.72353 11.5412 8.36784 11.8633 8.36784 12.296V21.5162C8.36784 21.9489 8.72349 22.2665 9.14591 22.2665H23.0533C23.4757 22.2665 23.7928 21.949 23.7928 21.5162V12.296C23.7928 11.8633 23.4757 11.5412 23.0533 11.5412ZM10.0928 14.9479L14.0122 14.1975L14.3083 15.6685L10.4284 16.4188L10.0928 14.9479ZM16.1348 19.4301C14.9303 20.7431 13.6667 19.0155 13.6667 19.0155L14.3084 18.6008C14.3084 18.6008 15.1673 20.1508 16.125 18.0973C17.0432 20.0916 18.06 18.6206 18.06 18.6304L18.6426 19.0057C18.6426 19.0057 17.5565 20.7431 16.1348 19.4301ZM21.9498 16.4189L18.06 15.6686L18.3661 14.1976L22.2756 14.948L21.9498 16.4189Z" fill="currentColor"></path><path d="M16 2C8.26801 2 2 8.26805 2 16C2 23.7319 8.26806 30 16 30C23.7319 30 30 23.7319 30 16C30 8.26801 23.732 2 16 2ZM23.3727 24.1329C22.3941 24.1019 22.0644 24.1329 22.0644 24.1329C22.0644 24.1329 21.9923 25.2558 21.0343 25.2764C20.0659 25.2867 19.9216 24.4934 19.8907 24.1947C19.3035 24.1947 12.2467 24.2255 12.2467 24.2255C12.2467 24.2255 12.1231 25.266 11.165 25.266C10.1967 25.266 10.1451 24.4006 10.0833 24.2255C9.45486 24.2255 8.6101 24.2049 8.6101 24.2049C8.6101 24.2049 6.48791 23.7621 6.20978 21.0012C6.24067 18.2402 6.20978 12.7801 6.20978 12.7801C6.20978 12.7801 6.01404 10.2356 8.54836 9.50415C9.33118 9.4733 11.0208 9.46293 12.9781 9.46293L11.1753 7.71159C11.1753 7.71159 10.8971 7.36131 11.371 6.96986C11.8553 6.57846 11.8757 6.73797 12.0406 6.85128C12.2055 6.96456 14.7295 9.45229 14.7295 9.45229H14.3895C15.3579 9.45229 16.3572 9.46799 17.3152 9.46799C17.686 9.09711 19.798 7.02903 19.9422 6.92612C20.107 6.82309 20.1378 6.64927 20.6118 7.04068C21.0857 7.43213 20.8075 7.78309 20.8075 7.78309L19.0459 9.48322C21.4668 9.50386 23.3315 9.51423 23.3315 9.51423C23.3315 9.51423 25.7214 10.0398 25.7833 12.78C25.7524 15.5203 25.7936 21.0319 25.7936 21.0319C25.7936 21.0319 25.6598 23.7104 23.3727 24.1329Z" fill="currentColor"></path></svg></a><a class="item wechat"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M20.7578 11.5169C21.0708 11.5169 21.3795 11.5398 21.6851 11.573C20.8524 7.73517 16.7052 4.88306 11.9718 4.88306C6.67951 4.88306 2.34412 8.45283 2.34412 12.9854C2.34412 15.6013 3.78679 17.7498 6.19667 19.4161L5.2339 22.2827L8.59917 20.6122C9.80411 20.8478 10.7698 21.0906 11.9718 21.0906C12.2738 21.0906 12.5728 21.0759 12.8703 21.0523C12.682 20.4159 12.5728 19.7485 12.5728 19.0566C12.5728 14.8947 16.1847 11.5169 20.7578 11.5169ZM15.5822 8.9335C16.3072 8.9335 16.7871 9.40601 16.7871 10.1229C16.7871 10.8369 16.3072 11.3153 15.5822 11.3153C14.8601 11.3153 14.1365 10.8369 14.1365 10.1229C14.1365 9.40601 14.8601 8.9335 15.5822 8.9335ZM8.84429 11.3153C8.12218 11.3153 7.3942 10.8368 7.3942 10.1229C7.3942 9.40597 8.12218 8.93346 8.84429 8.93346C9.56559 8.93346 10.0463 9.40597 10.0463 10.1229C10.0463 10.8369 9.56559 11.3153 8.84429 11.3153ZM29.5453 18.9422C29.5453 15.1332 25.6935 12.0285 21.3677 12.0285C16.7871 12.0285 13.1797 15.1332 13.1797 18.9422C13.1797 22.7567 16.7871 25.8547 21.3677 25.8547C22.326 25.8547 23.2932 25.6169 24.2559 25.3777L26.897 26.8086L26.1726 24.4282C28.1056 22.993 29.5453 21.0906 29.5453 18.9422ZM18.7126 17.7498C18.2335 17.7498 17.7499 17.278 17.7499 16.7966C17.7499 16.3219 18.2335 15.8442 18.7126 15.8442C19.4406 15.8442 19.9176 16.3219 19.9176 16.7966C19.9176 17.278 19.4406 17.7498 18.7126 17.7498ZM24.0079 17.7498C23.5324 17.7498 23.0518 17.278 23.0518 16.7966C23.0518 16.3219 23.5324 15.8442 24.0079 15.8442C24.73 15.8442 25.2128 16.3219 25.2128 16.7966C25.2128 17.278 24.73 17.7498 24.0079 17.7498Z" fill="currentColor"></path></svg><div class="wechat-dropdown"><img src="data:image/png;base64,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" alt=""></div></a></div></div></div></div><div class="footer__copyright">Copyright © 2022 The Apache Software Foundation,Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License, Version 2.0</a>. Apache, Doris, Apache Doris, the Apache feather logo and the Apache Doris logo are trademarks of The Apache Software Foundation.</div></div></div></div>
<script src="https://cdn-tencent.selectdb.com/zh-CN/assets/js/runtime~main.276d6eed.js"></script>
<script src="https://cdn-tencent.selectdb.com/zh-CN/assets/js/main.826cf4ca.js"></script>
</body>
</html>